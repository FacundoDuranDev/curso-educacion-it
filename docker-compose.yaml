
services:
  # PostgreSQL - Base de datos principal
  postgres:
    image: postgres:15
    container_name: postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin
      POSTGRES_DB: educacionit
      POSTGRES_MULTIPLE_DATABASES: metastore
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./scripts/init_hive_metastore.sql:/docker-entrypoint-initdb.d/init_hive_metastore.sql
    networks:
      - hadoop-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d educacionit"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Hadoop Namenode
  hadoop-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-namenode
    restart: unless-stopped
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - CORE_CONF_io_compression_codecs=org.apache.hadoop.io.compress.SnappyCodec
    ports:
      - "9870:9870"
      - "9864:9864"
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    networks:
      - hadoop-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Hadoop Datanodes
  hadoop-datanode-1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-datanode-1
    restart: unless-stopped
    environment:
      - SERVICE_PRECONDITION=hadoop-namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - CORE_CONF_io_compression_codecs=org.apache.hadoop.io.compress.SnappyCodec
    volumes:
      - hadoop_datanode_1:/hadoop/dfs/data
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    depends_on:
      hadoop-namenode:
        condition: service_healthy
    networks:
      - hadoop-network

  hadoop-datanode-2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-datanode-2
    restart: unless-stopped
    environment:
      - SERVICE_PRECONDITION=hadoop-namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - CORE_CONF_io_compression_codecs=org.apache.hadoop.io.compress.SnappyCodec
    volumes:
      - hadoop_datanode_2:/hadoop/dfs/data
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    depends_on:
      hadoop-namenode:
        condition: service_healthy
    networks:
      - hadoop-network

  hadoop-datanode-3:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-datanode-3
    restart: unless-stopped
    environment:
      - SERVICE_PRECONDITION=hadoop-namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - CORE_CONF_io_compression_codecs=org.apache.hadoop.io.compress.SnappyCodec
    volumes:
      - hadoop_datanode_3:/hadoop/dfs/data
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    depends_on:
      hadoop-namenode:
        condition: service_healthy
    networks:
      - hadoop-network

  # Spark Master
  spark-master:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    container_name: spark-master
    restart: unless-stopped
    ports:
      - "7077:7077"
      - "8080:8080"
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:8020
      - SPARK_CONF_spark_eventLog_enabled=true
      - SPARK_CONF_spark_eventLog_dir=hdfs://hadoop-namenode:8020/spark-logs
      - SPARK_CONF_spark_history_fs_logDirectory=hdfs://hadoop-namenode:8020/spark-logs
      - SPARK_CONF_spark_sql_warehouse_dir=hdfs://hadoop-namenode:8020/user/hive/warehouse
    depends_on:
      hadoop-namenode:
        condition: service_healthy
    networks:
      - hadoop-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Spark Worker
  spark-worker:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker
    restart: unless-stopped
    depends_on:
      spark-master:
        condition: service_healthy
      hadoop-namenode:
        condition: service_healthy
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:8020
      - SPARK_CONF_spark_eventLog_enabled=true
      - SPARK_CONF_spark_eventLog_dir=hdfs://hadoop-namenode:8020/spark-logs
      - SPARK_CONF_spark_sql_warehouse_dir=hdfs://hadoop-namenode:8020/user/hive/warehouse
    ports:
      - "8081:8081"
    networks:
      - hadoop-network

  # Hive Metastore
  hive-metastore:
    image: bde2020/hive-metastore:2.3.2-postgresql-metastore
    container_name: hive-metastore
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    ports:
      - "9083:9083"
    environment:
      - DB_DRIVER=postgres
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_NAME=metastore
      - DB_USER=admin
      - DB_PASS=admin
    volumes:
      - hive_warehouse:/opt/hive/data/warehouse
      - hive_logs:/opt/hive/logs

    networks:
      - hadoop-network

    mem_limit: 1g
    cpus: 0.5
    healthcheck:
      test: ["CMD", "netstat", "-an", "|", "grep", "9083"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Hive Server2
  hive-server:
    image: bde2020/hive-server:2.3.2-postgresql-metastore
    container_name: hive-server
    restart: unless-stopped
    depends_on:
      hive-metastore:
        condition: service_healthy
      hadoop-namenode:
        condition: service_healthy
    ports:
      - "10000:10000"
      - "10002:10002"
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:8020
      - CORE_CONF_hive_metastore_warehouse_dir=hdfs://hadoop-namenode:8020/user/hive/warehouse
    volumes:
      - hive_warehouse:/opt/hive/data/warehouse
      - hive_logs:/opt/hive/logs
      - ./Etapa 1:/opt/hive/data
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    networks:
      - hadoop-network
    command: ["hive", "--service", "hiveserver2"]
    mem_limit: 2g
    cpus: 1.0
    healthcheck:
      test: ["CMD", "netstat", "-an", "|", "grep", "10000"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Jupyter Lab
  jupyter:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyter
    restart: unless-stopped
    ports:
      - "8888:8888"
      - "4040:4040"
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - SPARK_OPTS=--driver-java-options=-Xms1024M --driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.log4j.configuration=file:/usr/local/spark/conf/log4j.properties
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./Etapa 1:/home/jovyan/work/data
      - ./scripts:/home/jovyan/work/scripts
    depends_on:
      postgres:
        condition: service_healthy
      spark-master:
        condition: service_healthy
      hive-metastore:
        condition: service_healthy
    command: start.sh jupyter lab --LabApp.token='' --LabApp.password='' --ip=0.0.0.0 --port=8888 --allow-root --no-browser
    networks:
      - hadoop-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  hadoop-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  pgdata:
    driver: local
  hadoop_namenode:
    driver: local
  hadoop_datanode_1:
    driver: local
  hadoop_datanode_2:
    driver: local
  hadoop_datanode_3:
    driver: local
  hive_warehouse:
    driver: local
  hive_logs:
    driver: local
