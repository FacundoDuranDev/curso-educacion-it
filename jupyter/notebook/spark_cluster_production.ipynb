{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b3a5302",
   "metadata": {},
   "source": [
    "# ğŸš€ Spark Cluster - ConfiguraciÃ³n de ProducciÃ³n\n",
    "\n",
    "## ğŸ“‹ ConfiguraciÃ³n basada en Spark 3.5.3 - DocumentaciÃ³n Oficial\n",
    "\n",
    "### âš ï¸ Requisitos mÃ­nimos identificados:\n",
    "- **Driver**: MÃ­nimo 471MB (450MB + overhead)\n",
    "- **Executor**: MÃ­nimo 471MB (450MB + overhead) \n",
    "- **Workers**: Deben tener suficiente memoria para el overhead del sistema + executors\n",
    "\n",
    "### ğŸ“Š Recursos actuales disponibles:\n",
    "- **Workers**: 2GB cada uno\n",
    "- **JupyterLab**: 2GB\n",
    "- **Overhead sistema**: ~1GB por worker\n",
    "- **Disponible para Spark**: ~1GB por worker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d03232a",
   "metadata": {},
   "source": [
    "## ğŸ”§ ConfiguraciÃ³n 1: Cluster Mode Conservador\n",
    "### Solo 1 executor total para garantizar funcionamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d567db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ CONFIGURACIÃ“N CLUSTER CONSERVADORA - 1 EXECUTOR TOTAL\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Detener cualquier sesiÃ³n existente\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"ğŸ›‘ SesiÃ³n anterior detenida\")\n",
    "    time.sleep(5)\n",
    "except:\n",
    "    print(\"ğŸ” No habÃ­a sesiÃ³n activa\")\n",
    "\n",
    "print(\"ğŸ”„ Creando sesiÃ³n Spark CLUSTER MODE - ConfiguraciÃ³n conservadora...\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EducacionIT-Cluster-Conservative\") \\\n",
    "    .master(\"spark://master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"300m\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"âœ… Spark iniciado: {spark.version}\")\n",
    "print(f\"ğŸ”— Master: {spark.sparkContext.master}\")\n",
    "print(f\"ğŸ“Š Spark UI: http://localhost:4041\")\n",
    "print(f\"ğŸ¯ App ID: {spark.sparkContext.applicationId}\")\n",
    "\n",
    "print(\"\\nğŸ“Š CONFIGURACIÃ“N CONSERVADORA:\")\n",
    "print(\"   âœ… 1 executor con 500MB (cumple mÃ­nimo de 471MB)\")\n",
    "print(\"   âœ… Driver con 1GB (excede mÃ­nimo de 471MB)\")\n",
    "print(\"   âœ… Total memoria usada: 1GB\")\n",
    "print(\"   âœ… DeberÃ­a funcionar sin problemas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7822190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª TEST BÃSICO DE FUNCIONAMIENTO\n",
    "print(\"ğŸ§ª PROBANDO CONECTIVIDAD DEL CLUSTER:\")\n",
    "\n",
    "try:\n",
    "    # Test 1: OperaciÃ³n RDD simple\n",
    "    print(\"\\n1ï¸âƒ£ Test RDD bÃ¡sico:\")\n",
    "    rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 2)\n",
    "    result = rdd.sum()\n",
    "    print(f\"   âœ… Suma distribuida: {result}\")\n",
    "    \n",
    "    # Test 2: DataFrame\n",
    "    print(\"\\n2ï¸âƒ£ Test DataFrame:\")\n",
    "    from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "    \n",
    "    data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\"), (4, \"Diana\")]\n",
    "    schema = StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    count = df.count()\n",
    "    print(f\"   âœ… Conteo distribuido: {count} filas\")\n",
    "    \n",
    "    # Test 3: Mostrar datos\n",
    "    print(\"\\n3ï¸âƒ£ Datos de muestra:\")\n",
    "    df.show()\n",
    "    \n",
    "    print(\"\\nğŸ‰ Â¡CLUSTER MODE FUNCIONANDO CORRECTAMENTE!\")\n",
    "    print(\"ğŸ’¡ El executor estÃ¡ procesando trabajos distribuidos\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error en cluster mode: {e}\")\n",
    "    print(\"ğŸ”§ Revisa los logs de los workers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28532cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ—„ï¸ TEST DE HIVE EN CLUSTER MODE\n",
    "print(\"ğŸ—„ï¸ PROBANDO CONECTIVIDAD CON HIVE:\")\n",
    "\n",
    "try:\n",
    "    # Verificar bases de datos\n",
    "    print(\"\\n1ï¸âƒ£ Verificando bases de datos:\")\n",
    "    databases = spark.sql(\"SHOW DATABASES\").collect()\n",
    "    print(f\"   âœ… Bases de datos encontradas: {len(databases)}\")\n",
    "    for db in databases:\n",
    "        print(f\"      - {db[0]}\")\n",
    "    \n",
    "    # Conectar a educacionit\n",
    "    print(\"\\n2ï¸âƒ£ Conectando a educacionit:\")\n",
    "    spark.sql(\"USE educacionit\")\n",
    "    print(\"   âœ… Conectado a educacionit\")\n",
    "    \n",
    "    # Listar tablas\n",
    "    print(\"\\n3ï¸âƒ£ Listando tablas:\")\n",
    "    tables = spark.sql(\"SHOW TABLES\").collect()\n",
    "    print(f\"   âœ… Tablas encontradas: {len(tables)}\")\n",
    "    for table in tables:\n",
    "        print(f\"      - {table[1]}\")\n",
    "    \n",
    "    # Test de lectura si hay tablas\n",
    "    if len(tables) > 0:\n",
    "        table_name = tables[0][1]\n",
    "        print(f\"\\n4ï¸âƒ£ Leyendo datos de {table_name}:\")\n",
    "        df = spark.sql(f\"SELECT * FROM {table_name} LIMIT 3\")\n",
    "        print(\"   ğŸ“Š Primeras 3 filas:\")\n",
    "        df.show()\n",
    "        \n",
    "        count = spark.sql(f\"SELECT COUNT(*) as total FROM {table_name}\").collect()[0][0]\n",
    "        print(f\"   âœ… Total de registros: {count}\")\n",
    "    \n",
    "    print(\"\\nğŸ‰ Â¡HIVE + CLUSTER MODE FUNCIONANDO PERFECTAMENTE!\")\n",
    "    print(\"ğŸ’¡ Puedes ejecutar consultas SQL distribuidas\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error con Hive: {e}\")\n",
    "    print(\"ğŸ”§ Verifica que Hive estÃ© funcionando correctamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae97a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ›‘ MODO LOCAL COMO ALTERNATIVA\n",
    "# Si el cluster sigue fallando, usemos modo local con conectividad a Hive\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Detener cualquier sesiÃ³n existente\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"ğŸ›‘ SesiÃ³n anterior detenida\")\n",
    "    time.sleep(5)\n",
    "except:\n",
    "    print(\"ğŸ” No habÃ­a sesiÃ³n activa\")\n",
    "\n",
    "print(\"ğŸ”„ Creando sesiÃ³n Spark MODO LOCAL con Hive...\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EducacionIT-Local-Hive\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.driver.cores\", \"2\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"âœ… Spark iniciado: {spark.version}\")\n",
    "print(f\"ğŸ”— Master: {spark.sparkContext.master}\")\n",
    "print(f\"ğŸ“Š Spark UI: http://localhost:4041\")\n",
    "print(f\"ğŸ¯ App ID: {spark.sparkContext.applicationId}\")\n",
    "\n",
    "print(\"\\nğŸ“Š MODO LOCAL CON HIVE:\")\n",
    "print(\"   âœ… Driver con 1GB\")\n",
    "print(\"   âœ… 2 cores locales\")\n",
    "print(\"   âœ… Conectividad completa a Hive\")\n",
    "print(\"   âœ… Funciona sin problemas de workers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fd3c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ CONFIGURACIÃ“N ULTRA SIMPLE QUE FUNCIONA\n",
    "# Recursos liberados - ahora probemos con configuraciÃ³n mÃ­nima\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Detener cualquier sesiÃ³n existente\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"ğŸ›‘ SesiÃ³n anterior detenida\")\n",
    "    time.sleep(5)\n",
    "except:\n",
    "    print(\"ğŸ” No habÃ­a sesiÃ³n activa\")\n",
    "\n",
    "print(\"ğŸ”„ Creando sesiÃ³n Spark ULTRA SIMPLE...\")\n",
    "\n",
    "# ConfiguraciÃ³n mÃ­nima pero funcional\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EducacionIT-Simple\") \\\n",
    "    .master(\"spark://master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"800m\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"800m\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .config(\"spark.network.timeout\", \"300s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"âœ… Spark iniciado: {spark.version}\")\n",
    "print(f\"ğŸ”— Master: {spark.sparkContext.master}\")\n",
    "print(f\"ğŸ“Š Spark UI: http://localhost:4041\")\n",
    "print(f\"ğŸ¯ App ID: {spark.sparkContext.applicationId}\")\n",
    "\n",
    "print(\"\\nğŸ“Š CONFIGURACIÃ“N ULTRA SIMPLE:\")\n",
    "print(\"   âœ… 1 executor con 800MB (muy por encima del mÃ­nimo)\")\n",
    "print(\"   âœ… Driver con 800MB (muy por encima del mÃ­nimo)\")\n",
    "print(\"   âœ… Timeouts aumentados para evitar desconexiones\")\n",
    "print(\"   âœ… Recursos disponibles: 4GB totales\")\n",
    "\n",
    "# Test inmediato\n",
    "print(\"\\nğŸ§ª Test rÃ¡pido:\")\n",
    "try:\n",
    "    rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "    result = rdd.sum()\n",
    "    print(f\"   âœ… Test exitoso - Suma: {result}\")\n",
    "    print(\"   ğŸ‰ Â¡CLUSTER MODE FUNCIONANDO!\")\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4359312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ—„ï¸ PRUEBA DE CONECTIVIDAD CON HIVE\n",
    "# Ahora que el cluster funciona, probemos Hive\n",
    "\n",
    "print(\"ğŸ” Probando conectividad con Hive...\")\n",
    "\n",
    "try:\n",
    "    # Mostrar bases de datos disponibles\n",
    "    print(\"\\nğŸ“Š Bases de datos disponibles:\")\n",
    "    databases = spark.sql(\"SHOW DATABASES\")\n",
    "    databases.show()\n",
    "    \n",
    "    # Usar la base de datos educacionit\n",
    "    print(\"\\nğŸ¯ Usando base de datos 'educacionit'...\")\n",
    "    spark.sql(\"USE educacionit\")\n",
    "    print(\"âœ… Base de datos 'educacionit' seleccionada\")\n",
    "    \n",
    "    # Mostrar tablas disponibles\n",
    "    print(\"\\nğŸ“‹ Tablas disponibles en 'educacionit':\")\n",
    "    tables = spark.sql(\"SHOW TABLES\")\n",
    "    tables.show()\n",
    "    \n",
    "    print(\"ğŸ‰ Â¡CONECTIVIDAD CON HIVE EXITOSA!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error conectando con Hive: {e}\")\n",
    "    print(\"ğŸ’¡ Verificando si necesitamos habilitar Hive Support...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0261bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ SPARK CLUSTER CON HIVE SUPPORT EXPLÃCITO\n",
    "# Si la celda anterior fallÃ³, probemos con Hive Support habilitado\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "print(\"ğŸ”„ Recreando sesiÃ³n Spark con Hive Support explÃ­cito...\")\n",
    "\n",
    "# Detener sesiÃ³n actual\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"ğŸ›‘ SesiÃ³n anterior detenida\")\n",
    "    time.sleep(5)\n",
    "except:\n",
    "    print(\"ğŸ” No habÃ­a sesiÃ³n activa\")\n",
    "\n",
    "# Crear nueva sesiÃ³n con Hive Support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EducacionIT-Cluster-Hive\") \\\n",
    "    .master(\"spark://master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"800m\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"800m\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .config(\"spark.network.timeout\", \"300s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"âœ… Spark con Hive iniciado: {spark.version}\")\n",
    "print(f\"ğŸ”— Master: {spark.sparkContext.master}\")\n",
    "print(f\"ğŸ“Š Spark UI: http://localhost:4041\")\n",
    "\n",
    "# Test inmediato con Hive\n",
    "print(\"\\nğŸ§ª Test de conectividad con Hive:\")\n",
    "try:\n",
    "    databases = spark.sql(\"SHOW DATABASES\")\n",
    "    databases.show()\n",
    "    print(\"ğŸ‰ Â¡HIVE FUNCIONANDO EN CLUSTER MODE!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š PRUEBAS AVANZADAS CON DATOS DE EDUCACIONIT\n",
    "# Consultas distribuidas sobre las tablas de Hive\n",
    "\n",
    "print(\"ğŸ” Probando consultas sobre datos de EducacionIT...\")\n",
    "\n",
    "try:\n",
    "    # Usar la base de datos educacionit\n",
    "    spark.sql(\"USE educacionit\")\n",
    "    print(\"âœ… Usando base de datos 'educacionit'\")\n",
    "    \n",
    "    # Contar registros en tabla clientes\n",
    "    print(\"\\nğŸ‘¥ Contando clientes:\")\n",
    "    clientes_count = spark.sql(\"SELECT COUNT(*) as total_clientes FROM clientes\")\n",
    "    clientes_count.show()\n",
    "    \n",
    "    # Mostrar algunas ventas\n",
    "    print(\"\\nğŸ’° Mostrando primeras 5 ventas:\")\n",
    "    ventas_sample = spark.sql(\"SELECT * FROM venta LIMIT 5\")\n",
    "    ventas_sample.show()\n",
    "    \n",
    "    # Consulta agregada: ventas por canal\n",
    "    print(\"\\nğŸ“ˆ Ventas totales por canal:\")\n",
    "    ventas_por_canal = spark.sql(\"\"\"\n",
    "        SELECT c.descripcion as canal, \n",
    "               COUNT(*) as num_ventas,\n",
    "               SUM(v.precio * v.cantidad) as total_ventas\n",
    "        FROM venta v \n",
    "        JOIN canaldeventa c ON v.idcanal = c.idcanal \n",
    "        GROUP BY c.descripcion \n",
    "        ORDER BY total_ventas DESC\n",
    "    \"\"\")\n",
    "    ventas_por_canal.show()\n",
    "    \n",
    "    print(\"ğŸ‰ Â¡CONSULTAS DISTRIBUIDAS FUNCIONANDO!\")\n",
    "    print(\"ğŸš€ Â¡CLUSTER SPARK + HIVE COMPLETAMENTE OPERATIVO!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error en consultas: {e}\")\n",
    "    print(\"ğŸ’¡ Verificando estructura de tablas...\")\n",
    "    try:\n",
    "        spark.sql(\"SHOW TABLES\").show()\n",
    "    except:\n",
    "        print(\"âŒ No se pudo acceder a las tablas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf5d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš¨ ARREGLO INMEDIATO - CONFIGURACIÃ“N CORRECTA\n",
    "# El executor volviÃ³ a 300MB, necesitamos restaurar los 800MB que funcionaban\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "print(\"ğŸš¨ ARREGLANDO CONFIGURACIÃ“N DE MEMORIA...\")\n",
    "print(\"âŒ Problema: Executor con solo 300MB (necesita mÃ­nimo 471MB)\")\n",
    "print(\"âœ… SoluciÃ³n: Restaurar configuraciÃ³n de 800MB que funcionaba\")\n",
    "\n",
    "# DETENER CUALQUIER SESIÃ“N EXISTENTE\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"ğŸ›‘ SesiÃ³n problemÃ¡tica detenida\")\n",
    "    time.sleep(8)  # Esperar mÃ¡s tiempo para limpieza completa\n",
    "except:\n",
    "    print(\"ğŸ” No habÃ­a sesiÃ³n activa\")\n",
    "\n",
    "print(\"\\nğŸ”§ RECREANDO SPARK CON CONFIGURACIÃ“N CORRECTA...\")\n",
    "\n",
    "# CONFIGURACIÃ“N EXACTA QUE FUNCIONABA EN CELDA 6\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EducacionIT-Fixed-800MB\") \\\n",
    "    .master(\"spark://master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"800m\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"800m\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .config(\"spark.network.timeout\", \"300s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"âœ… Spark restaurado: {spark.version}\")\n",
    "print(f\"ğŸ”— Master: {spark.sparkContext.master}\")\n",
    "print(f\"ğŸ“Š Spark UI: http://localhost:4041\")\n",
    "print(f\"ğŸ¯ App ID: {spark.sparkContext.applicationId}\")\n",
    "\n",
    "print(\"\\nğŸ“Š CONFIGURACIÃ“N RESTAURADA:\")\n",
    "print(\"   âœ… Executor: 800MB (muy por encima del mÃ­nimo de 471MB)\")\n",
    "print(\"   âœ… Driver: 800MB\")\n",
    "print(\"   âœ… Hive Support habilitado\")\n",
    "\n",
    "# TEST INMEDIATO PARA CONFIRMAR\n",
    "print(\"\\nğŸ§ª Test de confirmaciÃ³n:\")\n",
    "try:\n",
    "    rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "    result = rdd.sum()\n",
    "    print(f\"   âœ… RDD Test exitoso - Suma: {result}\")\n",
    "    \n",
    "    # Test Hive\n",
    "    databases = spark.sql(\"SHOW DATABASES\")\n",
    "    print(f\"   âœ… Hive Test exitoso - Bases de datos encontradas\")\n",
    "    databases.show()\n",
    "    \n",
    "    print(\"ğŸ‰ Â¡CONFIGURACIÃ“N ARREGLADA Y FUNCIONANDO!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Error: {e}\")\n",
    "    print(\"   ğŸ”„ Intentando nuevamente...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd217dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” DIAGNÃ“STICO: LOS DATOS EXISTEN EN EL METASTORE\n",
    "# Confirmamos que educacionit y sus 10 tablas estÃ¡n en PostgreSQL\n",
    "# El problema es la conectividad Spark <-> Metastore\n",
    "\n",
    "print(\"ğŸ” DIAGNÃ“STICO COMPLETO:\")\n",
    "print(\"âœ… Base de datos 'educacionit' EXISTE en metastore PostgreSQL\")\n",
    "print(\"âœ… 10 tablas registradas: ventas, compras, gastos, clientes, etc.\")\n",
    "print(\"âŒ Problema: Spark no se conecta correctamente al metastore\")\n",
    "\n",
    "print(\"\\nğŸ§ª Probando conectividad desde Spark:\")\n",
    "\n",
    "try:\n",
    "    # Forzar reconexiÃ³n al metastore\n",
    "    spark.sql(\"REFRESH\")\n",
    "    \n",
    "    # Intentar mostrar bases de datos\n",
    "    print(\"\\nğŸ“Š Bases de datos desde Spark:\")\n",
    "    databases = spark.sql(\"SHOW DATABASES\")\n",
    "    databases.show()\n",
    "    \n",
    "    # Contar cuÃ¡ntas bases de datos ve Spark\n",
    "    db_count = databases.count()\n",
    "    print(f\"ğŸ”¢ Spark ve {db_count} base(s) de datos\")\n",
    "    \n",
    "    if db_count >= 2:\n",
    "        print(\"âœ… Spark ve ambas bases de datos (default + educacionit)\")\n",
    "        \n",
    "        # Intentar usar educacionit\n",
    "        print(\"\\nğŸ¯ Intentando usar educacionit:\")\n",
    "        spark.sql(\"USE educacionit\")\n",
    "        \n",
    "        # Mostrar tablas\n",
    "        print(\"\\nğŸ“‹ Tablas en educacionit:\")\n",
    "        tables = spark.sql(\"SHOW TABLES\")\n",
    "        tables.show()\n",
    "        \n",
    "        print(\"ğŸ‰ Â¡PROBLEMA RESUELTO! Spark conectado correctamente\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ Spark solo ve 1 base de datos (default)\")\n",
    "        print(\"ğŸ’¡ Necesitamos reiniciar la sesiÃ³n Spark con configuraciÃ³n correcta\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error en conectividad: {e}\")\n",
    "    print(\"ğŸ’¡ Necesitamos verificar configuraciÃ³n de hive-site.xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f65ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ CONFIGURACIÃ“N EXITOSA COPIADA DE spark_cluster_final\n",
    "# Esta configuraciÃ³n SÃ FUNCIONA con Hive\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "print(\"ğŸ”„ Recreando Spark con configuraciÃ³n EXITOSA...\")\n",
    "\n",
    "# Detener sesiÃ³n actual\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"ğŸ›‘ SesiÃ³n anterior detenida\")\n",
    "    time.sleep(5)\n",
    "except:\n",
    "    print(\"ğŸ” No habÃ­a sesiÃ³n activa\")\n",
    "\n",
    "# CONFIGURACIÃ“N EXACTA QUE FUNCIONA EN spark_cluster_final\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EducacionIT-Cluster-Final-Copy\") \\\n",
    "    .master(\"spark://master:7077\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"ğŸ‰ Â¡SesiÃ³n Spark creada con configuraciÃ³n exitosa!\")\n",
    "print(f\"ğŸ“Š Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "print(f\"ğŸ—„ï¸ CatÃ¡logo: {spark.conf.get('spark.sql.catalogImplementation')}\")\n",
    "print(f\"ğŸ·ï¸ AplicaciÃ³n: {spark.sparkContext.appName}\")\n",
    "print(f\"ğŸ”— Master: {spark.sparkContext.master}\")\n",
    "\n",
    "# TEST INMEDIATO\n",
    "print(\"\\nğŸ§ª Test inmediato con configuraciÃ³n exitosa:\")\n",
    "try:\n",
    "    databases = spark.sql(\"SHOW DATABASES\")\n",
    "    print(\"ğŸ“‹ Bases de datos disponibles:\")\n",
    "    databases.show()\n",
    "    \n",
    "    db_count = databases.count()\n",
    "    if db_count >= 2:\n",
    "        print(\"ğŸ‰ Â¡Ã‰XITO! Spark ve ambas bases de datos\")\n",
    "        \n",
    "        spark.sql(\"USE educacionit\")\n",
    "        tables = spark.sql(\"SHOW TABLES\")\n",
    "        print(\"ğŸ“Š Tablas en educacionit:\")\n",
    "        tables.show()\n",
    "        \n",
    "        print(\"ğŸš€ Â¡CONFIGURACIÃ“N REPLICADA EXITOSAMENTE!\")\n",
    "    else:\n",
    "        print(\"âŒ AÃºn hay problemas de conectividad\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0808f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112ad290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ RECREAR BASE DE DATOS EDUCACIONIT DESDE SPARK\n",
    "# Como HiveServer2 tiene problemas, vamos a recrear todo desde Spark\n",
    "\n",
    "print(\"ğŸ”§ RECREANDO BASE DE DATOS EDUCACIONIT...\")\n",
    "\n",
    "try:\n",
    "    # Crear la base de datos educacionit\n",
    "    print(\"ğŸ“Š Creando base de datos 'educacionit'...\")\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS educacionit\")\n",
    "    print(\"âœ… Base de datos 'educacionit' creada\")\n",
    "    \n",
    "    # Usar la base de datos\n",
    "    spark.sql(\"USE educacionit\")\n",
    "    print(\"âœ… Usando base de datos 'educacionit'\")\n",
    "    \n",
    "    # Verificar que funciona\n",
    "    print(\"\\nğŸ“‹ Verificando bases de datos disponibles:\")\n",
    "    databases = spark.sql(\"SHOW DATABASES\")\n",
    "    databases.show()\n",
    "    \n",
    "    print(\"\\nğŸ“‹ Verificando tablas en educacionit:\")\n",
    "    tables = spark.sql(\"SHOW TABLES\")\n",
    "    tables.show()\n",
    "    \n",
    "    print(\"ğŸ‰ Â¡BASE DE DATOS EDUCACIONIT RECREADA!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error recreando base de datos: {e}\")\n",
    "    print(\"ğŸ’¡ Intentando crear tablas de ejemplo...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
