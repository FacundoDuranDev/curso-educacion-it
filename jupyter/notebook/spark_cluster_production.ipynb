{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b3a5302",
   "metadata": {},
   "source": [
    "# 🚀 Spark Cluster - Configuración de Producción\n",
    "\n",
    "## 📋 Configuración basada en Spark 3.5.3 - Documentación Oficial\n",
    "\n",
    "### ⚠️ Requisitos mínimos identificados:\n",
    "- **Driver**: Mínimo 471MB (450MB + overhead)\n",
    "- **Executor**: Mínimo 471MB (450MB + overhead) \n",
    "- **Workers**: Deben tener suficiente memoria para el overhead del sistema + executors\n",
    "\n",
    "### 📊 Recursos actuales disponibles:\n",
    "- **Workers**: 2GB cada uno\n",
    "- **JupyterLab**: 2GB\n",
    "- **Overhead sistema**: ~1GB por worker\n",
    "- **Disponible para Spark**: ~1GB por worker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d03232a",
   "metadata": {},
   "source": [
    "## 🔧 Configuración 1: Cluster Mode Conservador\n",
    "### Solo 1 executor total para garantizar funcionamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d567db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 CONFIGURACIÓN CLUSTER CONSERVADORA - 1 EXECUTOR TOTAL\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Detener cualquier sesión existente\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"🛑 Sesión anterior detenida\")\n",
    "    time.sleep(5)\n",
    "except:\n",
    "    print(\"🔍 No había sesión activa\")\n",
    "\n",
    "print(\"🔄 Creando sesión Spark CLUSTER MODE - Configuración conservadora...\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EducacionIT-Cluster-Conservative\") \\\n",
    "    .master(\"spark://master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"300m\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"✅ Spark iniciado: {spark.version}\")\n",
    "print(f\"🔗 Master: {spark.sparkContext.master}\")\n",
    "print(f\"📊 Spark UI: http://localhost:4041\")\n",
    "print(f\"🎯 App ID: {spark.sparkContext.applicationId}\")\n",
    "\n",
    "print(\"\\n📊 CONFIGURACIÓN CONSERVADORA:\")\n",
    "print(\"   ✅ 1 executor con 500MB (cumple mínimo de 471MB)\")\n",
    "print(\"   ✅ Driver con 1GB (excede mínimo de 471MB)\")\n",
    "print(\"   ✅ Total memoria usada: 1GB\")\n",
    "print(\"   ✅ Debería funcionar sin problemas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7822190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 TEST BÁSICO DE FUNCIONAMIENTO\n",
    "print(\"🧪 PROBANDO CONECTIVIDAD DEL CLUSTER:\")\n",
    "\n",
    "try:\n",
    "    # Test 1: Operación RDD simple\n",
    "    print(\"\\n1️⃣ Test RDD básico:\")\n",
    "    rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 2)\n",
    "    result = rdd.sum()\n",
    "    print(f\"   ✅ Suma distribuida: {result}\")\n",
    "    \n",
    "    # Test 2: DataFrame\n",
    "    print(\"\\n2️⃣ Test DataFrame:\")\n",
    "    from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "    \n",
    "    data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\"), (4, \"Diana\")]\n",
    "    schema = StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    count = df.count()\n",
    "    print(f\"   ✅ Conteo distribuido: {count} filas\")\n",
    "    \n",
    "    # Test 3: Mostrar datos\n",
    "    print(\"\\n3️⃣ Datos de muestra:\")\n",
    "    df.show()\n",
    "    \n",
    "    print(\"\\n🎉 ¡CLUSTER MODE FUNCIONANDO CORRECTAMENTE!\")\n",
    "    print(\"💡 El executor está procesando trabajos distribuidos\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error en cluster mode: {e}\")\n",
    "    print(\"🔧 Revisa los logs de los workers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28532cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🗄️ TEST DE HIVE EN CLUSTER MODE\n",
    "print(\"🗄️ PROBANDO CONECTIVIDAD CON HIVE:\")\n",
    "\n",
    "try:\n",
    "    # Verificar bases de datos\n",
    "    print(\"\\n1️⃣ Verificando bases de datos:\")\n",
    "    databases = spark.sql(\"SHOW DATABASES\").collect()\n",
    "    print(f\"   ✅ Bases de datos encontradas: {len(databases)}\")\n",
    "    for db in databases:\n",
    "        print(f\"      - {db[0]}\")\n",
    "    \n",
    "    # Conectar a educacionit\n",
    "    print(\"\\n2️⃣ Conectando a educacionit:\")\n",
    "    spark.sql(\"USE educacionit\")\n",
    "    print(\"   ✅ Conectado a educacionit\")\n",
    "    \n",
    "    # Listar tablas\n",
    "    print(\"\\n3️⃣ Listando tablas:\")\n",
    "    tables = spark.sql(\"SHOW TABLES\").collect()\n",
    "    print(f\"   ✅ Tablas encontradas: {len(tables)}\")\n",
    "    for table in tables:\n",
    "        print(f\"      - {table[1]}\")\n",
    "    \n",
    "    # Test de lectura si hay tablas\n",
    "    if len(tables) > 0:\n",
    "        table_name = tables[0][1]\n",
    "        print(f\"\\n4️⃣ Leyendo datos de {table_name}:\")\n",
    "        df = spark.sql(f\"SELECT * FROM {table_name} LIMIT 3\")\n",
    "        print(\"   📊 Primeras 3 filas:\")\n",
    "        df.show()\n",
    "        \n",
    "        count = spark.sql(f\"SELECT COUNT(*) as total FROM {table_name}\").collect()[0][0]\n",
    "        print(f\"   ✅ Total de registros: {count}\")\n",
    "    \n",
    "    print(\"\\n🎉 ¡HIVE + CLUSTER MODE FUNCIONANDO PERFECTAMENTE!\")\n",
    "    print(\"💡 Puedes ejecutar consultas SQL distribuidas\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error con Hive: {e}\")\n",
    "    print(\"🔧 Verifica que Hive esté funcionando correctamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae97a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🛑 MODO LOCAL COMO ALTERNATIVA\n",
    "# Si el cluster sigue fallando, usemos modo local con conectividad a Hive\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Detener cualquier sesión existente\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"🛑 Sesión anterior detenida\")\n",
    "    time.sleep(5)\n",
    "except:\n",
    "    print(\"🔍 No había sesión activa\")\n",
    "\n",
    "print(\"🔄 Creando sesión Spark MODO LOCAL con Hive...\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EducacionIT-Local-Hive\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.driver.cores\", \"2\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"✅ Spark iniciado: {spark.version}\")\n",
    "print(f\"🔗 Master: {spark.sparkContext.master}\")\n",
    "print(f\"📊 Spark UI: http://localhost:4041\")\n",
    "print(f\"🎯 App ID: {spark.sparkContext.applicationId}\")\n",
    "\n",
    "print(\"\\n📊 MODO LOCAL CON HIVE:\")\n",
    "print(\"   ✅ Driver con 1GB\")\n",
    "print(\"   ✅ 2 cores locales\")\n",
    "print(\"   ✅ Conectividad completa a Hive\")\n",
    "print(\"   ✅ Funciona sin problemas de workers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fd3c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 CONFIGURACIÓN ULTRA SIMPLE QUE FUNCIONA\n",
    "# Recursos liberados - ahora probemos con configuración mínima\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Detener cualquier sesión existente\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"🛑 Sesión anterior detenida\")\n",
    "    time.sleep(5)\n",
    "except:\n",
    "    print(\"🔍 No había sesión activa\")\n",
    "\n",
    "print(\"🔄 Creando sesión Spark ULTRA SIMPLE...\")\n",
    "\n",
    "# Configuración mínima pero funcional\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EducacionIT-Simple\") \\\n",
    "    .master(\"spark://master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"800m\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"800m\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .config(\"spark.network.timeout\", \"300s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"✅ Spark iniciado: {spark.version}\")\n",
    "print(f\"🔗 Master: {spark.sparkContext.master}\")\n",
    "print(f\"📊 Spark UI: http://localhost:4041\")\n",
    "print(f\"🎯 App ID: {spark.sparkContext.applicationId}\")\n",
    "\n",
    "print(\"\\n📊 CONFIGURACIÓN ULTRA SIMPLE:\")\n",
    "print(\"   ✅ 1 executor con 800MB (muy por encima del mínimo)\")\n",
    "print(\"   ✅ Driver con 800MB (muy por encima del mínimo)\")\n",
    "print(\"   ✅ Timeouts aumentados para evitar desconexiones\")\n",
    "print(\"   ✅ Recursos disponibles: 4GB totales\")\n",
    "\n",
    "# Test inmediato\n",
    "print(\"\\n🧪 Test rápido:\")\n",
    "try:\n",
    "    rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "    result = rdd.sum()\n",
    "    print(f\"   ✅ Test exitoso - Suma: {result}\")\n",
    "    print(\"   🎉 ¡CLUSTER MODE FUNCIONANDO!\")\n",
    "except Exception as e:\n",
    "    print(f\"   ❌ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4359312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🗄️ PRUEBA DE CONECTIVIDAD CON HIVE\n",
    "# Ahora que el cluster funciona, probemos Hive\n",
    "\n",
    "print(\"🔍 Probando conectividad con Hive...\")\n",
    "\n",
    "try:\n",
    "    # Mostrar bases de datos disponibles\n",
    "    print(\"\\n📊 Bases de datos disponibles:\")\n",
    "    databases = spark.sql(\"SHOW DATABASES\")\n",
    "    databases.show()\n",
    "    \n",
    "    # Usar la base de datos educacionit\n",
    "    print(\"\\n🎯 Usando base de datos 'educacionit'...\")\n",
    "    spark.sql(\"USE educacionit\")\n",
    "    print(\"✅ Base de datos 'educacionit' seleccionada\")\n",
    "    \n",
    "    # Mostrar tablas disponibles\n",
    "    print(\"\\n📋 Tablas disponibles en 'educacionit':\")\n",
    "    tables = spark.sql(\"SHOW TABLES\")\n",
    "    tables.show()\n",
    "    \n",
    "    print(\"🎉 ¡CONECTIVIDAD CON HIVE EXITOSA!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error conectando con Hive: {e}\")\n",
    "    print(\"💡 Verificando si necesitamos habilitar Hive Support...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0261bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 SPARK CLUSTER CON HIVE SUPPORT EXPLÍCITO\n",
    "# Si la celda anterior falló, probemos con Hive Support habilitado\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "print(\"🔄 Recreando sesión Spark con Hive Support explícito...\")\n",
    "\n",
    "# Detener sesión actual\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"🛑 Sesión anterior detenida\")\n",
    "    time.sleep(5)\n",
    "except:\n",
    "    print(\"🔍 No había sesión activa\")\n",
    "\n",
    "# Crear nueva sesión con Hive Support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EducacionIT-Cluster-Hive\") \\\n",
    "    .master(\"spark://master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"800m\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"800m\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .config(\"spark.network.timeout\", \"300s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"✅ Spark con Hive iniciado: {spark.version}\")\n",
    "print(f\"🔗 Master: {spark.sparkContext.master}\")\n",
    "print(f\"📊 Spark UI: http://localhost:4041\")\n",
    "\n",
    "# Test inmediato con Hive\n",
    "print(\"\\n🧪 Test de conectividad con Hive:\")\n",
    "try:\n",
    "    databases = spark.sql(\"SHOW DATABASES\")\n",
    "    databases.show()\n",
    "    print(\"🎉 ¡HIVE FUNCIONANDO EN CLUSTER MODE!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 PRUEBAS AVANZADAS CON DATOS DE EDUCACIONIT\n",
    "# Consultas distribuidas sobre las tablas de Hive\n",
    "\n",
    "print(\"🔍 Probando consultas sobre datos de EducacionIT...\")\n",
    "\n",
    "try:\n",
    "    # Usar la base de datos educacionit\n",
    "    spark.sql(\"USE educacionit\")\n",
    "    print(\"✅ Usando base de datos 'educacionit'\")\n",
    "    \n",
    "    # Contar registros en tabla clientes\n",
    "    print(\"\\n👥 Contando clientes:\")\n",
    "    clientes_count = spark.sql(\"SELECT COUNT(*) as total_clientes FROM clientes\")\n",
    "    clientes_count.show()\n",
    "    \n",
    "    # Mostrar algunas ventas\n",
    "    print(\"\\n💰 Mostrando primeras 5 ventas:\")\n",
    "    ventas_sample = spark.sql(\"SELECT * FROM venta LIMIT 5\")\n",
    "    ventas_sample.show()\n",
    "    \n",
    "    # Consulta agregada: ventas por canal\n",
    "    print(\"\\n📈 Ventas totales por canal:\")\n",
    "    ventas_por_canal = spark.sql(\"\"\"\n",
    "        SELECT c.descripcion as canal, \n",
    "               COUNT(*) as num_ventas,\n",
    "               SUM(v.precio * v.cantidad) as total_ventas\n",
    "        FROM venta v \n",
    "        JOIN canaldeventa c ON v.idcanal = c.idcanal \n",
    "        GROUP BY c.descripcion \n",
    "        ORDER BY total_ventas DESC\n",
    "    \"\"\")\n",
    "    ventas_por_canal.show()\n",
    "    \n",
    "    print(\"🎉 ¡CONSULTAS DISTRIBUIDAS FUNCIONANDO!\")\n",
    "    print(\"🚀 ¡CLUSTER SPARK + HIVE COMPLETAMENTE OPERATIVO!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error en consultas: {e}\")\n",
    "    print(\"💡 Verificando estructura de tablas...\")\n",
    "    try:\n",
    "        spark.sql(\"SHOW TABLES\").show()\n",
    "    except:\n",
    "        print(\"❌ No se pudo acceder a las tablas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf5d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚨 ARREGLO INMEDIATO - CONFIGURACIÓN CORRECTA\n",
    "# El executor volvió a 300MB, necesitamos restaurar los 800MB que funcionaban\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "print(\"🚨 ARREGLANDO CONFIGURACIÓN DE MEMORIA...\")\n",
    "print(\"❌ Problema: Executor con solo 300MB (necesita mínimo 471MB)\")\n",
    "print(\"✅ Solución: Restaurar configuración de 800MB que funcionaba\")\n",
    "\n",
    "# DETENER CUALQUIER SESIÓN EXISTENTE\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"🛑 Sesión problemática detenida\")\n",
    "    time.sleep(8)  # Esperar más tiempo para limpieza completa\n",
    "except:\n",
    "    print(\"🔍 No había sesión activa\")\n",
    "\n",
    "print(\"\\n🔧 RECREANDO SPARK CON CONFIGURACIÓN CORRECTA...\")\n",
    "\n",
    "# CONFIGURACIÓN EXACTA QUE FUNCIONABA EN CELDA 6\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EducacionIT-Fixed-800MB\") \\\n",
    "    .master(\"spark://master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"800m\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"800m\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .config(\"spark.network.timeout\", \"300s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"✅ Spark restaurado: {spark.version}\")\n",
    "print(f\"🔗 Master: {spark.sparkContext.master}\")\n",
    "print(f\"📊 Spark UI: http://localhost:4041\")\n",
    "print(f\"🎯 App ID: {spark.sparkContext.applicationId}\")\n",
    "\n",
    "print(\"\\n📊 CONFIGURACIÓN RESTAURADA:\")\n",
    "print(\"   ✅ Executor: 800MB (muy por encima del mínimo de 471MB)\")\n",
    "print(\"   ✅ Driver: 800MB\")\n",
    "print(\"   ✅ Hive Support habilitado\")\n",
    "\n",
    "# TEST INMEDIATO PARA CONFIRMAR\n",
    "print(\"\\n🧪 Test de confirmación:\")\n",
    "try:\n",
    "    rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "    result = rdd.sum()\n",
    "    print(f\"   ✅ RDD Test exitoso - Suma: {result}\")\n",
    "    \n",
    "    # Test Hive\n",
    "    databases = spark.sql(\"SHOW DATABASES\")\n",
    "    print(f\"   ✅ Hive Test exitoso - Bases de datos encontradas\")\n",
    "    databases.show()\n",
    "    \n",
    "    print(\"🎉 ¡CONFIGURACIÓN ARREGLADA Y FUNCIONANDO!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ❌ Error: {e}\")\n",
    "    print(\"   🔄 Intentando nuevamente...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd217dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 DIAGNÓSTICO: LOS DATOS EXISTEN EN EL METASTORE\n",
    "# Confirmamos que educacionit y sus 10 tablas están en PostgreSQL\n",
    "# El problema es la conectividad Spark <-> Metastore\n",
    "\n",
    "print(\"🔍 DIAGNÓSTICO COMPLETO:\")\n",
    "print(\"✅ Base de datos 'educacionit' EXISTE en metastore PostgreSQL\")\n",
    "print(\"✅ 10 tablas registradas: ventas, compras, gastos, clientes, etc.\")\n",
    "print(\"❌ Problema: Spark no se conecta correctamente al metastore\")\n",
    "\n",
    "print(\"\\n🧪 Probando conectividad desde Spark:\")\n",
    "\n",
    "try:\n",
    "    # Forzar reconexión al metastore\n",
    "    spark.sql(\"REFRESH\")\n",
    "    \n",
    "    # Intentar mostrar bases de datos\n",
    "    print(\"\\n📊 Bases de datos desde Spark:\")\n",
    "    databases = spark.sql(\"SHOW DATABASES\")\n",
    "    databases.show()\n",
    "    \n",
    "    # Contar cuántas bases de datos ve Spark\n",
    "    db_count = databases.count()\n",
    "    print(f\"🔢 Spark ve {db_count} base(s) de datos\")\n",
    "    \n",
    "    if db_count >= 2:\n",
    "        print(\"✅ Spark ve ambas bases de datos (default + educacionit)\")\n",
    "        \n",
    "        # Intentar usar educacionit\n",
    "        print(\"\\n🎯 Intentando usar educacionit:\")\n",
    "        spark.sql(\"USE educacionit\")\n",
    "        \n",
    "        # Mostrar tablas\n",
    "        print(\"\\n📋 Tablas en educacionit:\")\n",
    "        tables = spark.sql(\"SHOW TABLES\")\n",
    "        tables.show()\n",
    "        \n",
    "        print(\"🎉 ¡PROBLEMA RESUELTO! Spark conectado correctamente\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ Spark solo ve 1 base de datos (default)\")\n",
    "        print(\"💡 Necesitamos reiniciar la sesión Spark con configuración correcta\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error en conectividad: {e}\")\n",
    "    print(\"💡 Necesitamos verificar configuración de hive-site.xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f65ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 CONFIGURACIÓN EXITOSA COPIADA DE spark_cluster_final\n",
    "# Esta configuración SÍ FUNCIONA con Hive\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "print(\"🔄 Recreando Spark con configuración EXITOSA...\")\n",
    "\n",
    "# Detener sesión actual\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"🛑 Sesión anterior detenida\")\n",
    "    time.sleep(5)\n",
    "except:\n",
    "    print(\"🔍 No había sesión activa\")\n",
    "\n",
    "# CONFIGURACIÓN EXACTA QUE FUNCIONA EN spark_cluster_final\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EducacionIT-Cluster-Final-Copy\") \\\n",
    "    .master(\"spark://master:7077\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"🎉 ¡Sesión Spark creada con configuración exitosa!\")\n",
    "print(f\"📊 Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "print(f\"🗄️ Catálogo: {spark.conf.get('spark.sql.catalogImplementation')}\")\n",
    "print(f\"🏷️ Aplicación: {spark.sparkContext.appName}\")\n",
    "print(f\"🔗 Master: {spark.sparkContext.master}\")\n",
    "\n",
    "# TEST INMEDIATO\n",
    "print(\"\\n🧪 Test inmediato con configuración exitosa:\")\n",
    "try:\n",
    "    databases = spark.sql(\"SHOW DATABASES\")\n",
    "    print(\"📋 Bases de datos disponibles:\")\n",
    "    databases.show()\n",
    "    \n",
    "    db_count = databases.count()\n",
    "    if db_count >= 2:\n",
    "        print(\"🎉 ¡ÉXITO! Spark ve ambas bases de datos\")\n",
    "        \n",
    "        spark.sql(\"USE educacionit\")\n",
    "        tables = spark.sql(\"SHOW TABLES\")\n",
    "        print(\"📊 Tablas en educacionit:\")\n",
    "        tables.show()\n",
    "        \n",
    "        print(\"🚀 ¡CONFIGURACIÓN REPLICADA EXITOSAMENTE!\")\n",
    "    else:\n",
    "        print(\"❌ Aún hay problemas de conectividad\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0808f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112ad290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 RECREAR BASE DE DATOS EDUCACIONIT DESDE SPARK\n",
    "# Como HiveServer2 tiene problemas, vamos a recrear todo desde Spark\n",
    "\n",
    "print(\"🔧 RECREANDO BASE DE DATOS EDUCACIONIT...\")\n",
    "\n",
    "try:\n",
    "    # Crear la base de datos educacionit\n",
    "    print(\"📊 Creando base de datos 'educacionit'...\")\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS educacionit\")\n",
    "    print(\"✅ Base de datos 'educacionit' creada\")\n",
    "    \n",
    "    # Usar la base de datos\n",
    "    spark.sql(\"USE educacionit\")\n",
    "    print(\"✅ Usando base de datos 'educacionit'\")\n",
    "    \n",
    "    # Verificar que funciona\n",
    "    print(\"\\n📋 Verificando bases de datos disponibles:\")\n",
    "    databases = spark.sql(\"SHOW DATABASES\")\n",
    "    databases.show()\n",
    "    \n",
    "    print(\"\\n📋 Verificando tablas en educacionit:\")\n",
    "    tables = spark.sql(\"SHOW TABLES\")\n",
    "    tables.show()\n",
    "    \n",
    "    print(\"🎉 ¡BASE DE DATOS EDUCACIONIT RECREADA!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error recreando base de datos: {e}\")\n",
    "    print(\"💡 Intentando crear tablas de ejemplo...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
