{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸš€ Apache Spark 3.5.3 - ConfiguraciÃ³n Profesional de Cluster\n",
        "\n",
        "## ğŸ“‹ **GuÃ­a Completa para ConfiguraciÃ³n y AnÃ¡lisis Distribuido**\n",
        "\n",
        "### ğŸ¯ **Objetivos del Notebook:**\n",
        "1. **Configurar correctamente** Apache Spark 3.5.3 en modo cluster\n",
        "2. **Integrar con Apache Hive** para anÃ¡lisis SQL distribuido\n",
        "3. **Implementar mejores prÃ¡cticas** de configuraciÃ³n y optimizaciÃ³n\n",
        "4. **Realizar anÃ¡lisis de datos** del mundo real con rendimiento Ã³ptimo\n",
        "\n",
        "### ğŸ—ï¸ **Arquitectura del Cluster:**\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚   JupyterLab    â”‚    â”‚     Master      â”‚    â”‚   Worker 1&2    â”‚\n",
        "â”‚  (Spark Driver) â”‚â—„â”€â”€â–ºâ”‚ (Cluster Mgr)   â”‚â—„â”€â”€â–ºâ”‚   (Executors)   â”‚\n",
        "â”‚   Spark 3.5.3   â”‚    â”‚  Spark 3.5.3    â”‚    â”‚  Spark 3.5.3    â”‚\n",
        "â”‚     2GB RAM     â”‚    â”‚    2GB RAM      â”‚    â”‚   2GB RAM c/u   â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "         â”‚                       â”‚                       â”‚\n",
        "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                                 â”‚\n",
        "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "                    â”‚   Hive Metastore â”‚\n",
        "                    â”‚   (PostgreSQL)   â”‚\n",
        "                    â”‚    Datos: HDFS   â”‚\n",
        "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ“Š **Recursos y ConfiguraciÃ³n:**\n",
        "- **Total RAM disponible:** 8GB (2GB Ã— 4 contenedores)\n",
        "- **ConfiguraciÃ³n recomendada:** Driver 1.2GB + Executors 800MB c/u\n",
        "- **Overhead JVM:** ~20% reservado para gestiÃ³n de memoria\n",
        "- **Cores totales:** 8 (2 cores Ã— 4 contenedores)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“š **Fundamentos de Apache Spark 3.5.3**\n",
        "\n",
        "### ğŸ” **Â¿QuÃ© es Apache Spark?**\n",
        "\n",
        "Apache Spark es un **motor de anÃ¡lisis distribuido** diseÃ±ado para procesar grandes volÃºmenes de datos de manera eficiente. A diferencia de Hadoop MapReduce, Spark mantiene los datos en memoria entre operaciones, lo que resulta en un rendimiento hasta **100x mÃ¡s rÃ¡pido**.\n",
        "\n",
        "### ğŸ›ï¸ **Arquitectura de Spark:**\n",
        "\n",
        "#### **1. Driver Program (Conductor)**\n",
        "- **FunciÃ³n:** Coordina la ejecuciÃ³n de la aplicaciÃ³n Spark\n",
        "- **Responsabilidades:** \n",
        "  - Crear SparkContext\n",
        "  - Planificar tareas\n",
        "  - Distribuir cÃ³digo a executors\n",
        "  - Recolectar resultados\n",
        "\n",
        "#### **2. Cluster Manager (Gestor de Cluster)**\n",
        "- **FunciÃ³n:** Gestiona recursos del cluster\n",
        "- **Tipos:** Spark Standalone, YARN, Kubernetes, Mesos\n",
        "- **En nuestro caso:** Spark Standalone Manager\n",
        "\n",
        "#### **3. Executors (Ejecutores)**\n",
        "- **FunciÃ³n:** Procesan datos y ejecutan tareas\n",
        "- **CaracterÃ­sticas:**\n",
        "  - Corren en nodos worker\n",
        "  - Mantienen datos en memoria/disco\n",
        "  - Ejecutan tareas en paralelo\n",
        "\n",
        "### ğŸš€ **Ventajas de Spark 3.5.3:**\n",
        "\n",
        "1. **Adaptive Query Execution (AQE):** OptimizaciÃ³n dinÃ¡mica de consultas\n",
        "2. **Dynamic Partition Pruning:** Filtrado inteligente de particiones\n",
        "3. **Columnar Storage Support:** Mejor rendimiento con formatos como Parquet\n",
        "4. **Improved Catalyst Optimizer:** Optimizador de consultas mÃ¡s inteligente\n",
        "5. **Better Memory Management:** GestiÃ³n mÃ¡s eficiente de memoria\n",
        "\n",
        "### ğŸ“ˆ **Casos de Uso TÃ­picos:**\n",
        "- **ETL (Extract, Transform, Load):** Procesamiento de datos batch\n",
        "- **AnÃ¡lisis en tiempo real:** Streaming de datos\n",
        "- **Machine Learning:** MLlib para algoritmos distribuidos\n",
        "- **AnÃ¡lisis interactivo:** Consultas SQL sobre Big Data\n",
        "- **Graph Processing:** AnÃ¡lisis de redes y grafos\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# ğŸ“¦ ImportaciÃ³n de LibrerÃ­as y ConfiguraciÃ³n Inicial\n",
        "\n",
        "import findspark\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "\n",
        "# Configurar findspark para localizar Spark\n",
        "findspark.init('/opt/spark')\n",
        "\n",
        "# Importar librerÃ­as de PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Suprimir warnings para una salida mÃ¡s limpia\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# InformaciÃ³n del entorno\n",
        "print(\"ğŸ”§ CONFIGURACIÃ“N INICIAL COMPLETADA\")\n",
        "print(\"=\"*50)\n",
        "print(f\"ğŸ“ Spark Home: {os.environ.get('SPARK_HOME', '/opt/spark')}\")\n",
        "print(f\"ğŸ Python Version: {sys.version.split()[0]}\")\n",
        "print(f\"ğŸ–¥ï¸ Hostname: {os.environ.get('HOSTNAME', 'jupyterlab')}\")\n",
        "print(f\"â° Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "print(\"\\nâœ… LibrerÃ­as importadas correctamente\")\n",
        "print(\"ğŸš€ Listo para crear sesiÃ³n Spark...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# ğŸš€ CREACIÃ“N DE SESIÃ“N SPARK - CONFIGURACIÃ“N OPTIMIZADA DE RECURSOS\n",
        "\n",
        "print(\"ğŸ”„ Iniciando creaciÃ³n de sesiÃ³n Spark...\")\n",
        "print(\"âš™ï¸ Aplicando configuraciÃ³n optimizada de recursos\")\n",
        "\n",
        "# Detener cualquier sesiÃ³n existente\n",
        "try:\n",
        "    spark.stop()\n",
        "    print(\"ğŸ›‘ SesiÃ³n anterior detenida correctamente\")\n",
        "except:\n",
        "    print(\"ğŸ” No habÃ­a sesiÃ³n activa\")\n",
        "\n",
        "# CONFIGURACIÃ“N DE RECURSOS OPTIMIZADA\n",
        "# Basada en 8GB RAM total disponible (2GB x 4 contenedores)\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"EducacionIT-Spark-Professional-v3.5.3\") \\\n",
        "    .master(\"spark://master:7077\") \\\n",
        "    .config(\"spark.driver.memory\", \"1200m\") \\\n",
        "    .config(\"spark.driver.cores\", \"2\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"500m\") \\\n",
        "    .config(\"spark.executor.memory\", \"800m\") \\\n",
        "    .config(\"spark.executor.cores\", \"1\") \\\n",
        "    .config(\"spark.executor.instances\", \"2\") \\\n",
        "    .config(\"spark.executor.memoryFraction\", \"0.8\") \\\n",
        "    .config(\"spark.executor.memoryStorageFraction\", \"0.5\") \\\n",
        "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
        "    .config(\"spark.shuffle.service.enabled\", \"false\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
        "    .config(\"spark.network.timeout\", \"300s\") \\\n",
        "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Configurar nivel de logging\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "# InformaciÃ³n de la sesiÃ³n creada\n",
        "print(\"\\nğŸ‰ Â¡SESIÃ“N SPARK CREADA EXITOSAMENTE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"ğŸ·ï¸  AplicaciÃ³n: {spark.sparkContext.appName}\")\n",
        "print(f\"ğŸ”— Master URL: {spark.sparkContext.master}\")\n",
        "print(f\"ğŸ“Š Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
        "print(f\"ğŸ¯ App ID: {spark.sparkContext.applicationId}\")\n",
        "print(f\"ğŸ—„ï¸ CatÃ¡logo: {spark.conf.get('spark.sql.catalogImplementation')}\")\n",
        "print(f\"ğŸš€ VersiÃ³n Spark: {spark.version}\")\n",
        "\n",
        "# Verificar configuraciÃ³n de recursos aplicada\n",
        "print(\"\\nğŸ“‹ CONFIGURACIÃ“N DE RECURSOS:\")\n",
        "print(f\"   ğŸ’¾ Driver Memory: {spark.conf.get('spark.driver.memory')}\")\n",
        "print(f\"   ğŸ–¥ï¸ Driver Cores: {spark.conf.get('spark.driver.cores')}\")\n",
        "print(f\"   ğŸ“Š Max Result Size: {spark.conf.get('spark.driver.maxResultSize')}\")\n",
        "print(f\"   ğŸ’¾ Executor Memory: {spark.conf.get('spark.executor.memory')}\")\n",
        "print(f\"   ğŸ–¥ï¸ Executor Cores: {spark.conf.get('spark.executor.cores')}\")\n",
        "print(f\"   ğŸ“Š Executor Instances: {spark.conf.get('spark.executor.instances')}\")\n",
        "print(f\"   ğŸ”„ Adaptive Query: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
        "print(f\"   ğŸ“¦ Shuffle Partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
        "\n",
        "print(\"\\nğŸŒŸ URLs de Monitoreo:\")\n",
        "print(\"   ğŸ›ï¸ Spark Master UI: http://localhost:8080\")\n",
        "print(\"   ğŸ“Š Spark Driver UI: http://localhost:4040\")\n",
        "\n",
        "print(\"\\nâœ… Spark configurado con recursos optimizados para el entorno\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“š **Tutorial Paso a Paso: Operaciones BÃ¡sicas con Spark**\n",
        "\n",
        "### ğŸ¯ **Objetivo del Tutorial:**\n",
        "Aprender las operaciones fundamentales de Apache Spark a travÃ©s de ejemplos prÃ¡cticos, desde la creaciÃ³n de DataFrames hasta consultas SQL complejas.\n",
        "\n",
        "### ğŸ“‹ **Operaciones que Cubriremos:**\n",
        "1. **Crear DataFrames** desde datos en memoria\n",
        "2. **Definir esquemas** personalizados\n",
        "3. **Operaciones de transformaciÃ³n** (select, filter, groupBy)\n",
        "4. **Operaciones de acciÃ³n** (show, collect, count)\n",
        "5. **Consultas SQL** sobre DataFrames\n",
        "6. **Joins** entre mÃºltiples DataFrames\n",
        "7. **Agregaciones** y funciones de ventana\n",
        "8. **Escritura y lectura** de datos\n",
        "\n",
        "### ğŸ—ï¸ **Estructura del Dataset de Ejemplo:**\n",
        "Crearemos un dataset empresarial simulado con las siguientes entidades:\n",
        "- **ğŸ‘¥ Empleados**: ID, nombre, departamento, salario, fecha de contrataciÃ³n\n",
        "- **ğŸ¢ Departamentos**: ID, nombre, presupuesto, ubicaciÃ³n\n",
        "- **ğŸ“Š Ventas**: ID, empleado_id, producto, monto, fecha\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# ğŸ“Š PASO 1: CREAR DATAFRAMES CON ESQUEMAS PERSONALIZADOS\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
        "from pyspark.sql.functions import col, when, lit\n",
        "from datetime import date\n",
        "import time\n",
        "\n",
        "print(\"ğŸ“Š CREANDO DATAFRAMES DE EJEMPLO...\")\n",
        "print(\"=\"*45)\n",
        "\n",
        "# 1. DEFINIR ESQUEMAS (SCHEMAS)\n",
        "print(\"\\n1ï¸âƒ£ DEFINIENDO ESQUEMAS PERSONALIZADOS:\")\n",
        "\n",
        "# Esquema para empleados\n",
        "empleados_schema = StructType([\n",
        "    StructField(\"emp_id\", IntegerType(), False),\n",
        "    StructField(\"nombre\", StringType(), False),\n",
        "    StructField(\"departamento_id\", IntegerType(), False),\n",
        "    StructField(\"salario\", DoubleType(), False),\n",
        "    StructField(\"fecha_contratacion\", DateType(), False)\n",
        "])\n",
        "\n",
        "# Esquema para departamentos\n",
        "departamentos_schema = StructType([\n",
        "    StructField(\"dept_id\", IntegerType(), False),\n",
        "    StructField(\"nombre_dept\", StringType(), False),\n",
        "    StructField(\"presupuesto\", DoubleType(), False),\n",
        "    StructField(\"ubicacion\", StringType(), False)\n",
        "])\n",
        "\n",
        "# Esquema para ventas\n",
        "ventas_schema = StructType([\n",
        "    StructField(\"venta_id\", IntegerType(), False),\n",
        "    StructField(\"emp_id\", IntegerType(), False),\n",
        "    StructField(\"producto\", StringType(), False),\n",
        "    StructField(\"monto\", DoubleType(), False),\n",
        "    StructField(\"fecha_venta\", DateType(), False)\n",
        "])\n",
        "\n",
        "print(\"âœ… Esquemas definidos correctamente\")\n",
        "\n",
        "# 2. CREAR DATOS DE EJEMPLO\n",
        "print(\"\\n2ï¸âƒ£ CREANDO DATOS DE EJEMPLO:\")\n",
        "\n",
        "# Datos de empleados\n",
        "empleados_data = [\n",
        "    (1, \"Ana GarcÃ­a\", 1, 75000.0, date(2020, 1, 15)),\n",
        "    (2, \"Carlos LÃ³pez\", 2, 85000.0, date(2019, 3, 22)),\n",
        "    (3, \"MarÃ­a RodrÃ­guez\", 1, 70000.0, date(2021, 6, 10)),\n",
        "    (4, \"Juan PÃ©rez\", 3, 90000.0, date(2018, 9, 5)),\n",
        "    (5, \"Laura MartÃ­n\", 2, 82000.0, date(2020, 11, 18)),\n",
        "    (6, \"Pedro SÃ¡nchez\", 3, 95000.0, date(2017, 4, 12)),\n",
        "    (7, \"Isabel Torres\", 1, 73000.0, date(2021, 2, 28)),\n",
        "    (8, \"Miguel Ruiz\", 2, 88000.0, date(2019, 8, 14))\n",
        "]\n",
        "\n",
        "# Datos de departamentos\n",
        "departamentos_data = [\n",
        "    (1, \"Recursos Humanos\", 500000.0, \"Madrid\"),\n",
        "    (2, \"TecnologÃ­a\", 1200000.0, \"Barcelona\"),\n",
        "    (3, \"Ventas\", 800000.0, \"Valencia\")\n",
        "]\n",
        "\n",
        "# Datos de ventas\n",
        "ventas_data = [\n",
        "    (1, 4, \"Laptop Pro\", 1299.99, date(2023, 1, 10)),\n",
        "    (2, 6, \"Smartphone\", 899.99, date(2023, 1, 12)),\n",
        "    (3, 4, \"Tablet\", 599.99, date(2023, 1, 15)),\n",
        "    (4, 6, \"Monitor 4K\", 399.99, date(2023, 1, 18)),\n",
        "    (5, 4, \"Teclado MecÃ¡nico\", 149.99, date(2023, 1, 20)),\n",
        "    (6, 6, \"Mouse Gaming\", 79.99, date(2023, 1, 22)),\n",
        "    (7, 4, \"Webcam HD\", 129.99, date(2023, 1, 25)),\n",
        "    (8, 6, \"Auriculares\", 199.99, date(2023, 1, 28))\n",
        "]\n",
        "\n",
        "print(f\"ğŸ“‹ Empleados: {len(empleados_data)} registros\")\n",
        "print(f\"ğŸ¢ Departamentos: {len(departamentos_data)} registros\") \n",
        "print(f\"ğŸ’° Ventas: {len(ventas_data)} registros\")\n",
        "\n",
        "# 3. CREAR DATAFRAMES\n",
        "print(\"\\n3ï¸âƒ£ CREANDO DATAFRAMES:\")\n",
        "\n",
        "empleados_df = spark.createDataFrame(empleados_data, empleados_schema)\n",
        "departamentos_df = spark.createDataFrame(departamentos_data, departamentos_schema)\n",
        "ventas_df = spark.createDataFrame(ventas_data, ventas_schema)\n",
        "\n",
        "print(\"âœ… DataFrames creados exitosamente\")\n",
        "\n",
        "# 4. VERIFICAR ESTRUCTURA DE LOS DATAFRAMES\n",
        "print(\"\\n4ï¸âƒ£ VERIFICANDO ESTRUCTURA:\")\n",
        "\n",
        "print(\"\\nğŸ‘¥ ESQUEMA - EMPLEADOS:\")\n",
        "empleados_df.printSchema()\n",
        "\n",
        "print(\"\\nğŸ¢ ESQUEMA - DEPARTAMENTOS:\")\n",
        "departamentos_df.printSchema()\n",
        "\n",
        "print(\"\\nğŸ’° ESQUEMA - VENTAS:\")\n",
        "ventas_df.printSchema()\n",
        "\n",
        "# 5. MOSTRAR DATOS DE MUESTRA\n",
        "print(\"\\n5ï¸âƒ£ DATOS DE MUESTRA:\")\n",
        "\n",
        "print(\"\\nğŸ‘¥ EMPLEADOS (primeras 5 filas):\")\n",
        "empleados_df.show(5)\n",
        "\n",
        "print(\"\\nğŸ¢ DEPARTAMENTOS:\")\n",
        "departamentos_df.show()\n",
        "\n",
        "print(\"\\nğŸ’° VENTAS (primeras 5 filas):\")\n",
        "ventas_df.show(5)\n",
        "\n",
        "print(\"\\nğŸ‰ Â¡DataFrames creados y verificados exitosamente!\")\n",
        "print(\"ğŸ“Š Listos para operaciones de transformaciÃ³n y anÃ¡lisis\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
