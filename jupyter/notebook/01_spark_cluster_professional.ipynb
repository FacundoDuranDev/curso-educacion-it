{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🚀 Apache Spark 3.5.3 - Configuración Profesional de Cluster\n",
        "\n",
        "## 📋 **Guía Completa para Configuración y Análisis Distribuido**\n",
        "\n",
        "### 🎯 **Objetivos del Notebook:**\n",
        "1. **Configurar correctamente** Apache Spark 3.5.3 en modo cluster\n",
        "2. **Integrar con Apache Hive** para análisis SQL distribuido\n",
        "3. **Implementar mejores prácticas** de configuración y optimización\n",
        "4. **Realizar análisis de datos** del mundo real con rendimiento óptimo\n",
        "\n",
        "### 🏗️ **Arquitectura del Cluster:**\n",
        "\n",
        "```\n",
        "┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n",
        "│   JupyterLab    │    │     Master      │    │   Worker 1&2    │\n",
        "│  (Spark Driver) │◄──►│ (Cluster Mgr)   │◄──►│   (Executors)   │\n",
        "│   Spark 3.5.3   │    │  Spark 3.5.3    │    │  Spark 3.5.3    │\n",
        "│     2GB RAM     │    │    2GB RAM      │    │   2GB RAM c/u   │\n",
        "└─────────────────┘    └─────────────────┘    └─────────────────┘\n",
        "         │                       │                       │\n",
        "         └───────────────────────┼───────────────────────┘\n",
        "                                 │\n",
        "                    ┌─────────────────┐\n",
        "                    │   Hive Metastore │\n",
        "                    │   (PostgreSQL)   │\n",
        "                    │    Datos: HDFS   │\n",
        "                    └─────────────────┘\n",
        "```\n",
        "\n",
        "### 📊 **Recursos y Configuración:**\n",
        "- **Total RAM disponible:** 8GB (2GB × 4 contenedores)\n",
        "- **Configuración recomendada:** Driver 1.2GB + Executors 800MB c/u\n",
        "- **Overhead JVM:** ~20% reservado para gestión de memoria\n",
        "- **Cores totales:** 8 (2 cores × 4 contenedores)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📚 **Fundamentos de Apache Spark 3.5.3**\n",
        "\n",
        "### 🔍 **¿Qué es Apache Spark?**\n",
        "\n",
        "Apache Spark es un **motor de análisis distribuido** diseñado para procesar grandes volúmenes de datos de manera eficiente. A diferencia de Hadoop MapReduce, Spark mantiene los datos en memoria entre operaciones, lo que resulta en un rendimiento hasta **100x más rápido**.\n",
        "\n",
        "### 🏛️ **Arquitectura de Spark:**\n",
        "\n",
        "#### **1. Driver Program (Conductor)**\n",
        "- **Función:** Coordina la ejecución de la aplicación Spark\n",
        "- **Responsabilidades:** \n",
        "  - Crear SparkContext\n",
        "  - Planificar tareas\n",
        "  - Distribuir código a executors\n",
        "  - Recolectar resultados\n",
        "\n",
        "#### **2. Cluster Manager (Gestor de Cluster)**\n",
        "- **Función:** Gestiona recursos del cluster\n",
        "- **Tipos:** Spark Standalone, YARN, Kubernetes, Mesos\n",
        "- **En nuestro caso:** Spark Standalone Manager\n",
        "\n",
        "#### **3. Executors (Ejecutores)**\n",
        "- **Función:** Procesan datos y ejecutan tareas\n",
        "- **Características:**\n",
        "  - Corren en nodos worker\n",
        "  - Mantienen datos en memoria/disco\n",
        "  - Ejecutan tareas en paralelo\n",
        "\n",
        "### 🚀 **Ventajas de Spark 3.5.3:**\n",
        "\n",
        "1. **Adaptive Query Execution (AQE):** Optimización dinámica de consultas\n",
        "2. **Dynamic Partition Pruning:** Filtrado inteligente de particiones\n",
        "3. **Columnar Storage Support:** Mejor rendimiento con formatos como Parquet\n",
        "4. **Improved Catalyst Optimizer:** Optimizador de consultas más inteligente\n",
        "5. **Better Memory Management:** Gestión más eficiente de memoria\n",
        "\n",
        "### 📈 **Casos de Uso Típicos:**\n",
        "- **ETL (Extract, Transform, Load):** Procesamiento de datos batch\n",
        "- **Análisis en tiempo real:** Streaming de datos\n",
        "- **Machine Learning:** MLlib para algoritmos distribuidos\n",
        "- **Análisis interactivo:** Consultas SQL sobre Big Data\n",
        "- **Graph Processing:** Análisis de redes y grafos\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 📦 Importación de Librerías y Configuración Inicial\n",
        "\n",
        "import findspark\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "\n",
        "# Configurar findspark para localizar Spark\n",
        "findspark.init('/opt/spark')\n",
        "\n",
        "# Importar librerías de PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Suprimir warnings para una salida más limpia\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Información del entorno\n",
        "print(\"🔧 CONFIGURACIÓN INICIAL COMPLETADA\")\n",
        "print(\"=\"*50)\n",
        "print(f\"📍 Spark Home: {os.environ.get('SPARK_HOME', '/opt/spark')}\")\n",
        "print(f\"🐍 Python Version: {sys.version.split()[0]}\")\n",
        "print(f\"🖥️ Hostname: {os.environ.get('HOSTNAME', 'jupyterlab')}\")\n",
        "print(f\"⏰ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "print(\"\\n✅ Librerías importadas correctamente\")\n",
        "print(\"🚀 Listo para crear sesión Spark...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 🚀 CREACIÓN DE SESIÓN SPARK - CONFIGURACIÓN OPTIMIZADA DE RECURSOS\n",
        "\n",
        "print(\"🔄 Iniciando creación de sesión Spark...\")\n",
        "print(\"⚙️ Aplicando configuración optimizada de recursos\")\n",
        "\n",
        "# Detener cualquier sesión existente\n",
        "try:\n",
        "    spark.stop()\n",
        "    print(\"🛑 Sesión anterior detenida correctamente\")\n",
        "except:\n",
        "    print(\"🔍 No había sesión activa\")\n",
        "\n",
        "# CONFIGURACIÓN DE RECURSOS OPTIMIZADA\n",
        "# Basada en 8GB RAM total disponible (2GB x 4 contenedores)\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"EducacionIT-Spark-Professional-v3.5.3\") \\\n",
        "    .master(\"spark://master:7077\") \\\n",
        "    .config(\"spark.driver.memory\", \"1200m\") \\\n",
        "    .config(\"spark.driver.cores\", \"2\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"500m\") \\\n",
        "    .config(\"spark.executor.memory\", \"800m\") \\\n",
        "    .config(\"spark.executor.cores\", \"1\") \\\n",
        "    .config(\"spark.executor.instances\", \"2\") \\\n",
        "    .config(\"spark.executor.memoryFraction\", \"0.8\") \\\n",
        "    .config(\"spark.executor.memoryStorageFraction\", \"0.5\") \\\n",
        "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
        "    .config(\"spark.shuffle.service.enabled\", \"false\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
        "    .config(\"spark.network.timeout\", \"300s\") \\\n",
        "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Configurar nivel de logging\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "# Información de la sesión creada\n",
        "print(\"\\n🎉 ¡SESIÓN SPARK CREADA EXITOSAMENTE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"🏷️  Aplicación: {spark.sparkContext.appName}\")\n",
        "print(f\"🔗 Master URL: {spark.sparkContext.master}\")\n",
        "print(f\"📊 Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
        "print(f\"🎯 App ID: {spark.sparkContext.applicationId}\")\n",
        "print(f\"🗄️ Catálogo: {spark.conf.get('spark.sql.catalogImplementation')}\")\n",
        "print(f\"🚀 Versión Spark: {spark.version}\")\n",
        "\n",
        "# Verificar configuración de recursos aplicada\n",
        "print(\"\\n📋 CONFIGURACIÓN DE RECURSOS:\")\n",
        "print(f\"   💾 Driver Memory: {spark.conf.get('spark.driver.memory')}\")\n",
        "print(f\"   🖥️ Driver Cores: {spark.conf.get('spark.driver.cores')}\")\n",
        "print(f\"   📊 Max Result Size: {spark.conf.get('spark.driver.maxResultSize')}\")\n",
        "print(f\"   💾 Executor Memory: {spark.conf.get('spark.executor.memory')}\")\n",
        "print(f\"   🖥️ Executor Cores: {spark.conf.get('spark.executor.cores')}\")\n",
        "print(f\"   📊 Executor Instances: {spark.conf.get('spark.executor.instances')}\")\n",
        "print(f\"   🔄 Adaptive Query: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
        "print(f\"   📦 Shuffle Partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
        "\n",
        "print(\"\\n🌟 URLs de Monitoreo:\")\n",
        "print(\"   🎛️ Spark Master UI: http://localhost:8080\")\n",
        "print(\"   📊 Spark Driver UI: http://localhost:4040\")\n",
        "\n",
        "print(\"\\n✅ Spark configurado con recursos optimizados para el entorno\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📚 **Tutorial Paso a Paso: Operaciones Básicas con Spark**\n",
        "\n",
        "### 🎯 **Objetivo del Tutorial:**\n",
        "Aprender las operaciones fundamentales de Apache Spark a través de ejemplos prácticos, desde la creación de DataFrames hasta consultas SQL complejas.\n",
        "\n",
        "### 📋 **Operaciones que Cubriremos:**\n",
        "1. **Crear DataFrames** desde datos en memoria\n",
        "2. **Definir esquemas** personalizados\n",
        "3. **Operaciones de transformación** (select, filter, groupBy)\n",
        "4. **Operaciones de acción** (show, collect, count)\n",
        "5. **Consultas SQL** sobre DataFrames\n",
        "6. **Joins** entre múltiples DataFrames\n",
        "7. **Agregaciones** y funciones de ventana\n",
        "8. **Escritura y lectura** de datos\n",
        "\n",
        "### 🏗️ **Estructura del Dataset de Ejemplo:**\n",
        "Crearemos un dataset empresarial simulado con las siguientes entidades:\n",
        "- **👥 Empleados**: ID, nombre, departamento, salario, fecha de contratación\n",
        "- **🏢 Departamentos**: ID, nombre, presupuesto, ubicación\n",
        "- **📊 Ventas**: ID, empleado_id, producto, monto, fecha\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 📊 PASO 1: CREAR DATAFRAMES CON ESQUEMAS PERSONALIZADOS\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
        "from pyspark.sql.functions import col, when, lit\n",
        "from datetime import date\n",
        "import time\n",
        "\n",
        "print(\"📊 CREANDO DATAFRAMES DE EJEMPLO...\")\n",
        "print(\"=\"*45)\n",
        "\n",
        "# 1. DEFINIR ESQUEMAS (SCHEMAS)\n",
        "print(\"\\n1️⃣ DEFINIENDO ESQUEMAS PERSONALIZADOS:\")\n",
        "\n",
        "# Esquema para empleados\n",
        "empleados_schema = StructType([\n",
        "    StructField(\"emp_id\", IntegerType(), False),\n",
        "    StructField(\"nombre\", StringType(), False),\n",
        "    StructField(\"departamento_id\", IntegerType(), False),\n",
        "    StructField(\"salario\", DoubleType(), False),\n",
        "    StructField(\"fecha_contratacion\", DateType(), False)\n",
        "])\n",
        "\n",
        "# Esquema para departamentos\n",
        "departamentos_schema = StructType([\n",
        "    StructField(\"dept_id\", IntegerType(), False),\n",
        "    StructField(\"nombre_dept\", StringType(), False),\n",
        "    StructField(\"presupuesto\", DoubleType(), False),\n",
        "    StructField(\"ubicacion\", StringType(), False)\n",
        "])\n",
        "\n",
        "# Esquema para ventas\n",
        "ventas_schema = StructType([\n",
        "    StructField(\"venta_id\", IntegerType(), False),\n",
        "    StructField(\"emp_id\", IntegerType(), False),\n",
        "    StructField(\"producto\", StringType(), False),\n",
        "    StructField(\"monto\", DoubleType(), False),\n",
        "    StructField(\"fecha_venta\", DateType(), False)\n",
        "])\n",
        "\n",
        "print(\"✅ Esquemas definidos correctamente\")\n",
        "\n",
        "# 2. CREAR DATOS DE EJEMPLO\n",
        "print(\"\\n2️⃣ CREANDO DATOS DE EJEMPLO:\")\n",
        "\n",
        "# Datos de empleados\n",
        "empleados_data = [\n",
        "    (1, \"Ana García\", 1, 75000.0, date(2020, 1, 15)),\n",
        "    (2, \"Carlos López\", 2, 85000.0, date(2019, 3, 22)),\n",
        "    (3, \"María Rodríguez\", 1, 70000.0, date(2021, 6, 10)),\n",
        "    (4, \"Juan Pérez\", 3, 90000.0, date(2018, 9, 5)),\n",
        "    (5, \"Laura Martín\", 2, 82000.0, date(2020, 11, 18)),\n",
        "    (6, \"Pedro Sánchez\", 3, 95000.0, date(2017, 4, 12)),\n",
        "    (7, \"Isabel Torres\", 1, 73000.0, date(2021, 2, 28)),\n",
        "    (8, \"Miguel Ruiz\", 2, 88000.0, date(2019, 8, 14))\n",
        "]\n",
        "\n",
        "# Datos de departamentos\n",
        "departamentos_data = [\n",
        "    (1, \"Recursos Humanos\", 500000.0, \"Madrid\"),\n",
        "    (2, \"Tecnología\", 1200000.0, \"Barcelona\"),\n",
        "    (3, \"Ventas\", 800000.0, \"Valencia\")\n",
        "]\n",
        "\n",
        "# Datos de ventas\n",
        "ventas_data = [\n",
        "    (1, 4, \"Laptop Pro\", 1299.99, date(2023, 1, 10)),\n",
        "    (2, 6, \"Smartphone\", 899.99, date(2023, 1, 12)),\n",
        "    (3, 4, \"Tablet\", 599.99, date(2023, 1, 15)),\n",
        "    (4, 6, \"Monitor 4K\", 399.99, date(2023, 1, 18)),\n",
        "    (5, 4, \"Teclado Mecánico\", 149.99, date(2023, 1, 20)),\n",
        "    (6, 6, \"Mouse Gaming\", 79.99, date(2023, 1, 22)),\n",
        "    (7, 4, \"Webcam HD\", 129.99, date(2023, 1, 25)),\n",
        "    (8, 6, \"Auriculares\", 199.99, date(2023, 1, 28))\n",
        "]\n",
        "\n",
        "print(f\"📋 Empleados: {len(empleados_data)} registros\")\n",
        "print(f\"🏢 Departamentos: {len(departamentos_data)} registros\") \n",
        "print(f\"💰 Ventas: {len(ventas_data)} registros\")\n",
        "\n",
        "# 3. CREAR DATAFRAMES\n",
        "print(\"\\n3️⃣ CREANDO DATAFRAMES:\")\n",
        "\n",
        "empleados_df = spark.createDataFrame(empleados_data, empleados_schema)\n",
        "departamentos_df = spark.createDataFrame(departamentos_data, departamentos_schema)\n",
        "ventas_df = spark.createDataFrame(ventas_data, ventas_schema)\n",
        "\n",
        "print(\"✅ DataFrames creados exitosamente\")\n",
        "\n",
        "# 4. VERIFICAR ESTRUCTURA DE LOS DATAFRAMES\n",
        "print(\"\\n4️⃣ VERIFICANDO ESTRUCTURA:\")\n",
        "\n",
        "print(\"\\n👥 ESQUEMA - EMPLEADOS:\")\n",
        "empleados_df.printSchema()\n",
        "\n",
        "print(\"\\n🏢 ESQUEMA - DEPARTAMENTOS:\")\n",
        "departamentos_df.printSchema()\n",
        "\n",
        "print(\"\\n💰 ESQUEMA - VENTAS:\")\n",
        "ventas_df.printSchema()\n",
        "\n",
        "# 5. MOSTRAR DATOS DE MUESTRA\n",
        "print(\"\\n5️⃣ DATOS DE MUESTRA:\")\n",
        "\n",
        "print(\"\\n👥 EMPLEADOS (primeras 5 filas):\")\n",
        "empleados_df.show(5)\n",
        "\n",
        "print(\"\\n🏢 DEPARTAMENTOS:\")\n",
        "departamentos_df.show()\n",
        "\n",
        "print(\"\\n💰 VENTAS (primeras 5 filas):\")\n",
        "ventas_df.show(5)\n",
        "\n",
        "print(\"\\n🎉 ¡DataFrames creados y verificados exitosamente!\")\n",
        "print(\"📊 Listos para operaciones de transformación y análisis\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
