{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🔗 Spark + NoSQL Integration - Análisis Distribuido\n",
        "\n",
        "## 📋 **Guía Completa de Integración Big Data**\n",
        "\n",
        "### 🎯 **Objetivos del Notebook:**\n",
        "1. **Integrar Apache Spark** con HBase y Cassandra\n",
        "2. **Implementar análisis distribuido** sobre datos NoSQL\n",
        "3. **Optimizar rendimiento** de consultas híbridas\n",
        "4. **Casos de uso reales** de Big Data Analytics\n",
        "5. **Mejores prácticas** de arquitectura distribuida\n",
        "\n",
        "### 🏗️ **Arquitectura de Integración:**\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│                    SPARK + NoSQL ECOSYSTEM                     │\n",
        "├─────────────────────────────────────────────────────────────────┤\n",
        "│                                                                 │\n",
        "│  ┌─────────────────┐    ┌─────────────────┐                    │\n",
        "│  │   Spark Driver  │    │  Spark Workers  │                    │\n",
        "│  │                 │    │                 │                    │\n",
        "│  │ ┌─────────────┐ │    │ ┌─────────────┐ │                    │\n",
        "│  │ │ SparkSession│ │◄──►│ │  Executors  │ │                    │\n",
        "│  │ │             │ │    │ │             │ │                    │\n",
        "│  │ │ • Catalyst  │ │    │ │ • Tasks     │ │                    │\n",
        "│  │ │ • Optimizer │ │    │ │ • Cache     │ │                    │\n",
        "│  │ └─────────────┘ │    │ └─────────────┘ │                    │\n",
        "│  └─────────────────┘    └─────────────────┘                    │\n",
        "│           │                       │                            │\n",
        "│           └───────────┬───────────┘                            │\n",
        "│                       │                                        │\n",
        "│  ┌────────────────────┼────────────────────┐                   │\n",
        "│  │              CONNECTORS                │                   │\n",
        "│  │                    │                    │                   │\n",
        "│  │  ┌─────────────────┼─────────────────┐  │                   │\n",
        "│  │  │    HBase        │   Cassandra     │  │                   │\n",
        "│  │  │   Connector     │   Connector     │  │                   │\n",
        "│  │  │                 │                 │  │                   │\n",
        "│  │  │ • spark-hbase   │ • spark-cass    │  │                   │\n",
        "│  │  │ • TableInputs   │ • CQL Support   │  │                   │\n",
        "│  │  │ • Bulk Ops      │ • Token Aware   │  │                   │\n",
        "│  │  └─────────────────┼─────────────────┘  │                   │\n",
        "│  └────────────────────┼────────────────────┘                   │\n",
        "│                       │                                        │\n",
        "│  ┌─────────────────────────────────────────────────────────┐   │\n",
        "│  │                    DATA SOURCES                         │   │\n",
        "│  │                                                         │   │\n",
        "│  │  ┌─────────────────┐      ┌─────────────────────────┐   │   │\n",
        "│  │  │     HBase       │      │       Cassandra        │   │   │\n",
        "│  │  │                 │      │                         │   │   │\n",
        "│  │  │ • Column Store  │      │ • Wide Column Store     │   │   │\n",
        "│  │  │ • HDFS Storage  │      │ • Distributed Nodes     │   │   │\n",
        "│  │  │ • Strong Cons.  │      │ • Eventual Cons.        │   │   │\n",
        "│  │  │ • Real-time     │      │ • High Throughput       │   │   │\n",
        "│  │  └─────────────────┘      └─────────────────────────┘   │   │\n",
        "│  └─────────────────────────────────────────────────────────┘   │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "### 🎯 **Casos de Uso Cubiertos:**\n",
        "\n",
        "#### **1. 📊 Real-time Analytics**\n",
        "- **Fuente**: Datos streaming en HBase\n",
        "- **Procesamiento**: Agregaciones con Spark\n",
        "- **Destino**: Dashboards en tiempo real\n",
        "\n",
        "#### **2. 🌐 IoT Data Processing**\n",
        "- **Fuente**: Sensores → Cassandra\n",
        "- **Procesamiento**: ML con Spark MLlib\n",
        "- **Destino**: Alertas y predicciones\n",
        "\n",
        "#### **3. 📈 Business Intelligence**\n",
        "- **Fuente**: Transacciones en HBase\n",
        "- **Procesamiento**: ETL con Spark SQL\n",
        "- **Destino**: Data Warehouse\n",
        "\n",
        "#### **4. 🔍 Log Analytics**\n",
        "- **Fuente**: Logs distribuidos en Cassandra\n",
        "- **Procesamiento**: Pattern mining con Spark\n",
        "- **Destino**: Monitoreo y alertas\n",
        "\n",
        "### 📊 **Beneficios de la Integración:**\n",
        "\n",
        "| Beneficio | Spark + HBase | Spark + Cassandra |\n",
        "|-----------|---------------|-------------------|\n",
        "| **Consistencia** | Strong (ACID) | Tunable (BASE) |\n",
        "| **Latencia** | Baja (< 100ms) | Ultra-baja (< 10ms) |\n",
        "| **Throughput** | Alto (GB/s) | Muy Alto (TB/s) |\n",
        "| **Escalabilidad** | Vertical + Horizontal | Horizontal Lineal |\n",
        "| **Casos de Uso** | OLTP + Analytics | IoT + Time Series |\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 🚀 CONFIGURACIÓN INICIAL - SPARK + NoSQL INTEGRATION\n",
        "\n",
        "import findspark\n",
        "findspark.init('/opt/spark')\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import subprocess\n",
        "\n",
        "# Suprimir warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Importar librerías de Spark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "print(\"🔗 INICIANDO CONFIGURACIÓN SPARK + NoSQL INTEGRATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Verificar servicios requeridos\n",
        "services_status = {}\n",
        "required_services = {\n",
        "    \"master\": (\"Spark Master\", \"7077\"),\n",
        "    \"hbase\": (\"HBase Thrift\", \"9090\"), \n",
        "    \"cassandra\": (\"Cassandra Native\", \"9042\")\n",
        "}\n",
        "\n",
        "print(\"\\n1️⃣ VERIFICANDO SERVICIOS REQUERIDOS:\")\n",
        "\n",
        "import socket\n",
        "for service, (description, port) in required_services.items():\n",
        "    try:\n",
        "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "        sock.settimeout(3)\n",
        "        result = sock.connect_ex((service, int(port)))\n",
        "        sock.close()\n",
        "        \n",
        "        if result == 0:\n",
        "            print(f\"   ✅ {description}: Disponible ({service}:{port})\")\n",
        "            services_status[service] = True\n",
        "        else:\n",
        "            print(f\"   ❌ {description}: No disponible ({service}:{port})\")\n",
        "            services_status[service] = False\n",
        "    except Exception as e:\n",
        "        print(f\"   ⚠️ {description}: Error - {e}\")\n",
        "        services_status[service] = False\n",
        "\n",
        "# Resumen de servicios\n",
        "available_services = sum(services_status.values())\n",
        "total_services = len(services_status)\n",
        "print(f\"\\n📊 Servicios disponibles: {available_services}/{total_services}\")\n",
        "\n",
        "if available_services >= 2:\n",
        "    print(\"✅ Configuración mínima disponible para continuar\")\n",
        "else:\n",
        "    print(\"⚠️ Servicios insuficientes - algunas funciones pueden no estar disponibles\")\n",
        "\n",
        "# Información del entorno\n",
        "print(f\"\\n2️⃣ INFORMACIÓN DEL ENTORNO:\")\n",
        "print(f\"   🐍 Python: {sys.version.split()[0]}\")\n",
        "print(f\"   🏠 Spark Home: {os.environ.get('SPARK_HOME', '/opt/spark')}\")\n",
        "print(f\"   🖥️ Hostname: {os.environ.get('HOSTNAME', 'jupyterlab')}\")\n",
        "print(f\"   ⏰ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "print(\"\\n🎯 Configuración inicial completada\")\n",
        "print(\"🚀 Listo para crear sesión Spark integrada...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 🚀 CONFIGURACIÓN INICIAL Y VERIFICACIÓN DEL ENTORNO\n",
        "\n",
        "import findspark\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import warnings\n",
        "from datetime import datetime, date\n",
        "import time\n",
        "\n",
        "# Configurar findspark\n",
        "findspark.init('/opt/spark')\n",
        "\n",
        "# Importar librerías de PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Suprimir warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"🔗 SPARK + NOSQL INTEGRATION - CONFIGURACIÓN INICIAL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Información del entorno\n",
        "print(f\"📍 Spark Home: {os.environ.get('SPARK_HOME', '/opt/spark')}\")\n",
        "print(f\"🐍 Python Version: {sys.version.split()[0]}\")\n",
        "print(f\"🖥️ Hostname: {os.environ.get('HOSTNAME', 'jupyterlab')}\")\n",
        "print(f\"⏰ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Verificar servicios NoSQL\n",
        "print(\"\\n🔍 VERIFICANDO SERVICIOS NOSQL:\")\n",
        "\n",
        "import socket\n",
        "\n",
        "services_to_check = {\n",
        "    \"hbase\": (\"HBase Thrift\", \"9090\"),\n",
        "    \"cassandra\": (\"Cassandra Native\", \"9042\"),\n",
        "    \"master\": (\"Spark Master\", \"7077\")\n",
        "}\n",
        "\n",
        "services_status = {}\n",
        "for service, (description, port) in services_to_check.items():\n",
        "    try:\n",
        "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "        sock.settimeout(3)\n",
        "        result = sock.connect_ex((service, int(port)))\n",
        "        sock.close()\n",
        "        \n",
        "        if result == 0:\n",
        "            print(f\"   ✅ {description}: Disponible en {service}:{port}\")\n",
        "            services_status[service] = True\n",
        "        else:\n",
        "            print(f\"   ❌ {description}: No disponible en {service}:{port}\")\n",
        "            services_status[service] = False\n",
        "    except Exception as e:\n",
        "        print(f\"   ⚠️ {description}: Error verificando - {e}\")\n",
        "        services_status[service] = False\n",
        "\n",
        "print(f\"\\n📊 Servicios disponibles: {sum(services_status.values())}/{len(services_status)}\")\n",
        "print(\"✅ Configuración inicial completada\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 🚀 CREAR SESIÓN SPARK OPTIMIZADA PARA INTEGRACIÓN NOSQL\n",
        "\n",
        "print(\"🚀 CREANDO SESIÓN SPARK PARA INTEGRACIÓN NOSQL...\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "# Detener cualquier sesión existente\n",
        "try:\n",
        "    spark.stop()\n",
        "    print(\"🛑 Sesión anterior detenida correctamente\")\n",
        "except:\n",
        "    print(\"🔍 No había sesión activa\")\n",
        "\n",
        "# CONFIGURACIÓN OPTIMIZADA PARA INTEGRACIÓN NOSQL\n",
        "# Incluye configuraciones específicas para conectores NoSQL\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"EducacionIT-Spark-NoSQL-Integration\") \\\n",
        "    .master(\"spark://master:7077\") \\\n",
        "    .config(\"spark.driver.memory\", \"1200m\") \\\n",
        "    .config(\"spark.driver.cores\", \"2\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"500m\") \\\n",
        "    .config(\"spark.executor.memory\", \"800m\") \\\n",
        "    .config(\"spark.executor.cores\", \"1\") \\\n",
        "    .config(\"spark.executor.instances\", \"2\") \\\n",
        "    .config(\"spark.executor.memoryFraction\", \"0.8\") \\\n",
        "    .config(\"spark.executor.memoryStorageFraction\", \"0.5\") \\\n",
        "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
        "    .config(\"spark.shuffle.service.enabled\", \"false\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
        "    .config(\"spark.network.timeout\", \"300s\") \\\n",
        "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.cassandra.connection.host\", \"cassandra\") \\\n",
        "    .config(\"spark.cassandra.connection.port\", \"9042\") \\\n",
        "    .config(\"spark.cassandra.connection.keep_alive_ms\", \"30000\") \\\n",
        "    .config(\"spark.cassandra.connection.timeout_ms\", \"30000\") \\\n",
        "    .config(\"spark.hbase.host\", \"hbase\") \\\n",
        "    .config(\"spark.hbase.port\", \"9090\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Configurar nivel de logging\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "# Información de la sesión\n",
        "print(\"\\n🎉 ¡SESIÓN SPARK PARA NOSQL CREADA EXITOSAMENTE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"🏷️  Aplicación: {spark.sparkContext.appName}\")\n",
        "print(f\"🔗 Master URL: {spark.sparkContext.master}\")\n",
        "print(f\"📊 Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
        "print(f\"🎯 App ID: {spark.sparkContext.applicationId}\")\n",
        "print(f\"🚀 Versión Spark: {spark.version}\")\n",
        "\n",
        "# Verificar configuración específica para NoSQL\n",
        "print(\"\\n📋 CONFIGURACIÓN NOSQL:\")\n",
        "print(f\"   🌟 Cassandra Host: {spark.conf.get('spark.cassandra.connection.host')}\")\n",
        "print(f\"   🔌 Cassandra Port: {spark.conf.get('spark.cassandra.connection.port')}\")\n",
        "print(f\"   🏛️ HBase Host: {spark.conf.get('spark.hbase.host')}\")\n",
        "print(f\"   🔌 HBase Port: {spark.conf.get('spark.hbase.port')}\")\n",
        "\n",
        "# Verificar estado del cluster\n",
        "print(\"\\n🔍 ESTADO DEL CLUSTER SPARK:\")\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try:\n",
        "    # Información de executors\n",
        "    executor_infos = sc.statusTracker().getExecutorInfos()\n",
        "    active_executors = len([e for e in executor_infos if e.isActive])\n",
        "    print(f\"   ⚡ Executors activos: {active_executors}\")\n",
        "    print(f\"   🖥️ Total executors: {len(executor_infos)}\")\n",
        "    \n",
        "    # Test rápido de conectividad\n",
        "    test_rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "    result = test_rdd.sum()\n",
        "    print(f\"   ✅ Test de conectividad: Suma = {result}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"   ⚠️ Error verificando cluster: {e}\")\n",
        "\n",
        "print(\"\\n🌟 URLs de Monitoreo:\")\n",
        "print(\"   🎛️ Spark Master UI: http://localhost:8080\")\n",
        "print(\"   📊 Spark Driver UI: http://localhost:4040\")\n",
        "\n",
        "print(\"\\n✅ Spark configurado para integración NoSQL\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔗 **Tutorial Paso a Paso: Integración Spark + HBase**\n",
        "\n",
        "### 🎯 **Objetivo:**\n",
        "Aprender a leer datos desde HBase usando Spark, realizar transformaciones distribuidas y escribir resultados de vuelta a HBase.\n",
        "\n",
        "### 📋 **Casos de Uso Reales:**\n",
        "1. **ETL desde HBase**: Extraer datos de HBase, transformarlos y cargarlos en otro sistema\n",
        "2. **Análisis en tiempo real**: Procesar datos de HBase con Spark para analytics\n",
        "3. **Agregaciones complejas**: Usar Spark SQL para consultas complejas sobre datos HBase\n",
        "4. **Data Lake Integration**: Combinar datos de HBase con datos de HDFS/S3\n",
        "\n",
        "### 🏗️ **Arquitectura de Integración:**\n",
        "\n",
        "```\n",
        "┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n",
        "│   Spark Driver  │◄──►│  HBase Client   │◄──►│  HBase Cluster  │\n",
        "│                 │    │   (happybase)   │    │                 │\n",
        "│ • SparkSession  │    │ • Thrift API    │    │ • RegionServers │\n",
        "│ • DataFrame API │    │ • Connection    │    │ • HDFS Storage  │\n",
        "│ • SQL Engine   │    │ • Table Ops     │    │ • Zookeeper     │\n",
        "└─────────────────┘    └─────────────────┘    └─────────────────┘\n",
        "         │                       │                       │\n",
        "         └───────────────────────┼───────────────────────┘\n",
        "                                 │\n",
        "                    ┌─────────────────┐\n",
        "                    │  Spark Workers  │\n",
        "                    │                 │\n",
        "                    │ • Data Parallel │\n",
        "                    │ • Executors     │\n",
        "                    │ • Task Exec     │\n",
        "                    └─────────────────┘\n",
        "```\n",
        "\n",
        "### 💡 **Mejores Prácticas:**\n",
        "- **Partitioning**: Diseñar row keys para distribución uniforme\n",
        "- **Batch Processing**: Usar batch operations para mejor rendimiento\n",
        "- **Caching**: Cache DataFrames frecuentemente usados\n",
        "- **Resource Management**: Configurar memoria adecuadamente\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 🏛️ INTEGRACIÓN SPARK + HBASE: LECTURA Y ANÁLISIS DE DATOS (CONFIGURACIÓN SIMPLE)\n",
        "\n",
        "print(\"🏛️ INICIANDO INTEGRACIÓN SPARK + HBASE...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Instalar happybase si no está disponible\n",
        "try:\n",
        "    import happybase\n",
        "    print(\"✅ HappyBase ya está disponible\")\n",
        "except ImportError:\n",
        "    print(\"📦 Instalando HappyBase...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"happybase\"])\n",
        "    import happybase\n",
        "    print(\"✅ HappyBase instalado correctamente\")\n",
        "\n",
        "# Establecer conexión con HBase usando configuración simple\n",
        "print(\"\\n🔗 CONECTANDO A HBASE CON CONFIGURACIÓN SIMPLE:\")\n",
        "\n",
        "try:\n",
        "    # CONFIGURACIÓN EXACTAMENTE IGUAL AL TUTORIAL ORIGINAL QUE FUNCIONABA\n",
        "    hbase_connection = happybase.Connection(\n",
        "        host='hbase',  # Nombre del servicio en Docker\n",
        "        port=9090,     # Puerto Thrift\n",
        "        timeout=10000\n",
        "    )\n",
        "    \n",
        "    # Test de conexión básico (igual al original)\n",
        "    tables = list(hbase_connection.tables())\n",
        "    print(\"🎉 ¡Conexión a HBase exitosa!\")\n",
        "    print(f\"📊 Tablas existentes: {len(tables)}\")\n",
        "    \n",
        "    # PASO 1: LEER DATOS DE HBASE Y CONVERTIR A SPARK DATAFRAME\n",
        "    print(\"\\n1️⃣ LEYENDO DATOS DE HBASE Y CREANDO SPARK DATAFRAME:\")\n",
        "    \n",
        "    # Verificar si existe la tabla de productos (del notebook anterior)\n",
        "    existing_tables = [table.decode('utf-8') for table in hbase_connection.tables()]\n",
        "    \n",
        "    if 'productos_ecommerce' not in existing_tables:\n",
        "        print(\"⚠️ Tabla 'productos_ecommerce' no existe. Creándola con datos de ejemplo...\")\n",
        "        \n",
        "        # Crear tabla y datos de ejemplo\n",
        "        column_families = {\n",
        "            'info': {'compression': 'SNAPPY'},\n",
        "            'inventory': {'compression': 'SNAPPY'},\n",
        "            'sales': {'compression': 'SNAPPY'}\n",
        "        }\n",
        "        \n",
        "        hbase_connection.create_table('productos_ecommerce', column_families)\n",
        "        table = hbase_connection.table('productos_ecommerce')\n",
        "        \n",
        "        # Insertar datos de ejemplo\n",
        "        sample_data = [\n",
        "            ('PROD_001', {\n",
        "                b'info:name': b'Laptop Gaming Pro',\n",
        "                b'info:category': b'Electronics',\n",
        "                b'info:price': b'1299.99',\n",
        "                b'inventory:stock': b'25',\n",
        "                b'sales:total_sold': b'156',\n",
        "                b'sales:revenue': b'202798.44'\n",
        "            }),\n",
        "            ('PROD_002', {\n",
        "                b'info:name': b'Smartphone Ultra',\n",
        "                b'info:category': b'Electronics', \n",
        "                b'info:price': b'899.99',\n",
        "                b'inventory:stock': b'45',\n",
        "                b'sales:total_sold': b'289',\n",
        "                b'sales:revenue': b'260097.11'\n",
        "            }),\n",
        "            ('PROD_003', {\n",
        "                b'info:name': b'Wireless Headphones',\n",
        "                b'info:category': b'Audio',\n",
        "                b'info:price': b'199.99',\n",
        "                b'inventory:stock': b'78',\n",
        "                b'sales:total_sold': b'432',\n",
        "                b'sales:revenue': b'86395.68'\n",
        "            })\n",
        "        ]\n",
        "        \n",
        "        for row_key, data in sample_data:\n",
        "            table.put(row_key, data)\n",
        "        \n",
        "        print(\"✅ Tabla y datos de ejemplo creados\")\n",
        "    else:\n",
        "        table = hbase_connection.table('productos_ecommerce')\n",
        "        print(\"✅ Usando tabla existente 'productos_ecommerce'\")\n",
        "    \n",
        "    # Leer datos de HBase y convertir a lista para Spark DataFrame\n",
        "    print(\"\\n📊 EXTRAYENDO DATOS DE HBASE:\")\n",
        "    \n",
        "    hbase_data = []\n",
        "    for key, data in table.scan():\n",
        "        row_dict = {'product_id': key.decode('utf-8')}\n",
        "        \n",
        "        # Extraer datos de diferentes column families\n",
        "        for col_key, col_value in data.items():\n",
        "            family, qualifier = col_key.decode('utf-8').split(':')\n",
        "            column_name = f\"{family}_{qualifier}\"\n",
        "            row_dict[column_name] = col_value.decode('utf-8')\n",
        "        \n",
        "        hbase_data.append(row_dict)\n",
        "    \n",
        "    print(f\"✅ Extraídos {len(hbase_data)} registros de HBase\")\n",
        "    \n",
        "    # Mostrar datos extraídos\n",
        "    print(\"\\n📋 DATOS EXTRAÍDOS DE HBASE:\")\n",
        "    for i, row in enumerate(hbase_data, 1):\n",
        "        print(f\"   {i}. {row['product_id']}: {row.get('info_name', 'N/A')}\")\n",
        "    \n",
        "    # PASO 2: CREAR SPARK DATAFRAME DESDE DATOS HBASE\n",
        "    print(\"\\n2️⃣ CREANDO SPARK DATAFRAME DESDE DATOS HBASE:\")\n",
        "    \n",
        "    # Definir esquema para el DataFrame\n",
        "    hbase_schema = StructType([\n",
        "        StructField(\"product_id\", StringType(), False),\n",
        "        StructField(\"info_name\", StringType(), True),\n",
        "        StructField(\"info_category\", StringType(), True),\n",
        "        StructField(\"info_price\", StringType(), True),\n",
        "        StructField(\"inventory_stock\", StringType(), True),\n",
        "        StructField(\"sales_total_sold\", StringType(), True),\n",
        "        StructField(\"sales_revenue\", StringType(), True)\n",
        "    ])\n",
        "    \n",
        "    # Crear DataFrame desde los datos HBase\n",
        "    hbase_df = spark.createDataFrame(hbase_data, schema=hbase_schema)\n",
        "    \n",
        "    print(\"✅ Spark DataFrame creado desde datos HBase\")\n",
        "    print(f\"📊 Registros en DataFrame: {hbase_df.count()}\")\n",
        "    \n",
        "    # Mostrar esquema y datos\n",
        "    print(\"\\n📋 ESQUEMA DEL DATAFRAME:\")\n",
        "    hbase_df.printSchema()\n",
        "    \n",
        "    print(\"\\n📊 DATOS DEL DATAFRAME:\")\n",
        "    hbase_df.show(truncate=False)\n",
        "    \n",
        "    # PASO 3: TRANSFORMACIONES CON SPARK\n",
        "    print(\"\\n3️⃣ APLICANDO TRANSFORMACIONES CON SPARK:\")\n",
        "    \n",
        "    # Convertir tipos de datos\n",
        "    transformed_df = hbase_df \\\n",
        "        .withColumn(\"price\", col(\"info_price\").cast(\"double\")) \\\n",
        "        .withColumn(\"stock\", col(\"inventory_stock\").cast(\"integer\")) \\\n",
        "        .withColumn(\"total_sold\", col(\"sales_total_sold\").cast(\"integer\")) \\\n",
        "        .withColumn(\"revenue\", col(\"sales_revenue\").cast(\"double\")) \\\n",
        "        .withColumn(\"inventory_value\", col(\"price\") * col(\"stock\")) \\\n",
        "        .select(\n",
        "            col(\"product_id\"),\n",
        "            col(\"info_name\").alias(\"name\"),\n",
        "            col(\"info_category\").alias(\"category\"),\n",
        "            col(\"price\"),\n",
        "            col(\"stock\"),\n",
        "            col(\"total_sold\"),\n",
        "            col(\"revenue\"),\n",
        "            col(\"inventory_value\")\n",
        "        )\n",
        "    \n",
        "    print(\"✅ Transformaciones aplicadas\")\n",
        "    print(\"\\n📊 DATAFRAME TRANSFORMADO:\")\n",
        "    transformed_df.show()\n",
        "    \n",
        "    # PASO 4: ANÁLISIS CON SPARK SQL\n",
        "    print(\"\\n4️⃣ ANÁLISIS CON SPARK SQL:\")\n",
        "    \n",
        "    # Registrar DataFrame como tabla temporal\n",
        "    transformed_df.createOrReplaceTempView(\"productos_hbase\")\n",
        "    \n",
        "    # Análisis 1: Productos más vendidos\n",
        "    print(\"\\n📈 TOP PRODUCTOS MÁS VENDIDOS:\")\n",
        "    top_selling = spark.sql(\"\"\"\n",
        "        SELECT name, category, total_sold, revenue\n",
        "        FROM productos_hbase\n",
        "        ORDER BY total_sold DESC\n",
        "    \"\"\")\n",
        "    top_selling.show()\n",
        "    \n",
        "    # Análisis 2: Valor total del inventario por categoría\n",
        "    print(\"\\n💰 VALOR DEL INVENTARIO POR CATEGORÍA:\")\n",
        "    inventory_by_category = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            category,\n",
        "            COUNT(*) as num_products,\n",
        "            SUM(stock) as total_stock,\n",
        "            SUM(inventory_value) as total_inventory_value,\n",
        "            AVG(price) as avg_price\n",
        "        FROM productos_hbase\n",
        "        GROUP BY category\n",
        "        ORDER BY total_inventory_value DESC\n",
        "    \"\"\")\n",
        "    inventory_by_category.show()\n",
        "    \n",
        "    # Análisis 3: ROI por producto\n",
        "    print(\"\\n📊 ROI (RETURN ON INVESTMENT) POR PRODUCTO:\")\n",
        "    roi_analysis = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            name,\n",
        "            category,\n",
        "            price,\n",
        "            inventory_value,\n",
        "            revenue,\n",
        "            ROUND((revenue / inventory_value) * 100, 2) as roi_percentage\n",
        "        FROM productos_hbase\n",
        "        WHERE inventory_value > 0\n",
        "        ORDER BY roi_percentage DESC\n",
        "    \"\"\")\n",
        "    roi_analysis.show()\n",
        "    \n",
        "    # PASO 5: AGREGACIONES AVANZADAS\n",
        "    print(\"\\n5️⃣ AGREGACIONES AVANZADAS:\")\n",
        "    \n",
        "    # Estadísticas generales\n",
        "    stats = transformed_df.agg(\n",
        "        sum(\"revenue\").alias(\"total_revenue\"),\n",
        "        sum(\"inventory_value\").alias(\"total_inventory_value\"),\n",
        "        sum(\"total_sold\").alias(\"total_units_sold\"),\n",
        "        avg(\"price\").alias(\"avg_price\"),\n",
        "        count(\"*\").alias(\"total_products\")\n",
        "    ).collect()[0]\n",
        "    \n",
        "    print(\"\\n📊 ESTADÍSTICAS GENERALES:\")\n",
        "    print(f\"   💰 Ingresos totales: ${stats['total_revenue']:,.2f}\")\n",
        "    print(f\"   📦 Valor total inventario: ${stats['total_inventory_value']:,.2f}\")\n",
        "    print(f\"   📈 Unidades vendidas: {stats['total_units_sold']:,}\")\n",
        "    print(f\"   💵 Precio promedio: ${stats['avg_price']:,.2f}\")\n",
        "    print(f\"   📱 Total productos: {stats['total_products']}\")\n",
        "    \n",
        "    # Cache del DataFrame para consultas futuras\n",
        "    transformed_df.cache()\n",
        "    print(\"\\n✅ DataFrame cacheado para consultas futuras\")\n",
        "    \n",
        "    print(\"\\n🎉 ¡Integración Spark + HBase completada exitosamente!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error en integración Spark + HBase: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🌟 **Tutorial Paso a Paso: Integración Spark + Cassandra**\n",
        "\n",
        "### 🎯 **Objetivo:**\n",
        "Aprender a leer datos desde Cassandra usando Spark, realizar análisis distribuidos y aprovechar la escalabilidad de ambas tecnologías.\n",
        "\n",
        "### 📋 **Casos de Uso Reales:**\n",
        "1. **Real-time Analytics**: Procesar streams de datos almacenados en Cassandra\n",
        "2. **Time-series Analysis**: Analizar datos temporales con Spark SQL\n",
        "3. **Cross-datacenter Analytics**: Consultas distribuidas en múltiples regiones\n",
        "4. **IoT Data Processing**: Procesar millones de eventos de sensores\n",
        "\n",
        "### 🏗️ **Arquitectura de Integración:**\n",
        "\n",
        "```\n",
        "┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n",
        "│   Spark Driver  │◄──►│ Cassandra       │◄──►│  Cassandra Ring │\n",
        "│                 │    │ Connector       │    │                 │\n",
        "│ • SparkSession  │    │ • Token Aware   │    │ • Node 1, 2, 3  │\n",
        "│ • DataFrame API │    │ • Load Balance  │    │ • Replication   │\n",
        "│ • SQL Engine   │    │ • Pushdown      │    │ • Partitioning  │\n",
        "└─────────────────┘    └─────────────────┘    └─────────────────┘\n",
        "         │                       │                       │\n",
        "         └───────────────────────┼───────────────────────┘\n",
        "                                 │\n",
        "                    ┌─────────────────┐\n",
        "                    │  Spark Workers  │\n",
        "                    │                 │\n",
        "                    │ • Parallel Read │\n",
        "                    │ • Local Compute │\n",
        "                    │ • Aggregation   │\n",
        "                    └─────────────────┘\n",
        "```\n",
        "\n",
        "### 💡 **Ventajas de la Integración:**\n",
        "- **Locality Awareness**: Spark ejecuta tareas cerca de los datos\n",
        "- **Predicate Pushdown**: Filtros aplicados en Cassandra\n",
        "- **Token Range Splitting**: Paralelización automática\n",
        "- **Fault Tolerance**: Recuperación automática de fallos\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 🌟 INTEGRACIÓN SPARK + CASSANDRA: ANÁLISIS DISTRIBUIDO (CONFIGURACIÓN SIMPLE)\n",
        "\n",
        "print(\"🌟 INICIANDO INTEGRACIÓN SPARK + CASSANDRA...\")\n",
        "print(\"=\"*52)\n",
        "\n",
        "# Instalar driver de Cassandra si no está disponible\n",
        "try:\n",
        "    from cassandra.cluster import Cluster\n",
        "    print(\"✅ Driver de Cassandra ya está disponible\")\n",
        "except ImportError:\n",
        "    print(\"📦 Instalando driver de Cassandra...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"cassandra-driver\"])\n",
        "    from cassandra.cluster import Cluster\n",
        "    print(\"✅ Driver de Cassandra instalado correctamente\")\n",
        "\n",
        "# Establecer conexión con Cassandra usando configuración simple\n",
        "print(\"\\n🔗 CONECTANDO A CASSANDRA CON CONFIGURACIÓN SIMPLE:\")\n",
        "\n",
        "try:\n",
        "    # CONFIGURACIÓN EXACTAMENTE IGUAL AL TUTORIAL ORIGINAL QUE FUNCIONABA\n",
        "    cluster = Cluster(\n",
        "        contact_points=['cassandra'],  # Nombre del servicio en Docker\n",
        "        port=9042\n",
        "    )\n",
        "    \n",
        "    # Crear sesión\n",
        "    session = cluster.connect()\n",
        "    \n",
        "    # Test de conexión (igual al original)\n",
        "    result = session.execute(\"SELECT release_version FROM system.local\")\n",
        "    version = result.one()[0]\n",
        "    \n",
        "    print(\"🎉 ¡Conexión a Cassandra exitosa!\")\n",
        "    print(f\"🔢 Versión de Cassandra: {version}\")\n",
        "    \n",
        "    # Verificar si existe el keyspace del tutorial anterior\n",
        "    keyspaces = session.execute(\"SELECT keyspace_name FROM system_schema.keyspaces\")\n",
        "    existing_keyspaces = [ks.keyspace_name for ks in keyspaces]\n",
        "    \n",
        "    if 'ecommerce_nosql' not in existing_keyspaces:\n",
        "        print(\"⚠️ Keyspace 'ecommerce_nosql' no existe. Creándolo con datos de ejemplo...\")\n",
        "        \n",
        "        # Crear keyspace\n",
        "        session.execute(\"\"\"\n",
        "            CREATE KEYSPACE ecommerce_nosql\n",
        "            WITH REPLICATION = {\n",
        "                'class': 'SimpleStrategy',\n",
        "                'replication_factor': 1\n",
        "            }\n",
        "        \"\"\")\n",
        "        \n",
        "        session.set_keyspace('ecommerce_nosql')\n",
        "        \n",
        "        # Crear tabla de productos\n",
        "        session.execute(\"\"\"\n",
        "            CREATE TABLE productos (\n",
        "                producto_id text PRIMARY KEY,\n",
        "                nombre text,\n",
        "                categoria text,\n",
        "                precio decimal,\n",
        "                descripcion text,\n",
        "                fecha_creacion timestamp,\n",
        "                activo boolean\n",
        "            )\n",
        "        \"\"\")\n",
        "        \n",
        "        # Crear tabla de ventas por fecha\n",
        "        session.execute(\"\"\"\n",
        "            CREATE TABLE ventas_por_fecha (\n",
        "                fecha_venta date,\n",
        "                hora_venta time,\n",
        "                venta_id uuid,\n",
        "                producto_id text,\n",
        "                cantidad int,\n",
        "                monto decimal,\n",
        "                cliente_id text,\n",
        "                PRIMARY KEY ((fecha_venta), hora_venta, venta_id)\n",
        "            ) WITH CLUSTERING ORDER BY (hora_venta DESC, venta_id ASC)\n",
        "        \"\"\")\n",
        "        \n",
        "        # Insertar datos de ejemplo\n",
        "        import uuid\n",
        "        from datetime import date, datetime\n",
        "        \n",
        "        # Productos\n",
        "        productos_sample = [\n",
        "            (\"PROD_001\", \"MacBook Pro 16\", \"Laptops\", 2499.99, \"Laptop profesional\", True),\n",
        "            (\"PROD_002\", \"iPhone 15 Pro\", \"Smartphones\", 1199.99, \"Smartphone premium\", True),\n",
        "            (\"PROD_003\", \"iPad Air\", \"Tablets\", 599.99, \"Tablet ligera\", True),\n",
        "            (\"PROD_004\", \"AirPods Pro\", \"Audio\", 249.99, \"Auriculares premium\", True),\n",
        "            (\"PROD_005\", \"Apple Watch\", \"Wearables\", 399.99, \"Smartwatch avanzado\", True)\n",
        "        ]\n",
        "        \n",
        "        for pid, nombre, categoria, precio, desc, activo in productos_sample:\n",
        "            session.execute(\"\"\"\n",
        "                INSERT INTO productos (producto_id, nombre, categoria, precio, descripcion, fecha_creacion, activo)\n",
        "                VALUES (?, ?, ?, ?, ?, toTimestamp(now()), ?)\n",
        "            \"\"\", [pid, nombre, categoria, precio, desc, activo])\n",
        "        \n",
        "        # Ventas (datos más ricos para análisis)\n",
        "        ventas_sample = []\n",
        "        for day in range(1, 16):  # 15 días de datos\n",
        "            for hour in range(9, 18):  # Horario comercial\n",
        "                for _ in range(2):  # 2 ventas por hora\n",
        "                    ventas_sample.append((\n",
        "                        f\"2024-01-{day:02d}\",\n",
        "                        f\"{hour}:{30 if _ == 0 else 45}:00\",\n",
        "                        str(uuid.uuid4()),\n",
        "                        f\"PROD_{(day + hour + _) % 5 + 1:03d}\",\n",
        "                        1 + (_ % 3),  # Cantidad 1-3\n",
        "                        round(200 + (day * hour * 10) + (_ * 50), 2),  # Monto variable\n",
        "                        f\"CLIENTE_{(day * 100 + hour * 10 + _):05d}\"\n",
        "                    ))\n",
        "        \n",
        "        prepared_venta = session.prepare(\"\"\"\n",
        "            INSERT INTO ventas_por_fecha (fecha_venta, hora_venta, venta_id, producto_id, cantidad, monto, cliente_id)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
        "        \"\"\")\n",
        "        \n",
        "        for venta in ventas_sample:\n",
        "            session.execute(prepared_venta, venta)\n",
        "        \n",
        "        print(f\"✅ Keyspace creado con {len(productos_sample)} productos y {len(ventas_sample)} ventas\")\n",
        "    else:\n",
        "        session.set_keyspace('ecommerce_nosql')\n",
        "        print(\"✅ Usando keyspace existente 'ecommerce_nosql'\")\n",
        "    \n",
        "    # PASO 1: LEER DATOS DE CASSANDRA USANDO SPARK\n",
        "    print(\"\\n1️⃣ LEYENDO DATOS DE CASSANDRA CON SPARK:\")\n",
        "    \n",
        "    # Leer tabla de productos\n",
        "    print(\"\\n📊 CARGANDO PRODUCTOS DESDE CASSANDRA:\")\n",
        "    \n",
        "    # Simular lectura de Cassandra (en un entorno real usaríamos spark-cassandra-connector)\n",
        "    # Por ahora leemos directamente y convertimos a Spark DataFrame\n",
        "    productos_cass = session.execute(\"SELECT * FROM productos\")\n",
        "    productos_data = []\n",
        "    \n",
        "    for producto in productos_cass:\n",
        "        productos_data.append({\n",
        "            'producto_id': producto.producto_id,\n",
        "            'nombre': producto.nombre,\n",
        "            'categoria': producto.categoria,\n",
        "            'precio': float(producto.precio),\n",
        "            'descripcion': producto.descripcion,\n",
        "            'activo': producto.activo\n",
        "        })\n",
        "    \n",
        "    print(f\"✅ Extraídos {len(productos_data)} productos de Cassandra\")\n",
        "    \n",
        "    # Crear DataFrame de Spark desde datos Cassandra\n",
        "    productos_schema = StructType([\n",
        "        StructField(\"producto_id\", StringType(), False),\n",
        "        StructField(\"nombre\", StringType(), True),\n",
        "        StructField(\"categoria\", StringType(), True),\n",
        "        StructField(\"precio\", DoubleType(), True),\n",
        "        StructField(\"descripcion\", StringType(), True),\n",
        "        StructField(\"activo\", BooleanType(), True)\n",
        "    ])\n",
        "    \n",
        "    productos_df = spark.createDataFrame(productos_data, schema=productos_schema)\n",
        "    \n",
        "    print(\"📋 PRODUCTOS CARGADOS EN SPARK:\")\n",
        "    productos_df.show()\n",
        "    \n",
        "    # Leer tabla de ventas\n",
        "    print(\"\\n💰 CARGANDO VENTAS DESDE CASSANDRA:\")\n",
        "    \n",
        "    ventas_cass = session.execute(\"SELECT * FROM ventas_por_fecha\")\n",
        "    ventas_data = []\n",
        "    \n",
        "    for venta in ventas_cass:\n",
        "        ventas_data.append({\n",
        "            'fecha_venta': str(venta.fecha_venta),\n",
        "            'hora_venta': str(venta.hora_venta),\n",
        "            'venta_id': str(venta.venta_id),\n",
        "            'producto_id': venta.producto_id,\n",
        "            'cantidad': venta.cantidad,\n",
        "            'monto': float(venta.monto),\n",
        "            'cliente_id': venta.cliente_id\n",
        "        })\n",
        "    \n",
        "    print(f\"✅ Extraídas {len(ventas_data)} ventas de Cassandra\")\n",
        "    \n",
        "    # Crear DataFrame de ventas\n",
        "    ventas_schema = StructType([\n",
        "        StructField(\"fecha_venta\", StringType(), False),\n",
        "        StructField(\"hora_venta\", StringType(), False),\n",
        "        StructField(\"venta_id\", StringType(), False),\n",
        "        StructField(\"producto_id\", StringType(), True),\n",
        "        StructField(\"cantidad\", IntegerType(), True),\n",
        "        StructField(\"monto\", DoubleType(), True),\n",
        "        StructField(\"cliente_id\", StringType(), True)\n",
        "    ])\n",
        "    \n",
        "    ventas_df = spark.createDataFrame(ventas_data, schema=ventas_schema)\n",
        "    \n",
        "    # Convertir fecha a formato Date\n",
        "    ventas_df = ventas_df.withColumn(\"fecha\", to_date(col(\"fecha_venta\"), \"yyyy-MM-dd\"))\n",
        "    \n",
        "    print(\"📋 MUESTRA DE VENTAS CARGADAS EN SPARK:\")\n",
        "    ventas_df.select(\"fecha\", \"producto_id\", \"cantidad\", \"monto\", \"cliente_id\").show(10)\n",
        "    \n",
        "    print(\"\\n✅ Datos de Cassandra cargados exitosamente en Spark\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error en integración Spark + Cassandra: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    productos_df = None\n",
        "    ventas_df = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 🔬 ANÁLISIS AVANZADO CON SPARK + CASSANDRA\n",
        "\n",
        "if 'productos_df' in locals() and productos_df and 'ventas_df' in locals() and ventas_df:\n",
        "    print(\"🔬 INICIANDO ANÁLISIS AVANZADO CON DATOS DE CASSANDRA...\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # PASO 2: JOINS ENTRE DATAFRAMES DE CASSANDRA\n",
        "    print(\"\\n2️⃣ REALIZANDO JOINS ENTRE TABLAS DE CASSANDRA:\")\n",
        "    \n",
        "    # Join entre productos y ventas\n",
        "    ventas_con_productos = ventas_df.join(\n",
        "        productos_df,\n",
        "        ventas_df.producto_id == productos_df.producto_id,\n",
        "        \"inner\"\n",
        "    ).select(\n",
        "        ventas_df.fecha,\n",
        "        ventas_df.hora_venta,\n",
        "        ventas_df.venta_id,\n",
        "        productos_df.nombre.alias(\"producto_nombre\"),\n",
        "        productos_df.categoria,\n",
        "        productos_df.precio.alias(\"precio_unitario\"),\n",
        "        ventas_df.cantidad,\n",
        "        ventas_df.monto,\n",
        "        ventas_df.cliente_id\n",
        "    ).withColumn(\n",
        "        \"revenue_calculado\", \n",
        "        col(\"precio_unitario\") * col(\"cantidad\")\n",
        "    )\n",
        "    \n",
        "    print(\"✅ Join completado entre productos y ventas\")\n",
        "    print(\"\\n📊 MUESTRA DE DATOS UNIDOS:\")\n",
        "    ventas_con_productos.show(10, truncate=False)\n",
        "    \n",
        "    # PASO 3: ANÁLISIS TEMPORAL DE VENTAS\n",
        "    print(\"\\n3️⃣ ANÁLISIS TEMPORAL DE VENTAS:\")\n",
        "    \n",
        "    # Registrar como tablas temporales para SQL\n",
        "    ventas_con_productos.createOrReplaceTempView(\"ventas_completas\")\n",
        "    productos_df.createOrReplaceTempView(\"productos_cassandra\")\n",
        "    ventas_df.createOrReplaceTempView(\"ventas_cassandra\")\n",
        "    \n",
        "    # Análisis 1: Ventas por día\n",
        "    print(\"\\n📈 VENTAS DIARIAS:\")\n",
        "    ventas_diarias = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            fecha,\n",
        "            COUNT(*) as num_transacciones,\n",
        "            SUM(cantidad) as unidades_vendidas,\n",
        "            SUM(monto) as revenue_total,\n",
        "            AVG(monto) as ticket_promedio,\n",
        "            COUNT(DISTINCT cliente_id) as clientes_unicos\n",
        "        FROM ventas_completas\n",
        "        GROUP BY fecha\n",
        "        ORDER BY fecha\n",
        "    \"\"\")\n",
        "    \n",
        "    ventas_diarias.show()\n",
        "    \n",
        "    # Análisis 2: Productos más vendidos\n",
        "    print(\"\\n🏆 TOP PRODUCTOS MÁS VENDIDOS:\")\n",
        "    top_productos = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            producto_nombre,\n",
        "            categoria,\n",
        "            COUNT(*) as num_ventas,\n",
        "            SUM(cantidad) as unidades_totales,\n",
        "            SUM(monto) as revenue_total,\n",
        "            AVG(monto) as ticket_promedio\n",
        "        FROM ventas_completas\n",
        "        GROUP BY producto_nombre, categoria\n",
        "        ORDER BY revenue_total DESC\n",
        "    \"\"\")\n",
        "    \n",
        "    top_productos.show()\n",
        "    \n",
        "    # Análisis 3: Patrón horario de ventas\n",
        "    print(\"\\n⏰ PATRÓN HORARIO DE VENTAS:\")\n",
        "    patron_horario = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            SUBSTRING(hora_venta, 1, 2) as hora,\n",
        "            COUNT(*) as num_ventas,\n",
        "            SUM(monto) as revenue_total,\n",
        "            AVG(monto) as ticket_promedio\n",
        "        FROM ventas_completas\n",
        "        GROUP BY SUBSTRING(hora_venta, 1, 2)\n",
        "        ORDER BY hora\n",
        "    \"\"\")\n",
        "    \n",
        "    patron_horario.show()\n",
        "    \n",
        "    # PASO 4: ANÁLISIS POR CATEGORÍAS\n",
        "    print(\"\\n4️⃣ ANÁLISIS DETALLADO POR CATEGORÍAS:\")\n",
        "    \n",
        "    categoria_analysis = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            categoria,\n",
        "            COUNT(DISTINCT producto_nombre) as productos_diferentes,\n",
        "            COUNT(*) as total_transacciones,\n",
        "            SUM(cantidad) as unidades_vendidas,\n",
        "            SUM(monto) as revenue_total,\n",
        "            AVG(monto) as ticket_promedio,\n",
        "            MIN(monto) as venta_minima,\n",
        "            MAX(monto) as venta_maxima,\n",
        "            COUNT(DISTINCT cliente_id) as clientes_unicos\n",
        "        FROM ventas_completas\n",
        "        GROUP BY categoria\n",
        "        ORDER BY revenue_total DESC\n",
        "    \"\"\")\n",
        "    \n",
        "    categoria_analysis.show()\n",
        "    \n",
        "    # PASO 5: ANÁLISIS DE CLIENTES\n",
        "    print(\"\\n5️⃣ ANÁLISIS DE COMPORTAMIENTO DE CLIENTES:\")\n",
        "    \n",
        "    cliente_analysis = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            COUNT(DISTINCT cliente_id) as total_clientes,\n",
        "            AVG(compras_por_cliente) as avg_compras_por_cliente,\n",
        "            AVG(gasto_por_cliente) as avg_gasto_por_cliente,\n",
        "            MAX(compras_por_cliente) as max_compras_cliente,\n",
        "            MAX(gasto_por_cliente) as max_gasto_cliente\n",
        "        FROM (\n",
        "            SELECT \n",
        "                cliente_id,\n",
        "                COUNT(*) as compras_por_cliente,\n",
        "                SUM(monto) as gasto_por_cliente\n",
        "            FROM ventas_completas\n",
        "            GROUP BY cliente_id\n",
        "        ) cliente_stats\n",
        "    \"\"\")\n",
        "    \n",
        "    cliente_analysis.show()\n",
        "    \n",
        "    # Top clientes\n",
        "    print(\"\\n👑 TOP 10 CLIENTES POR REVENUE:\")\n",
        "    top_clientes = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            cliente_id,\n",
        "            COUNT(*) as num_compras,\n",
        "            SUM(cantidad) as unidades_compradas,\n",
        "            SUM(monto) as gasto_total,\n",
        "            AVG(monto) as ticket_promedio,\n",
        "            COUNT(DISTINCT categoria) as categorias_diferentes\n",
        "        FROM ventas_completas\n",
        "        GROUP BY cliente_id\n",
        "        ORDER BY gasto_total DESC\n",
        "        LIMIT 10\n",
        "    \"\"\")\n",
        "    \n",
        "    top_clientes.show()\n",
        "    \n",
        "    # PASO 6: MÉTRICAS DE RENDIMIENTO Y CACHING\n",
        "    print(\"\\n6️⃣ OPTIMIZACIÓN Y CACHING:\")\n",
        "    \n",
        "    # Cache de DataFrames frecuentemente usados\n",
        "    ventas_con_productos.cache()\n",
        "    print(\"✅ DataFrame ventas_con_productos cacheado\")\n",
        "    \n",
        "    # Contar registros (fuerza la evaluación del cache)\n",
        "    total_registros = ventas_con_productos.count()\n",
        "    print(f\"📊 Total de registros en análisis: {total_registros:,}\")\n",
        "    \n",
        "    # Estadísticas de particionamiento\n",
        "    print(f\"📊 Número de particiones: {ventas_con_productos.rdd.getNumPartitions()}\")\n",
        "    \n",
        "    # PASO 7: AGREGACIONES COMPLEJAS CON WINDOW FUNCTIONS\n",
        "    print(\"\\n7️⃣ ANÁLISIS AVANZADO CON WINDOW FUNCTIONS:\")\n",
        "    \n",
        "    from pyspark.sql.window import Window\n",
        "    \n",
        "    # Ranking de productos por día\n",
        "    window_spec = Window.partitionBy(\"fecha\").orderBy(col(\"revenue_total\").desc())\n",
        "    \n",
        "    ranking_diario = ventas_con_productos.groupBy(\"fecha\", \"producto_nombre\", \"categoria\") \\\n",
        "        .agg(\n",
        "            sum(\"monto\").alias(\"revenue_total\"),\n",
        "            sum(\"cantidad\").alias(\"unidades_vendidas\"),\n",
        "            count(\"*\").alias(\"num_transacciones\")\n",
        "        ) \\\n",
        "        .withColumn(\"ranking_dia\", row_number().over(window_spec)) \\\n",
        "        .filter(col(\"ranking_dia\") <= 3) \\\n",
        "        .orderBy(\"fecha\", \"ranking_dia\")\n",
        "    \n",
        "    print(\"\\n🏆 TOP 3 PRODUCTOS POR DÍA:\")\n",
        "    ranking_diario.show(20, truncate=False)\n",
        "    \n",
        "    print(\"\\n🎉 ¡Análisis avanzado completado exitosamente!\")\n",
        "    \n",
        "else:\n",
        "    print(\"❌ No hay datos disponibles para análisis\")\n",
        "    print(\"💡 Ejecuta las celdas anteriores para cargar los datos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔄 **Casos de Uso Híbridos: Spark + HBase + Cassandra**\n",
        "\n",
        "### 🎯 **Escenarios del Mundo Real:**\n",
        "\n",
        "#### **1. 🏪 E-commerce Multi-canal**\n",
        "- **HBase**: Catálogo de productos, inventario en tiempo real\n",
        "- **Cassandra**: Historial de clicks, eventos de usuario, logs\n",
        "- **Spark**: Análisis de comportamiento, recomendaciones, ETL\n",
        "\n",
        "#### **2. 🏦 Sistema Bancario**\n",
        "- **HBase**: Transacciones en tiempo real, balances de cuentas\n",
        "- **Cassandra**: Logs de auditoría, historial de transacciones\n",
        "- **Spark**: Detección de fraude, análisis de riesgo, reportes\n",
        "\n",
        "#### **3. 🌐 IoT y Telemetría**\n",
        "- **HBase**: Estados actuales de sensores, configuraciones\n",
        "- **Cassandra**: Series temporales de sensores, métricas\n",
        "- **Spark**: Análisis de patrones, alertas, predicciones\n",
        "\n",
        "#### **4. 📱 Aplicación Social**\n",
        "- **HBase**: Perfiles de usuario, relaciones sociales\n",
        "- **Cassandra**: Timeline de posts, mensajes, notificaciones\n",
        "- **Spark**: Análisis de sentimientos, trending topics, ML\n",
        "\n",
        "### 🏗️ **Arquitectura Híbrida:**\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│                    HYBRID BIG DATA ARCHITECTURE                 │\n",
        "├─────────────────────────────────────────────────────────────────┤\n",
        "│                                                                 │\n",
        "│  ┌─────────────────┐    ┌─────────────────┐    ┌─────────────┐  │\n",
        "│  │   Data Sources  │    │  Processing     │    │   Storage   │  │\n",
        "│  │                 │    │                 │    │             │  │\n",
        "│  │ • Web Apps      │───►│ ┌─────────────┐ │───►│ ┌─────────┐ │  │\n",
        "│  │ • Mobile Apps   │    │ │   Apache    │ │    │ │ HBase   │ │  │\n",
        "│  │ • IoT Sensors   │    │ │   Spark     │ │    │ │ (OLTP)  │ │  │\n",
        "│  │ • APIs          │    │ │             │ │    │ └─────────┘ │  │\n",
        "│  │ • Batch Files   │    │ │ • Driver    │ │    │             │  │\n",
        "│  └─────────────────┘    │ │ • Workers   │ │    │ ┌─────────┐ │  │\n",
        "│                         │ │ • SQL       │ │    │ │Cassandra│ │  │\n",
        "│                         │ │ • MLlib     │ │    │ │(Analytics)│ │  │\n",
        "│                         │ │ • Streaming │ │    │ └─────────┘ │  │\n",
        "│                         │ └─────────────┘ │    │             │  │\n",
        "│                         └─────────────────┘    │ ┌─────────┐ │  │\n",
        "│                                                │ │  HDFS   │ │  │\n",
        "│                                                │ │(Archive)│ │  │\n",
        "│                                                │ └─────────┘ │  │\n",
        "│                                                └─────────────┘  │\n",
        "│                                                                 │\n",
        "│  ┌─────────────────────────────────────────────────────────┐   │\n",
        "│  │                    DATA FLOW                            │   │\n",
        "│  │                                                         │   │\n",
        "│  │  Real-time ────► HBase ────► Spark ────► Cassandra     │   │\n",
        "│  │  (OLTP)          (Fast)     (Process)   (Analytics)    │   │\n",
        "│  │                                                         │   │\n",
        "│  │  Batch ─────────► HDFS ────► Spark ────► HBase/Cass    │   │\n",
        "│  │  (ETL)           (Storage)   (Process)   (Serve)       │   │\n",
        "│  └─────────────────────────────────────────────────────────┘   │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "### 💡 **Mejores Prácticas de Integración:**\n",
        "\n",
        "#### **🔧 Configuración:**\n",
        "- **Connection Pooling**: Reutilizar conexiones entre Spark tasks\n",
        "- **Batch Size**: Optimizar tamaño de lotes para escritura\n",
        "- **Partitioning**: Alinear particiones Spark con particiones NoSQL\n",
        "- **Caching**: Cache DataFrames frecuentemente accedidos\n",
        "\n",
        "#### **⚡ Rendimiento:**\n",
        "- **Predicate Pushdown**: Filtros aplicados en la fuente\n",
        "- **Columnar Projection**: Leer solo columnas necesarias\n",
        "- **Locality Awareness**: Ejecutar tareas cerca de los datos\n",
        "- **Parallel Processing**: Maximizar paralelización\n",
        "\n",
        "#### **🛡️ Confiabilidad:**\n",
        "- **Retry Logic**: Reintentos automáticos en fallos\n",
        "- **Circuit Breakers**: Protección contra cascading failures\n",
        "- **Monitoring**: Métricas de rendimiento y salud\n",
        "- **Backup Strategies**: Estrategias de respaldo híbridas\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 🌟 CASO DE USO HÍBRIDO: ANÁLISIS CROSS-PLATFORM\n",
        "\n",
        "print(\"🌟 DEMOSTRANDO CASO DE USO HÍBRIDO...\")\n",
        "print(\"=\"*45)\n",
        "print(\"📊 Combinando datos de HBase y Cassandra en un análisis unificado\")\n",
        "\n",
        "# Verificar que tenemos datos de ambas fuentes\n",
        "hbase_available = 'transformed_df' in locals() and transformed_df is not None\n",
        "cassandra_available = 'ventas_con_productos' in locals() and ventas_con_productos is not None\n",
        "\n",
        "if hbase_available and cassandra_available:\n",
        "    print(\"\\n✅ Datos disponibles de ambas fuentes:\")\n",
        "    print(f\"   🏛️ HBase: DataFrame 'transformed_df'\")\n",
        "    print(f\"   🌟 Cassandra: DataFrame 'ventas_con_productos'\")\n",
        "    \n",
        "    # PASO 1: ANÁLISIS COMPARATIVO DE INVENTARIOS\n",
        "    print(\"\\n1️⃣ ANÁLISIS COMPARATIVO DE INVENTARIOS:\")\n",
        "    \n",
        "    # Datos de HBase (inventario actual)\n",
        "    print(\"\\n📦 INVENTARIO ACTUAL (HBase):\")\n",
        "    hbase_inventory = transformed_df.select(\n",
        "        col(\"product_id\"),\n",
        "        col(\"name\").alias(\"producto_nombre\"),\n",
        "        col(\"category\").alias(\"categoria\"),\n",
        "        col(\"price\").alias(\"precio\"),\n",
        "        col(\"stock\"),\n",
        "        col(\"inventory_value\")\n",
        "    )\n",
        "    \n",
        "    hbase_inventory.show()\n",
        "    \n",
        "    # Datos de Cassandra (ventas históricas)\n",
        "    print(\"\\n💰 RESUMEN DE VENTAS (Cassandra):\")\n",
        "    cassandra_sales = ventas_con_productos.groupBy(\"producto_nombre\", \"categoria\") \\\n",
        "        .agg(\n",
        "            sum(\"cantidad\").alias(\"total_vendido\"),\n",
        "            sum(\"monto\").alias(\"revenue_total\"),\n",
        "            count(\"*\").alias(\"num_transacciones\"),\n",
        "            avg(\"monto\").alias(\"ticket_promedio\")\n",
        "        )\n",
        "    \n",
        "    cassandra_sales.show()\n",
        "    \n",
        "    # PASO 2: ANÁLISIS HÍBRIDO - ROTACIÓN DE INVENTARIO\n",
        "    print(\"\\n2️⃣ ANÁLISIS HÍBRIDO - ROTACIÓN DE INVENTARIO:\")\n",
        "    \n",
        "    # Unir datos de ambas fuentes por nombre de producto\n",
        "    inventory_turnover = hbase_inventory.join(\n",
        "        cassandra_sales,\n",
        "        hbase_inventory.producto_nombre == cassandra_sales.producto_nombre,\n",
        "        \"left_outer\"\n",
        "    ).select(\n",
        "        hbase_inventory.product_id,\n",
        "        hbase_inventory.producto_nombre,\n",
        "        hbase_inventory.categoria,\n",
        "        hbase_inventory.precio,\n",
        "        hbase_inventory.stock,\n",
        "        hbase_inventory.inventory_value,\n",
        "        coalesce(cassandra_sales.total_vendido, lit(0)).alias(\"total_vendido\"),\n",
        "        coalesce(cassandra_sales.revenue_total, lit(0.0)).alias(\"revenue_total\"),\n",
        "        coalesce(cassandra_sales.num_transacciones, lit(0)).alias(\"num_transacciones\")\n",
        "    ).withColumn(\n",
        "        \"turnover_ratio\",\n",
        "        when(col(\"stock\") > 0, col(\"total_vendido\") / col(\"stock\")).otherwise(0)\n",
        "    ).withColumn(\n",
        "        \"stock_days\",\n",
        "        when(col(\"total_vendido\") > 0, col(\"stock\") * 30 / col(\"total_vendido\")).otherwise(999)\n",
        "    ).withColumn(\n",
        "        \"performance_score\",\n",
        "        (col(\"turnover_ratio\") * 0.4) + \n",
        "        (when(col(\"stock_days\") < 30, 1.0).otherwise(0.5) * 0.3) +\n",
        "        (col(\"revenue_total\") / 10000 * 0.3)\n",
        "    )\n",
        "    \n",
        "    print(\"\\n📊 ANÁLISIS DE ROTACIÓN DE INVENTARIO:\")\n",
        "    inventory_turnover.select(\n",
        "        \"product_id\", \"producto_nombre\", \"categoria\", \"stock\", \n",
        "        \"total_vendido\", \"turnover_ratio\", \"stock_days\", \"performance_score\"\n",
        "    ).orderBy(col(\"performance_score\").desc()).show()\n",
        "    \n",
        "    # PASO 3: RECOMENDACIONES INTELIGENTES\n",
        "    print(\"\\n3️⃣ RECOMENDACIONES INTELIGENTES BASADAS EN DATOS HÍBRIDOS:\")\n",
        "    \n",
        "    # Registrar vista temporal\n",
        "    inventory_turnover.createOrReplaceTempView(\"inventory_analysis\")\n",
        "    \n",
        "    # Productos con alto rendimiento\n",
        "    high_performers = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            product_id,\n",
        "            producto_nombre,\n",
        "            categoria,\n",
        "            stock,\n",
        "            total_vendido,\n",
        "            revenue_total,\n",
        "            ROUND(turnover_ratio, 2) as turnover_ratio,\n",
        "            ROUND(stock_days, 1) as stock_days,\n",
        "            ROUND(performance_score, 2) as performance_score,\n",
        "            CASE \n",
        "                WHEN performance_score > 1.5 THEN '🚀 Estrella'\n",
        "                WHEN performance_score > 1.0 THEN '⭐ Alto rendimiento'\n",
        "                WHEN performance_score > 0.5 THEN '📈 Promedio'\n",
        "                ELSE '⚠️ Bajo rendimiento'\n",
        "            END as clasificacion\n",
        "        FROM inventory_analysis\n",
        "        ORDER BY performance_score DESC\n",
        "    \"\"\")\n",
        "    \n",
        "    print(\"\\n🏆 CLASIFICACIÓN DE PRODUCTOS POR RENDIMIENTO:\")\n",
        "    high_performers.show(truncate=False)\n",
        "    \n",
        "    # Alertas de inventario\n",
        "    print(\"\\n🚨 ALERTAS DE INVENTARIO:\")\n",
        "    alerts = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            product_id,\n",
        "            producto_nombre,\n",
        "            categoria,\n",
        "            stock,\n",
        "            stock_days,\n",
        "            CASE \n",
        "                WHEN stock_days < 7 THEN '🔴 CRÍTICO: Restock inmediato'\n",
        "                WHEN stock_days < 15 THEN '🟡 ADVERTENCIA: Planificar restock'\n",
        "                WHEN stock_days > 90 THEN '🔵 INFO: Exceso de inventario'\n",
        "                ELSE '✅ OK: Niveles normales'\n",
        "            END as alerta\n",
        "        FROM inventory_analysis\n",
        "        WHERE stock_days < 15 OR stock_days > 90\n",
        "        ORDER BY stock_days ASC\n",
        "    \"\"\")\n",
        "    \n",
        "    alerts.show(truncate=False)\n",
        "    \n",
        "    # PASO 4: ESTADÍSTICAS FINALES DEL ANÁLISIS HÍBRIDO\n",
        "    print(\"\\n4️⃣ ESTADÍSTICAS FINALES DEL ANÁLISIS HÍBRIDO:\")\n",
        "    \n",
        "    final_stats = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            COUNT(*) as total_productos,\n",
        "            SUM(stock) as total_stock_units,\n",
        "            SUM(inventory_value) as total_inventory_value,\n",
        "            SUM(total_vendido) as total_units_sold,\n",
        "            SUM(revenue_total) as total_revenue,\n",
        "            AVG(turnover_ratio) as avg_turnover_ratio,\n",
        "            AVG(stock_days) as avg_stock_days,\n",
        "            COUNT(CASE WHEN stock_days < 15 THEN 1 END) as productos_criticos,\n",
        "            COUNT(CASE WHEN performance_score > 1.0 THEN 1 END) as productos_alto_rendimiento\n",
        "        FROM inventory_analysis\n",
        "    \"\"\").collect()[0]\n",
        "    \n",
        "    print(\"\\n📊 RESUMEN EJECUTIVO:\")\n",
        "    print(f\"   📦 Total productos analizados: {final_stats['total_productos']}\")\n",
        "    print(f\"   📈 Total unidades en stock: {final_stats['total_stock_units']:,}\")\n",
        "    print(f\"   💰 Valor total inventario: ${final_stats['total_inventory_value']:,.2f}\")\n",
        "    print(f\"   📊 Total unidades vendidas: {final_stats['total_units_sold']:,}\")\n",
        "    print(f\"   💵 Revenue total: ${final_stats['total_revenue']:,.2f}\")\n",
        "    print(f\"   🔄 Ratio rotación promedio: {final_stats['avg_turnover_ratio']:.2f}\")\n",
        "    print(f\"   📅 Días promedio de stock: {final_stats['avg_stock_days']:.1f}\")\n",
        "    print(f\"   🚨 Productos críticos: {final_stats['productos_criticos']}\")\n",
        "    print(f\"   ⭐ Productos alto rendimiento: {final_stats['productos_alto_rendimiento']}\")\n",
        "    \n",
        "    # PASO 5: EXPORTAR RESULTADOS\n",
        "    print(\"\\n5️⃣ PREPARANDO RESULTADOS PARA EXPORT:\")\n",
        "    \n",
        "    # Cache del análisis final para uso posterior\n",
        "    inventory_turnover.cache()\n",
        "    high_performers.cache()\n",
        "    \n",
        "    print(\"✅ DataFrames de análisis híbrido cacheados\")\n",
        "    print(\"💾 Listos para exportar a sistemas downstream\")\n",
        "    \n",
        "    print(\"\\n🎉 ¡ANÁLISIS HÍBRIDO COMPLETADO EXITOSAMENTE!\")\n",
        "    print(\"🔗 Datos de HBase y Cassandra integrados con éxito\")\n",
        "    print(\"📈 Insights accionables generados para el negocio\")\n",
        "\n",
        "elif hbase_available:\n",
        "    print(\"⚠️ Solo datos de HBase disponibles\")\n",
        "    print(\"💡 Ejecuta las celdas de integración con Cassandra para análisis completo\")\n",
        "    \n",
        "elif cassandra_available:\n",
        "    print(\"⚠️ Solo datos de Cassandra disponibles\")\n",
        "    print(\"💡 Ejecuta las celdas de integración con HBase para análisis completo\")\n",
        "    \n",
        "else:\n",
        "    print(\"❌ No hay datos disponibles de ninguna fuente\")\n",
        "    print(\"💡 Ejecuta las celdas anteriores para cargar datos de HBase y Cassandra\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎉 **Resumen y Conclusiones del Tutorial**\n",
        "\n",
        "### ✅ **Lo que Hemos Logrado:**\n",
        "\n",
        "#### **1. 🚀 Configuración Spark Optimizada**\n",
        "- ✅ Sesión Spark configurada para integración NoSQL\n",
        "- ✅ Recursos optimizados para cluster mode\n",
        "- ✅ Configuraciones específicas para conectores NoSQL\n",
        "- ✅ Monitoreo y verificación de estado del cluster\n",
        "\n",
        "#### **2. 🏛️ Integración HBase + Spark**\n",
        "- ✅ Conexión exitosa entre Spark y HBase\n",
        "- ✅ Lectura de datos desde HBase a DataFrames\n",
        "- ✅ Transformaciones distribuidas con Spark SQL\n",
        "- ✅ Análisis de ROI y agregaciones avanzadas\n",
        "- ✅ Caching para optimización de rendimiento\n",
        "\n",
        "#### **3. 🌟 Integración Cassandra + Spark**\n",
        "- ✅ Conexión exitosa entre Spark y Cassandra\n",
        "- ✅ Análisis temporal de series de datos\n",
        "- ✅ Joins complejos entre tablas distribuidas\n",
        "- ✅ Window functions para rankings dinámicos\n",
        "- ✅ Análisis de comportamiento de clientes\n",
        "\n",
        "#### **4. 🔄 Casos de Uso Híbridos**\n",
        "- ✅ Integración de datos de múltiples fuentes NoSQL\n",
        "- ✅ Análisis de rotación de inventario cross-platform\n",
        "- ✅ Generación de alertas inteligentes\n",
        "- ✅ Recomendaciones basadas en datos híbridos\n",
        "- ✅ Métricas de rendimiento empresarial\n",
        "\n",
        "### 📊 **Arquitectura Final Implementada:**\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│                  ECOSISTEMA BIG DATA COMPLETO                  │\n",
        "├─────────────────────────────────────────────────────────────────┤\n",
        "│                                                                 │\n",
        "│  ┌─────────────────┐    ┌─────────────────┐    ┌─────────────┐  │\n",
        "│  │   SPARK 3.5.3   │◄──►│     HBASE       │    │  CASSANDRA  │  │\n",
        "│  │                 │    │                 │    │             │  │\n",
        "│  │ • Driver: 1.2GB │    │ • Thrift API    │    │ • CQL API   │  │\n",
        "│  │ • Workers: 2x   │    │ • Column Store  │    │ • Ring Arch │  │\n",
        "│  │ • Executors:800M│    │ • HDFS Backend  │    │ • P2P Nodes │  │\n",
        "│  │ • Hive Support  │    │ • Strong Cons.  │    │ • Eventual  │  │\n",
        "│  │ • SQL Engine    │    │ • Real-time     │    │ • Scalable  │  │\n",
        "│  └─────────────────┘    └─────────────────┘    └─────────────┘  │\n",
        "│           │                       │                      │       │\n",
        "│           └───────────────────────┼──────────────────────┘       │\n",
        "│                                   │                              │\n",
        "│  ┌─────────────────────────────────────────────────────────┐     │\n",
        "│  │                  JUPYTER ECOSYSTEM                      │     │\n",
        "│  │                                                         │     │\n",
        "│  │  📓 01_spark_cluster_professional.ipynb               │     │\n",
        "│  │  📓 02_nosql_foundations.ipynb                         │     │\n",
        "│  │  📓 03_spark_nosql_integration.ipynb                  │     │\n",
        "│  │                                                         │     │\n",
        "│  │  • Tutoriales completos CRUD                           │     │\n",
        "│  │  • Casos de uso reales                                 │     │\n",
        "│  │  • Mejores prácticas                                   │     │\n",
        "│  │  • Análisis híbridos                                   │     │\n",
        "│  └─────────────────────────────────────────────────────────┘     │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "### 🎯 **Casos de Uso Implementados:**\n",
        "\n",
        "#### **🏪 E-commerce Analytics**\n",
        "- Análisis de inventario en tiempo real\n",
        "- Tracking de ventas y comportamiento de clientes\n",
        "- Recomendaciones de productos\n",
        "- Alertas de stock crítico\n",
        "\n",
        "#### **📊 Business Intelligence**\n",
        "- Dashboards en tiempo real\n",
        "- Análisis de tendencias temporales\n",
        "- Métricas de rendimiento KPI\n",
        "- Reportes ejecutivos automatizados\n",
        "\n",
        "#### **🔍 Data Science & ML**\n",
        "- Feature engineering distribuido\n",
        "- Análisis exploratorio de datos\n",
        "- Preparación de datasets para ML\n",
        "- Análisis de patrones complejos\n",
        "\n",
        "### 💡 **Mejores Prácticas Aplicadas:**\n",
        "\n",
        "#### **⚡ Rendimiento:**\n",
        "- ✅ Caching estratégico de DataFrames\n",
        "- ✅ Particionamiento optimizado\n",
        "- ✅ Pushdown de predicados\n",
        "- ✅ Configuración de memoria ajustada\n",
        "\n",
        "#### **🛡️ Confiabilidad:**\n",
        "- ✅ Manejo de errores robusto\n",
        "- ✅ Verificación de conectividad\n",
        "- ✅ Timeouts configurados\n",
        "- ✅ Logging estructurado\n",
        "\n",
        "#### **🔧 Mantenibilidad:**\n",
        "- ✅ Código bien documentado\n",
        "- ✅ Estructura modular\n",
        "- ✅ Configuración externalizada\n",
        "- ✅ Ejemplos reproducibles\n",
        "\n",
        "### 🚀 **Próximos Pasos Sugeridos:**\n",
        "\n",
        "1. **📈 Escalabilidad**: Implementar en clusters multi-nodo\n",
        "2. **🔄 Streaming**: Agregar Spark Streaming para datos en tiempo real  \n",
        "3. **🤖 Machine Learning**: Integrar MLlib para modelos predictivos\n",
        "4. **📊 Visualización**: Conectar con herramientas como Grafana/Tableau\n",
        "5. **🔐 Seguridad**: Implementar autenticación y autorización\n",
        "6. **📦 Productización**: Containerización con Kubernetes\n",
        "\n",
        "### 🎓 **Conocimientos Adquiridos:**\n",
        "\n",
        "- ✅ Configuración avanzada de Spark para entornos distribuidos\n",
        "- ✅ Operaciones CRUD completas en HBase y Cassandra\n",
        "- ✅ Integración seamless entre tecnologías Big Data\n",
        "- ✅ Análisis de datos híbridos y cross-platform\n",
        "- ✅ Optimización de rendimiento en ecosistemas complejos\n",
        "- ✅ Casos de uso reales de la industria\n",
        "\n",
        "---\n",
        "\n",
        "## 🎉 **¡Felicitaciones!**\n",
        "\n",
        "Has completado exitosamente un tutorial integral de **Apache Spark + NoSQL** que cubre desde conceptos fundamentales hasta implementaciones avanzadas de casos de uso híbridos. Este conocimiento te permitirá diseñar e implementar soluciones de Big Data robustas y escalables en entornos de producción.\n",
        "\n",
        "**¡Continúa explorando y construyendo soluciones increíbles con Big Data! 🚀**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 🚀 CREACIÓN DE SESIÓN SPARK INTEGRADA CON NoSQL\n",
        "\n",
        "print(\"🚀 CREANDO SESIÓN SPARK PARA INTEGRACIÓN NoSQL...\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "# Detener cualquier sesión existente\n",
        "try:\n",
        "    spark.stop()\n",
        "    print(\"🛑 Sesión anterior detenida\")\n",
        "    time.sleep(3)\n",
        "except:\n",
        "    print(\"🔍 No había sesión activa\")\n",
        "\n",
        "# CONFIGURACIÓN OPTIMIZADA PARA INTEGRACIÓN NoSQL\n",
        "print(\"\\n⚙️ Configurando Spark con conectores NoSQL...\")\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"EducacionIT-Spark-NoSQL-Integration\") \\\n",
        "    .master(\"spark://master:7077\") \\\n",
        "    .config(\"spark.driver.memory\", \"1200m\") \\\n",
        "    .config(\"spark.driver.cores\", \"2\") \\\n",
        "    .config(\"spark.executor.memory\", \"800m\") \\\n",
        "    .config(\"spark.executor.cores\", \"1\") \\\n",
        "    .config(\"spark.executor.instances\", \"2\") \\\n",
        "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.cassandra.connection.host\", \"cassandra\") \\\n",
        "    .config(\"spark.cassandra.connection.port\", \"9042\") \\\n",
        "    .config(\"spark.cassandra.connection.keepAlive\", \"true\") \\\n",
        "    .config(\"spark.cassandra.connection.timeout_ms\", \"10000\") \\\n",
        "    .config(\"spark.cassandra.read.timeout_ms\", \"120000\") \\\n",
        "    .config(\"spark.network.timeout\", \"300s\") \\\n",
        "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "# Información de la sesión integrada\n",
        "print(\"\\n🎉 ¡SESIÓN SPARK + NoSQL CREADA EXITOSAMENTE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"🏷️  Aplicación: {spark.sparkContext.appName}\")\n",
        "print(f\"🔗 Master URL: {spark.sparkContext.master}\")\n",
        "print(f\"📊 Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
        "print(f\"🚀 Versión Spark: {spark.version}\")\n",
        "\n",
        "# Verificar configuración de conectores\n",
        "print(f\"\\n🔧 CONFIGURACIÓN DE CONECTORES:\")\n",
        "print(f\"   🌟 Cassandra Host: {spark.conf.get('spark.cassandra.connection.host')}\")\n",
        "print(f\"   🔌 Cassandra Port: {spark.conf.get('spark.cassandra.connection.port')}\")\n",
        "print(f\"   ⏰ Connection Timeout: {spark.conf.get('spark.cassandra.connection.timeout_ms')}ms\")\n",
        "print(f\"   📖 Read Timeout: {spark.conf.get('spark.cassandra.read.timeout_ms')}ms\")\n",
        "\n",
        "# Verificar conectividad con executors\n",
        "print(f\"\\n📊 ESTADO DEL CLUSTER:\")\n",
        "try:\n",
        "    executors = spark.sparkContext.statusTracker().getExecutorInfos()\n",
        "    active_executors = [e for e in executors if e.isActive]\n",
        "    print(f\"   ✅ Executors activos: {len(active_executors)}\")\n",
        "    \n",
        "    total_cores = sum([e.totalCores for e in active_executors])\n",
        "    total_memory = sum([e.maxMemory for e in active_executors])\n",
        "    \n",
        "    print(f\"   🖥️ Total cores: {total_cores}\")\n",
        "    print(f\"   💾 Total memoria: {total_memory / (1024**3):.2f} GB\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"   ⚠️ Error obteniendo información de executors: {e}\")\n",
        "\n",
        "print(\"\\n✅ Spark configurado para integración con HBase y Cassandra\")\n",
        "print(\"🔗 Listo para análisis distribuido híbrido...\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
