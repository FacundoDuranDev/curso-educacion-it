{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ”— Spark + NoSQL Integration - AnÃ¡lisis Distribuido\n",
        "\n",
        "## ğŸ“‹ **GuÃ­a Completa de IntegraciÃ³n Big Data**\n",
        "\n",
        "### ğŸ¯ **Objetivos del Notebook:**\n",
        "1. **Integrar Apache Spark** con HBase y Cassandra\n",
        "2. **Implementar anÃ¡lisis distribuido** sobre datos NoSQL\n",
        "3. **Optimizar rendimiento** de consultas hÃ­bridas\n",
        "4. **Casos de uso reales** de Big Data Analytics\n",
        "5. **Mejores prÃ¡cticas** de arquitectura distribuida\n",
        "\n",
        "### ğŸ—ï¸ **Arquitectura de IntegraciÃ³n:**\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    SPARK + NoSQL ECOSYSTEM                     â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                 â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚\n",
        "â”‚  â”‚   Spark Driver  â”‚    â”‚  Spark Workers  â”‚                    â”‚\n",
        "â”‚  â”‚                 â”‚    â”‚                 â”‚                    â”‚\n",
        "â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                    â”‚\n",
        "â”‚  â”‚ â”‚ SparkSessionâ”‚ â”‚â—„â”€â”€â–ºâ”‚ â”‚  Executors  â”‚ â”‚                    â”‚\n",
        "â”‚  â”‚ â”‚             â”‚ â”‚    â”‚ â”‚             â”‚ â”‚                    â”‚\n",
        "â”‚  â”‚ â”‚ â€¢ Catalyst  â”‚ â”‚    â”‚ â”‚ â€¢ Tasks     â”‚ â”‚                    â”‚\n",
        "â”‚  â”‚ â”‚ â€¢ Optimizer â”‚ â”‚    â”‚ â”‚ â€¢ Cache     â”‚ â”‚                    â”‚\n",
        "â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                    â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚\n",
        "â”‚           â”‚                       â”‚                            â”‚\n",
        "â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚\n",
        "â”‚                       â”‚                                        â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
        "â”‚  â”‚              CONNECTORS                â”‚                   â”‚\n",
        "â”‚  â”‚                    â”‚                    â”‚                   â”‚\n",
        "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                   â”‚\n",
        "â”‚  â”‚  â”‚    HBase        â”‚   Cassandra     â”‚  â”‚                   â”‚\n",
        "â”‚  â”‚  â”‚   Connector     â”‚   Connector     â”‚  â”‚                   â”‚\n",
        "â”‚  â”‚  â”‚                 â”‚                 â”‚  â”‚                   â”‚\n",
        "â”‚  â”‚  â”‚ â€¢ spark-hbase   â”‚ â€¢ spark-cass    â”‚  â”‚                   â”‚\n",
        "â”‚  â”‚  â”‚ â€¢ TableInputs   â”‚ â€¢ CQL Support   â”‚  â”‚                   â”‚\n",
        "â”‚  â”‚  â”‚ â€¢ Bulk Ops      â”‚ â€¢ Token Aware   â”‚  â”‚                   â”‚\n",
        "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                   â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
        "â”‚                       â”‚                                        â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚  â”‚                    DATA SOURCES                         â”‚   â”‚\n",
        "â”‚  â”‚                                                         â”‚   â”‚\n",
        "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚\n",
        "â”‚  â”‚  â”‚     HBase       â”‚      â”‚       Cassandra        â”‚   â”‚   â”‚\n",
        "â”‚  â”‚  â”‚                 â”‚      â”‚                         â”‚   â”‚   â”‚\n",
        "â”‚  â”‚  â”‚ â€¢ Column Store  â”‚      â”‚ â€¢ Wide Column Store     â”‚   â”‚   â”‚\n",
        "â”‚  â”‚  â”‚ â€¢ HDFS Storage  â”‚      â”‚ â€¢ Distributed Nodes     â”‚   â”‚   â”‚\n",
        "â”‚  â”‚  â”‚ â€¢ Strong Cons.  â”‚      â”‚ â€¢ Eventual Cons.        â”‚   â”‚   â”‚\n",
        "â”‚  â”‚  â”‚ â€¢ Real-time     â”‚      â”‚ â€¢ High Throughput       â”‚   â”‚   â”‚\n",
        "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ¯ **Casos de Uso Cubiertos:**\n",
        "\n",
        "#### **1. ğŸ“Š Real-time Analytics**\n",
        "- **Fuente**: Datos streaming en HBase\n",
        "- **Procesamiento**: Agregaciones con Spark\n",
        "- **Destino**: Dashboards en tiempo real\n",
        "\n",
        "#### **2. ğŸŒ IoT Data Processing**\n",
        "- **Fuente**: Sensores â†’ Cassandra\n",
        "- **Procesamiento**: ML con Spark MLlib\n",
        "- **Destino**: Alertas y predicciones\n",
        "\n",
        "#### **3. ğŸ“ˆ Business Intelligence**\n",
        "- **Fuente**: Transacciones en HBase\n",
        "- **Procesamiento**: ETL con Spark SQL\n",
        "- **Destino**: Data Warehouse\n",
        "\n",
        "#### **4. ğŸ” Log Analytics**\n",
        "- **Fuente**: Logs distribuidos en Cassandra\n",
        "- **Procesamiento**: Pattern mining con Spark\n",
        "- **Destino**: Monitoreo y alertas\n",
        "\n",
        "### ğŸ“Š **Beneficios de la IntegraciÃ³n:**\n",
        "\n",
        "| Beneficio | Spark + HBase | Spark + Cassandra |\n",
        "|-----------|---------------|-------------------|\n",
        "| **Consistencia** | Strong (ACID) | Tunable (BASE) |\n",
        "| **Latencia** | Baja (< 100ms) | Ultra-baja (< 10ms) |\n",
        "| **Throughput** | Alto (GB/s) | Muy Alto (TB/s) |\n",
        "| **Escalabilidad** | Vertical + Horizontal | Horizontal Lineal |\n",
        "| **Casos de Uso** | OLTP + Analytics | IoT + Time Series |\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# ğŸš€ CONFIGURACIÃ“N INICIAL - SPARK + NoSQL INTEGRATION\n",
        "\n",
        "import findspark\n",
        "findspark.init('/opt/spark')\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import subprocess\n",
        "\n",
        "# Suprimir warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Importar librerÃ­as de Spark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "print(\"ğŸ”— INICIANDO CONFIGURACIÃ“N SPARK + NoSQL INTEGRATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Verificar servicios requeridos\n",
        "services_status = {}\n",
        "required_services = {\n",
        "    \"master\": (\"Spark Master\", \"7077\"),\n",
        "    \"hbase\": (\"HBase Thrift\", \"9090\"), \n",
        "    \"cassandra\": (\"Cassandra Native\", \"9042\")\n",
        "}\n",
        "\n",
        "print(\"\\n1ï¸âƒ£ VERIFICANDO SERVICIOS REQUERIDOS:\")\n",
        "\n",
        "import socket\n",
        "for service, (description, port) in required_services.items():\n",
        "    try:\n",
        "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "        sock.settimeout(3)\n",
        "        result = sock.connect_ex((service, int(port)))\n",
        "        sock.close()\n",
        "        \n",
        "        if result == 0:\n",
        "            print(f\"   âœ… {description}: Disponible ({service}:{port})\")\n",
        "            services_status[service] = True\n",
        "        else:\n",
        "            print(f\"   âŒ {description}: No disponible ({service}:{port})\")\n",
        "            services_status[service] = False\n",
        "    except Exception as e:\n",
        "        print(f\"   âš ï¸ {description}: Error - {e}\")\n",
        "        services_status[service] = False\n",
        "\n",
        "# Resumen de servicios\n",
        "available_services = sum(services_status.values())\n",
        "total_services = len(services_status)\n",
        "print(f\"\\nğŸ“Š Servicios disponibles: {available_services}/{total_services}\")\n",
        "\n",
        "if available_services >= 2:\n",
        "    print(\"âœ… ConfiguraciÃ³n mÃ­nima disponible para continuar\")\n",
        "else:\n",
        "    print(\"âš ï¸ Servicios insuficientes - algunas funciones pueden no estar disponibles\")\n",
        "\n",
        "# InformaciÃ³n del entorno\n",
        "print(f\"\\n2ï¸âƒ£ INFORMACIÃ“N DEL ENTORNO:\")\n",
        "print(f\"   ğŸ Python: {sys.version.split()[0]}\")\n",
        "print(f\"   ğŸ  Spark Home: {os.environ.get('SPARK_HOME', '/opt/spark')}\")\n",
        "print(f\"   ğŸ–¥ï¸ Hostname: {os.environ.get('HOSTNAME', 'jupyterlab')}\")\n",
        "print(f\"   â° Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "print(\"\\nğŸ¯ ConfiguraciÃ³n inicial completada\")\n",
        "print(\"ğŸš€ Listo para crear sesiÃ³n Spark integrada...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# ğŸš€ CONFIGURACIÃ“N INICIAL Y VERIFICACIÃ“N DEL ENTORNO\n",
        "\n",
        "import findspark\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import warnings\n",
        "from datetime import datetime, date\n",
        "import time\n",
        "\n",
        "# Configurar findspark\n",
        "findspark.init('/opt/spark')\n",
        "\n",
        "# Importar librerÃ­as de PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Suprimir warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"ğŸ”— SPARK + NOSQL INTEGRATION - CONFIGURACIÃ“N INICIAL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# InformaciÃ³n del entorno\n",
        "print(f\"ğŸ“ Spark Home: {os.environ.get('SPARK_HOME', '/opt/spark')}\")\n",
        "print(f\"ğŸ Python Version: {sys.version.split()[0]}\")\n",
        "print(f\"ğŸ–¥ï¸ Hostname: {os.environ.get('HOSTNAME', 'jupyterlab')}\")\n",
        "print(f\"â° Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Verificar servicios NoSQL\n",
        "print(\"\\nğŸ” VERIFICANDO SERVICIOS NOSQL:\")\n",
        "\n",
        "import socket\n",
        "\n",
        "services_to_check = {\n",
        "    \"hbase\": (\"HBase Thrift\", \"9090\"),\n",
        "    \"cassandra\": (\"Cassandra Native\", \"9042\"),\n",
        "    \"master\": (\"Spark Master\", \"7077\")\n",
        "}\n",
        "\n",
        "services_status = {}\n",
        "for service, (description, port) in services_to_check.items():\n",
        "    try:\n",
        "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "        sock.settimeout(3)\n",
        "        result = sock.connect_ex((service, int(port)))\n",
        "        sock.close()\n",
        "        \n",
        "        if result == 0:\n",
        "            print(f\"   âœ… {description}: Disponible en {service}:{port}\")\n",
        "            services_status[service] = True\n",
        "        else:\n",
        "            print(f\"   âŒ {description}: No disponible en {service}:{port}\")\n",
        "            services_status[service] = False\n",
        "    except Exception as e:\n",
        "        print(f\"   âš ï¸ {description}: Error verificando - {e}\")\n",
        "        services_status[service] = False\n",
        "\n",
        "print(f\"\\nğŸ“Š Servicios disponibles: {sum(services_status.values())}/{len(services_status)}\")\n",
        "print(\"âœ… ConfiguraciÃ³n inicial completada\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# ğŸš€ CREAR SESIÃ“N SPARK OPTIMIZADA PARA INTEGRACIÃ“N NOSQL\n",
        "\n",
        "print(\"ğŸš€ CREANDO SESIÃ“N SPARK PARA INTEGRACIÃ“N NOSQL...\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "# Detener cualquier sesiÃ³n existente\n",
        "try:\n",
        "    spark.stop()\n",
        "    print(\"ğŸ›‘ SesiÃ³n anterior detenida correctamente\")\n",
        "except:\n",
        "    print(\"ğŸ” No habÃ­a sesiÃ³n activa\")\n",
        "\n",
        "# CONFIGURACIÃ“N OPTIMIZADA PARA INTEGRACIÃ“N NOSQL\n",
        "# Incluye configuraciones especÃ­ficas para conectores NoSQL\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"EducacionIT-Spark-NoSQL-Integration\") \\\n",
        "    .master(\"spark://master:7077\") \\\n",
        "    .config(\"spark.driver.memory\", \"1200m\") \\\n",
        "    .config(\"spark.driver.cores\", \"2\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"500m\") \\\n",
        "    .config(\"spark.executor.memory\", \"800m\") \\\n",
        "    .config(\"spark.executor.cores\", \"1\") \\\n",
        "    .config(\"spark.executor.instances\", \"2\") \\\n",
        "    .config(\"spark.executor.memoryFraction\", \"0.8\") \\\n",
        "    .config(\"spark.executor.memoryStorageFraction\", \"0.5\") \\\n",
        "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
        "    .config(\"spark.shuffle.service.enabled\", \"false\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
        "    .config(\"spark.network.timeout\", \"300s\") \\\n",
        "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.cassandra.connection.host\", \"cassandra\") \\\n",
        "    .config(\"spark.cassandra.connection.port\", \"9042\") \\\n",
        "    .config(\"spark.cassandra.connection.keep_alive_ms\", \"30000\") \\\n",
        "    .config(\"spark.cassandra.connection.timeout_ms\", \"30000\") \\\n",
        "    .config(\"spark.hbase.host\", \"hbase\") \\\n",
        "    .config(\"spark.hbase.port\", \"9090\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Configurar nivel de logging\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "# InformaciÃ³n de la sesiÃ³n\n",
        "print(\"\\nğŸ‰ Â¡SESIÃ“N SPARK PARA NOSQL CREADA EXITOSAMENTE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"ğŸ·ï¸  AplicaciÃ³n: {spark.sparkContext.appName}\")\n",
        "print(f\"ğŸ”— Master URL: {spark.sparkContext.master}\")\n",
        "print(f\"ğŸ“Š Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
        "print(f\"ğŸ¯ App ID: {spark.sparkContext.applicationId}\")\n",
        "print(f\"ğŸš€ VersiÃ³n Spark: {spark.version}\")\n",
        "\n",
        "# Verificar configuraciÃ³n especÃ­fica para NoSQL\n",
        "print(\"\\nğŸ“‹ CONFIGURACIÃ“N NOSQL:\")\n",
        "print(f\"   ğŸŒŸ Cassandra Host: {spark.conf.get('spark.cassandra.connection.host')}\")\n",
        "print(f\"   ğŸ”Œ Cassandra Port: {spark.conf.get('spark.cassandra.connection.port')}\")\n",
        "print(f\"   ğŸ›ï¸ HBase Host: {spark.conf.get('spark.hbase.host')}\")\n",
        "print(f\"   ğŸ”Œ HBase Port: {spark.conf.get('spark.hbase.port')}\")\n",
        "\n",
        "# Verificar estado del cluster\n",
        "print(\"\\nğŸ” ESTADO DEL CLUSTER SPARK:\")\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try:\n",
        "    # InformaciÃ³n de executors\n",
        "    executor_infos = sc.statusTracker().getExecutorInfos()\n",
        "    active_executors = len([e for e in executor_infos if e.isActive])\n",
        "    print(f\"   âš¡ Executors activos: {active_executors}\")\n",
        "    print(f\"   ğŸ–¥ï¸ Total executors: {len(executor_infos)}\")\n",
        "    \n",
        "    # Test rÃ¡pido de conectividad\n",
        "    test_rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "    result = test_rdd.sum()\n",
        "    print(f\"   âœ… Test de conectividad: Suma = {result}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"   âš ï¸ Error verificando cluster: {e}\")\n",
        "\n",
        "print(\"\\nğŸŒŸ URLs de Monitoreo:\")\n",
        "print(\"   ğŸ›ï¸ Spark Master UI: http://localhost:8080\")\n",
        "print(\"   ğŸ“Š Spark Driver UI: http://localhost:4040\")\n",
        "\n",
        "print(\"\\nâœ… Spark configurado para integraciÃ³n NoSQL\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”— **Tutorial Paso a Paso: IntegraciÃ³n Spark + HBase**\n",
        "\n",
        "### ğŸ¯ **Objetivo:**\n",
        "Aprender a leer datos desde HBase usando Spark, realizar transformaciones distribuidas y escribir resultados de vuelta a HBase.\n",
        "\n",
        "### ğŸ“‹ **Casos de Uso Reales:**\n",
        "1. **ETL desde HBase**: Extraer datos de HBase, transformarlos y cargarlos en otro sistema\n",
        "2. **AnÃ¡lisis en tiempo real**: Procesar datos de HBase con Spark para analytics\n",
        "3. **Agregaciones complejas**: Usar Spark SQL para consultas complejas sobre datos HBase\n",
        "4. **Data Lake Integration**: Combinar datos de HBase con datos de HDFS/S3\n",
        "\n",
        "### ğŸ—ï¸ **Arquitectura de IntegraciÃ³n:**\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚   Spark Driver  â”‚â—„â”€â”€â–ºâ”‚  HBase Client   â”‚â—„â”€â”€â–ºâ”‚  HBase Cluster  â”‚\n",
        "â”‚                 â”‚    â”‚   (happybase)   â”‚    â”‚                 â”‚\n",
        "â”‚ â€¢ SparkSession  â”‚    â”‚ â€¢ Thrift API    â”‚    â”‚ â€¢ RegionServers â”‚\n",
        "â”‚ â€¢ DataFrame API â”‚    â”‚ â€¢ Connection    â”‚    â”‚ â€¢ HDFS Storage  â”‚\n",
        "â”‚ â€¢ SQL Engine   â”‚    â”‚ â€¢ Table Ops     â”‚    â”‚ â€¢ Zookeeper     â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "         â”‚                       â”‚                       â”‚\n",
        "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                                 â”‚\n",
        "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "                    â”‚  Spark Workers  â”‚\n",
        "                    â”‚                 â”‚\n",
        "                    â”‚ â€¢ Data Parallel â”‚\n",
        "                    â”‚ â€¢ Executors     â”‚\n",
        "                    â”‚ â€¢ Task Exec     â”‚\n",
        "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ’¡ **Mejores PrÃ¡cticas:**\n",
        "- **Partitioning**: DiseÃ±ar row keys para distribuciÃ³n uniforme\n",
        "- **Batch Processing**: Usar batch operations para mejor rendimiento\n",
        "- **Caching**: Cache DataFrames frecuentemente usados\n",
        "- **Resource Management**: Configurar memoria adecuadamente\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# ğŸ›ï¸ INTEGRACIÃ“N SPARK + HBASE: LECTURA Y ANÃLISIS DE DATOS (CONFIGURACIÃ“N SIMPLE)\n",
        "\n",
        "print(\"ğŸ›ï¸ INICIANDO INTEGRACIÃ“N SPARK + HBASE...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Instalar happybase si no estÃ¡ disponible\n",
        "try:\n",
        "    import happybase\n",
        "    print(\"âœ… HappyBase ya estÃ¡ disponible\")\n",
        "except ImportError:\n",
        "    print(\"ğŸ“¦ Instalando HappyBase...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"happybase\"])\n",
        "    import happybase\n",
        "    print(\"âœ… HappyBase instalado correctamente\")\n",
        "\n",
        "# Establecer conexiÃ³n con HBase usando configuraciÃ³n simple\n",
        "print(\"\\nğŸ”— CONECTANDO A HBASE CON CONFIGURACIÃ“N SIMPLE:\")\n",
        "\n",
        "try:\n",
        "    # CONFIGURACIÃ“N EXACTAMENTE IGUAL AL TUTORIAL ORIGINAL QUE FUNCIONABA\n",
        "    hbase_connection = happybase.Connection(\n",
        "        host='hbase',  # Nombre del servicio en Docker\n",
        "        port=9090,     # Puerto Thrift\n",
        "        timeout=10000\n",
        "    )\n",
        "    \n",
        "    # Test de conexiÃ³n bÃ¡sico (igual al original)\n",
        "    tables = list(hbase_connection.tables())\n",
        "    print(\"ğŸ‰ Â¡ConexiÃ³n a HBase exitosa!\")\n",
        "    print(f\"ğŸ“Š Tablas existentes: {len(tables)}\")\n",
        "    \n",
        "    # PASO 1: LEER DATOS DE HBASE Y CONVERTIR A SPARK DATAFRAME\n",
        "    print(\"\\n1ï¸âƒ£ LEYENDO DATOS DE HBASE Y CREANDO SPARK DATAFRAME:\")\n",
        "    \n",
        "    # Verificar si existe la tabla de productos (del notebook anterior)\n",
        "    existing_tables = [table.decode('utf-8') for table in hbase_connection.tables()]\n",
        "    \n",
        "    if 'productos_ecommerce' not in existing_tables:\n",
        "        print(\"âš ï¸ Tabla 'productos_ecommerce' no existe. CreÃ¡ndola con datos de ejemplo...\")\n",
        "        \n",
        "        # Crear tabla y datos de ejemplo\n",
        "        column_families = {\n",
        "            'info': {'compression': 'SNAPPY'},\n",
        "            'inventory': {'compression': 'SNAPPY'},\n",
        "            'sales': {'compression': 'SNAPPY'}\n",
        "        }\n",
        "        \n",
        "        hbase_connection.create_table('productos_ecommerce', column_families)\n",
        "        table = hbase_connection.table('productos_ecommerce')\n",
        "        \n",
        "        # Insertar datos de ejemplo\n",
        "        sample_data = [\n",
        "            ('PROD_001', {\n",
        "                b'info:name': b'Laptop Gaming Pro',\n",
        "                b'info:category': b'Electronics',\n",
        "                b'info:price': b'1299.99',\n",
        "                b'inventory:stock': b'25',\n",
        "                b'sales:total_sold': b'156',\n",
        "                b'sales:revenue': b'202798.44'\n",
        "            }),\n",
        "            ('PROD_002', {\n",
        "                b'info:name': b'Smartphone Ultra',\n",
        "                b'info:category': b'Electronics', \n",
        "                b'info:price': b'899.99',\n",
        "                b'inventory:stock': b'45',\n",
        "                b'sales:total_sold': b'289',\n",
        "                b'sales:revenue': b'260097.11'\n",
        "            }),\n",
        "            ('PROD_003', {\n",
        "                b'info:name': b'Wireless Headphones',\n",
        "                b'info:category': b'Audio',\n",
        "                b'info:price': b'199.99',\n",
        "                b'inventory:stock': b'78',\n",
        "                b'sales:total_sold': b'432',\n",
        "                b'sales:revenue': b'86395.68'\n",
        "            })\n",
        "        ]\n",
        "        \n",
        "        for row_key, data in sample_data:\n",
        "            table.put(row_key, data)\n",
        "        \n",
        "        print(\"âœ… Tabla y datos de ejemplo creados\")\n",
        "    else:\n",
        "        table = hbase_connection.table('productos_ecommerce')\n",
        "        print(\"âœ… Usando tabla existente 'productos_ecommerce'\")\n",
        "    \n",
        "    # Leer datos de HBase y convertir a lista para Spark DataFrame\n",
        "    print(\"\\nğŸ“Š EXTRAYENDO DATOS DE HBASE:\")\n",
        "    \n",
        "    hbase_data = []\n",
        "    for key, data in table.scan():\n",
        "        row_dict = {'product_id': key.decode('utf-8')}\n",
        "        \n",
        "        # Extraer datos de diferentes column families\n",
        "        for col_key, col_value in data.items():\n",
        "            family, qualifier = col_key.decode('utf-8').split(':')\n",
        "            column_name = f\"{family}_{qualifier}\"\n",
        "            row_dict[column_name] = col_value.decode('utf-8')\n",
        "        \n",
        "        hbase_data.append(row_dict)\n",
        "    \n",
        "    print(f\"âœ… ExtraÃ­dos {len(hbase_data)} registros de HBase\")\n",
        "    \n",
        "    # Mostrar datos extraÃ­dos\n",
        "    print(\"\\nğŸ“‹ DATOS EXTRAÃDOS DE HBASE:\")\n",
        "    for i, row in enumerate(hbase_data, 1):\n",
        "        print(f\"   {i}. {row['product_id']}: {row.get('info_name', 'N/A')}\")\n",
        "    \n",
        "    # PASO 2: CREAR SPARK DATAFRAME DESDE DATOS HBASE\n",
        "    print(\"\\n2ï¸âƒ£ CREANDO SPARK DATAFRAME DESDE DATOS HBASE:\")\n",
        "    \n",
        "    # Definir esquema para el DataFrame\n",
        "    hbase_schema = StructType([\n",
        "        StructField(\"product_id\", StringType(), False),\n",
        "        StructField(\"info_name\", StringType(), True),\n",
        "        StructField(\"info_category\", StringType(), True),\n",
        "        StructField(\"info_price\", StringType(), True),\n",
        "        StructField(\"inventory_stock\", StringType(), True),\n",
        "        StructField(\"sales_total_sold\", StringType(), True),\n",
        "        StructField(\"sales_revenue\", StringType(), True)\n",
        "    ])\n",
        "    \n",
        "    # Crear DataFrame desde los datos HBase\n",
        "    hbase_df = spark.createDataFrame(hbase_data, schema=hbase_schema)\n",
        "    \n",
        "    print(\"âœ… Spark DataFrame creado desde datos HBase\")\n",
        "    print(f\"ğŸ“Š Registros en DataFrame: {hbase_df.count()}\")\n",
        "    \n",
        "    # Mostrar esquema y datos\n",
        "    print(\"\\nğŸ“‹ ESQUEMA DEL DATAFRAME:\")\n",
        "    hbase_df.printSchema()\n",
        "    \n",
        "    print(\"\\nğŸ“Š DATOS DEL DATAFRAME:\")\n",
        "    hbase_df.show(truncate=False)\n",
        "    \n",
        "    # PASO 3: TRANSFORMACIONES CON SPARK\n",
        "    print(\"\\n3ï¸âƒ£ APLICANDO TRANSFORMACIONES CON SPARK:\")\n",
        "    \n",
        "    # Convertir tipos de datos\n",
        "    transformed_df = hbase_df \\\n",
        "        .withColumn(\"price\", col(\"info_price\").cast(\"double\")) \\\n",
        "        .withColumn(\"stock\", col(\"inventory_stock\").cast(\"integer\")) \\\n",
        "        .withColumn(\"total_sold\", col(\"sales_total_sold\").cast(\"integer\")) \\\n",
        "        .withColumn(\"revenue\", col(\"sales_revenue\").cast(\"double\")) \\\n",
        "        .withColumn(\"inventory_value\", col(\"price\") * col(\"stock\")) \\\n",
        "        .select(\n",
        "            col(\"product_id\"),\n",
        "            col(\"info_name\").alias(\"name\"),\n",
        "            col(\"info_category\").alias(\"category\"),\n",
        "            col(\"price\"),\n",
        "            col(\"stock\"),\n",
        "            col(\"total_sold\"),\n",
        "            col(\"revenue\"),\n",
        "            col(\"inventory_value\")\n",
        "        )\n",
        "    \n",
        "    print(\"âœ… Transformaciones aplicadas\")\n",
        "    print(\"\\nğŸ“Š DATAFRAME TRANSFORMADO:\")\n",
        "    transformed_df.show()\n",
        "    \n",
        "    # PASO 4: ANÃLISIS CON SPARK SQL\n",
        "    print(\"\\n4ï¸âƒ£ ANÃLISIS CON SPARK SQL:\")\n",
        "    \n",
        "    # Registrar DataFrame como tabla temporal\n",
        "    transformed_df.createOrReplaceTempView(\"productos_hbase\")\n",
        "    \n",
        "    # AnÃ¡lisis 1: Productos mÃ¡s vendidos\n",
        "    print(\"\\nğŸ“ˆ TOP PRODUCTOS MÃS VENDIDOS:\")\n",
        "    top_selling = spark.sql(\"\"\"\n",
        "        SELECT name, category, total_sold, revenue\n",
        "        FROM productos_hbase\n",
        "        ORDER BY total_sold DESC\n",
        "    \"\"\")\n",
        "    top_selling.show()\n",
        "    \n",
        "    # AnÃ¡lisis 2: Valor total del inventario por categorÃ­a\n",
        "    print(\"\\nğŸ’° VALOR DEL INVENTARIO POR CATEGORÃA:\")\n",
        "    inventory_by_category = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            category,\n",
        "            COUNT(*) as num_products,\n",
        "            SUM(stock) as total_stock,\n",
        "            SUM(inventory_value) as total_inventory_value,\n",
        "            AVG(price) as avg_price\n",
        "        FROM productos_hbase\n",
        "        GROUP BY category\n",
        "        ORDER BY total_inventory_value DESC\n",
        "    \"\"\")\n",
        "    inventory_by_category.show()\n",
        "    \n",
        "    # AnÃ¡lisis 3: ROI por producto\n",
        "    print(\"\\nğŸ“Š ROI (RETURN ON INVESTMENT) POR PRODUCTO:\")\n",
        "    roi_analysis = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            name,\n",
        "            category,\n",
        "            price,\n",
        "            inventory_value,\n",
        "            revenue,\n",
        "            ROUND((revenue / inventory_value) * 100, 2) as roi_percentage\n",
        "        FROM productos_hbase\n",
        "        WHERE inventory_value > 0\n",
        "        ORDER BY roi_percentage DESC\n",
        "    \"\"\")\n",
        "    roi_analysis.show()\n",
        "    \n",
        "    # PASO 5: AGREGACIONES AVANZADAS\n",
        "    print(\"\\n5ï¸âƒ£ AGREGACIONES AVANZADAS:\")\n",
        "    \n",
        "    # EstadÃ­sticas generales\n",
        "    stats = transformed_df.agg(\n",
        "        sum(\"revenue\").alias(\"total_revenue\"),\n",
        "        sum(\"inventory_value\").alias(\"total_inventory_value\"),\n",
        "        sum(\"total_sold\").alias(\"total_units_sold\"),\n",
        "        avg(\"price\").alias(\"avg_price\"),\n",
        "        count(\"*\").alias(\"total_products\")\n",
        "    ).collect()[0]\n",
        "    \n",
        "    print(\"\\nğŸ“Š ESTADÃSTICAS GENERALES:\")\n",
        "    print(f\"   ğŸ’° Ingresos totales: ${stats['total_revenue']:,.2f}\")\n",
        "    print(f\"   ğŸ“¦ Valor total inventario: ${stats['total_inventory_value']:,.2f}\")\n",
        "    print(f\"   ğŸ“ˆ Unidades vendidas: {stats['total_units_sold']:,}\")\n",
        "    print(f\"   ğŸ’µ Precio promedio: ${stats['avg_price']:,.2f}\")\n",
        "    print(f\"   ğŸ“± Total productos: {stats['total_products']}\")\n",
        "    \n",
        "    # Cache del DataFrame para consultas futuras\n",
        "    transformed_df.cache()\n",
        "    print(\"\\nâœ… DataFrame cacheado para consultas futuras\")\n",
        "    \n",
        "    print(\"\\nğŸ‰ Â¡IntegraciÃ³n Spark + HBase completada exitosamente!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error en integraciÃ³n Spark + HBase: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸŒŸ **Tutorial Paso a Paso: IntegraciÃ³n Spark + Cassandra**\n",
        "\n",
        "### ğŸ¯ **Objetivo:**\n",
        "Aprender a leer datos desde Cassandra usando Spark, realizar anÃ¡lisis distribuidos y aprovechar la escalabilidad de ambas tecnologÃ­as.\n",
        "\n",
        "### ğŸ“‹ **Casos de Uso Reales:**\n",
        "1. **Real-time Analytics**: Procesar streams de datos almacenados en Cassandra\n",
        "2. **Time-series Analysis**: Analizar datos temporales con Spark SQL\n",
        "3. **Cross-datacenter Analytics**: Consultas distribuidas en mÃºltiples regiones\n",
        "4. **IoT Data Processing**: Procesar millones de eventos de sensores\n",
        "\n",
        "### ğŸ—ï¸ **Arquitectura de IntegraciÃ³n:**\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚   Spark Driver  â”‚â—„â”€â”€â–ºâ”‚ Cassandra       â”‚â—„â”€â”€â–ºâ”‚  Cassandra Ring â”‚\n",
        "â”‚                 â”‚    â”‚ Connector       â”‚    â”‚                 â”‚\n",
        "â”‚ â€¢ SparkSession  â”‚    â”‚ â€¢ Token Aware   â”‚    â”‚ â€¢ Node 1, 2, 3  â”‚\n",
        "â”‚ â€¢ DataFrame API â”‚    â”‚ â€¢ Load Balance  â”‚    â”‚ â€¢ Replication   â”‚\n",
        "â”‚ â€¢ SQL Engine   â”‚    â”‚ â€¢ Pushdown      â”‚    â”‚ â€¢ Partitioning  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "         â”‚                       â”‚                       â”‚\n",
        "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                                 â”‚\n",
        "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "                    â”‚  Spark Workers  â”‚\n",
        "                    â”‚                 â”‚\n",
        "                    â”‚ â€¢ Parallel Read â”‚\n",
        "                    â”‚ â€¢ Local Compute â”‚\n",
        "                    â”‚ â€¢ Aggregation   â”‚\n",
        "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ’¡ **Ventajas de la IntegraciÃ³n:**\n",
        "- **Locality Awareness**: Spark ejecuta tareas cerca de los datos\n",
        "- **Predicate Pushdown**: Filtros aplicados en Cassandra\n",
        "- **Token Range Splitting**: ParalelizaciÃ³n automÃ¡tica\n",
        "- **Fault Tolerance**: RecuperaciÃ³n automÃ¡tica de fallos\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# ğŸŒŸ INTEGRACIÃ“N SPARK + CASSANDRA: ANÃLISIS DISTRIBUIDO (CONFIGURACIÃ“N SIMPLE)\n",
        "\n",
        "print(\"ğŸŒŸ INICIANDO INTEGRACIÃ“N SPARK + CASSANDRA...\")\n",
        "print(\"=\"*52)\n",
        "\n",
        "# Instalar driver de Cassandra si no estÃ¡ disponible\n",
        "try:\n",
        "    from cassandra.cluster import Cluster\n",
        "    print(\"âœ… Driver de Cassandra ya estÃ¡ disponible\")\n",
        "except ImportError:\n",
        "    print(\"ğŸ“¦ Instalando driver de Cassandra...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"cassandra-driver\"])\n",
        "    from cassandra.cluster import Cluster\n",
        "    print(\"âœ… Driver de Cassandra instalado correctamente\")\n",
        "\n",
        "# Establecer conexiÃ³n con Cassandra usando configuraciÃ³n simple\n",
        "print(\"\\nğŸ”— CONECTANDO A CASSANDRA CON CONFIGURACIÃ“N SIMPLE:\")\n",
        "\n",
        "try:\n",
        "    # CONFIGURACIÃ“N EXACTAMENTE IGUAL AL TUTORIAL ORIGINAL QUE FUNCIONABA\n",
        "    cluster = Cluster(\n",
        "        contact_points=['cassandra'],  # Nombre del servicio en Docker\n",
        "        port=9042\n",
        "    )\n",
        "    \n",
        "    # Crear sesiÃ³n\n",
        "    session = cluster.connect()\n",
        "    \n",
        "    # Test de conexiÃ³n (igual al original)\n",
        "    result = session.execute(\"SELECT release_version FROM system.local\")\n",
        "    version = result.one()[0]\n",
        "    \n",
        "    print(\"ğŸ‰ Â¡ConexiÃ³n a Cassandra exitosa!\")\n",
        "    print(f\"ğŸ”¢ VersiÃ³n de Cassandra: {version}\")\n",
        "    \n",
        "    # Verificar si existe el keyspace del tutorial anterior\n",
        "    keyspaces = session.execute(\"SELECT keyspace_name FROM system_schema.keyspaces\")\n",
        "    existing_keyspaces = [ks.keyspace_name for ks in keyspaces]\n",
        "    \n",
        "    if 'ecommerce_nosql' not in existing_keyspaces:\n",
        "        print(\"âš ï¸ Keyspace 'ecommerce_nosql' no existe. CreÃ¡ndolo con datos de ejemplo...\")\n",
        "        \n",
        "        # Crear keyspace\n",
        "        session.execute(\"\"\"\n",
        "            CREATE KEYSPACE ecommerce_nosql\n",
        "            WITH REPLICATION = {\n",
        "                'class': 'SimpleStrategy',\n",
        "                'replication_factor': 1\n",
        "            }\n",
        "        \"\"\")\n",
        "        \n",
        "        session.set_keyspace('ecommerce_nosql')\n",
        "        \n",
        "        # Crear tabla de productos\n",
        "        session.execute(\"\"\"\n",
        "            CREATE TABLE productos (\n",
        "                producto_id text PRIMARY KEY,\n",
        "                nombre text,\n",
        "                categoria text,\n",
        "                precio decimal,\n",
        "                descripcion text,\n",
        "                fecha_creacion timestamp,\n",
        "                activo boolean\n",
        "            )\n",
        "        \"\"\")\n",
        "        \n",
        "        # Crear tabla de ventas por fecha\n",
        "        session.execute(\"\"\"\n",
        "            CREATE TABLE ventas_por_fecha (\n",
        "                fecha_venta date,\n",
        "                hora_venta time,\n",
        "                venta_id uuid,\n",
        "                producto_id text,\n",
        "                cantidad int,\n",
        "                monto decimal,\n",
        "                cliente_id text,\n",
        "                PRIMARY KEY ((fecha_venta), hora_venta, venta_id)\n",
        "            ) WITH CLUSTERING ORDER BY (hora_venta DESC, venta_id ASC)\n",
        "        \"\"\")\n",
        "        \n",
        "        # Insertar datos de ejemplo\n",
        "        import uuid\n",
        "        from datetime import date, datetime\n",
        "        \n",
        "        # Productos\n",
        "        productos_sample = [\n",
        "            (\"PROD_001\", \"MacBook Pro 16\", \"Laptops\", 2499.99, \"Laptop profesional\", True),\n",
        "            (\"PROD_002\", \"iPhone 15 Pro\", \"Smartphones\", 1199.99, \"Smartphone premium\", True),\n",
        "            (\"PROD_003\", \"iPad Air\", \"Tablets\", 599.99, \"Tablet ligera\", True),\n",
        "            (\"PROD_004\", \"AirPods Pro\", \"Audio\", 249.99, \"Auriculares premium\", True),\n",
        "            (\"PROD_005\", \"Apple Watch\", \"Wearables\", 399.99, \"Smartwatch avanzado\", True)\n",
        "        ]\n",
        "        \n",
        "        for pid, nombre, categoria, precio, desc, activo in productos_sample:\n",
        "            session.execute(\"\"\"\n",
        "                INSERT INTO productos (producto_id, nombre, categoria, precio, descripcion, fecha_creacion, activo)\n",
        "                VALUES (?, ?, ?, ?, ?, toTimestamp(now()), ?)\n",
        "            \"\"\", [pid, nombre, categoria, precio, desc, activo])\n",
        "        \n",
        "        # Ventas (datos mÃ¡s ricos para anÃ¡lisis)\n",
        "        ventas_sample = []\n",
        "        for day in range(1, 16):  # 15 dÃ­as de datos\n",
        "            for hour in range(9, 18):  # Horario comercial\n",
        "                for _ in range(2):  # 2 ventas por hora\n",
        "                    ventas_sample.append((\n",
        "                        f\"2024-01-{day:02d}\",\n",
        "                        f\"{hour}:{30 if _ == 0 else 45}:00\",\n",
        "                        str(uuid.uuid4()),\n",
        "                        f\"PROD_{(day + hour + _) % 5 + 1:03d}\",\n",
        "                        1 + (_ % 3),  # Cantidad 1-3\n",
        "                        round(200 + (day * hour * 10) + (_ * 50), 2),  # Monto variable\n",
        "                        f\"CLIENTE_{(day * 100 + hour * 10 + _):05d}\"\n",
        "                    ))\n",
        "        \n",
        "        prepared_venta = session.prepare(\"\"\"\n",
        "            INSERT INTO ventas_por_fecha (fecha_venta, hora_venta, venta_id, producto_id, cantidad, monto, cliente_id)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
        "        \"\"\")\n",
        "        \n",
        "        for venta in ventas_sample:\n",
        "            session.execute(prepared_venta, venta)\n",
        "        \n",
        "        print(f\"âœ… Keyspace creado con {len(productos_sample)} productos y {len(ventas_sample)} ventas\")\n",
        "    else:\n",
        "        session.set_keyspace('ecommerce_nosql')\n",
        "        print(\"âœ… Usando keyspace existente 'ecommerce_nosql'\")\n",
        "    \n",
        "    # PASO 1: LEER DATOS DE CASSANDRA USANDO SPARK\n",
        "    print(\"\\n1ï¸âƒ£ LEYENDO DATOS DE CASSANDRA CON SPARK:\")\n",
        "    \n",
        "    # Leer tabla de productos\n",
        "    print(\"\\nğŸ“Š CARGANDO PRODUCTOS DESDE CASSANDRA:\")\n",
        "    \n",
        "    # Simular lectura de Cassandra (en un entorno real usarÃ­amos spark-cassandra-connector)\n",
        "    # Por ahora leemos directamente y convertimos a Spark DataFrame\n",
        "    productos_cass = session.execute(\"SELECT * FROM productos\")\n",
        "    productos_data = []\n",
        "    \n",
        "    for producto in productos_cass:\n",
        "        productos_data.append({\n",
        "            'producto_id': producto.producto_id,\n",
        "            'nombre': producto.nombre,\n",
        "            'categoria': producto.categoria,\n",
        "            'precio': float(producto.precio),\n",
        "            'descripcion': producto.descripcion,\n",
        "            'activo': producto.activo\n",
        "        })\n",
        "    \n",
        "    print(f\"âœ… ExtraÃ­dos {len(productos_data)} productos de Cassandra\")\n",
        "    \n",
        "    # Crear DataFrame de Spark desde datos Cassandra\n",
        "    productos_schema = StructType([\n",
        "        StructField(\"producto_id\", StringType(), False),\n",
        "        StructField(\"nombre\", StringType(), True),\n",
        "        StructField(\"categoria\", StringType(), True),\n",
        "        StructField(\"precio\", DoubleType(), True),\n",
        "        StructField(\"descripcion\", StringType(), True),\n",
        "        StructField(\"activo\", BooleanType(), True)\n",
        "    ])\n",
        "    \n",
        "    productos_df = spark.createDataFrame(productos_data, schema=productos_schema)\n",
        "    \n",
        "    print(\"ğŸ“‹ PRODUCTOS CARGADOS EN SPARK:\")\n",
        "    productos_df.show()\n",
        "    \n",
        "    # Leer tabla de ventas\n",
        "    print(\"\\nğŸ’° CARGANDO VENTAS DESDE CASSANDRA:\")\n",
        "    \n",
        "    ventas_cass = session.execute(\"SELECT * FROM ventas_por_fecha\")\n",
        "    ventas_data = []\n",
        "    \n",
        "    for venta in ventas_cass:\n",
        "        ventas_data.append({\n",
        "            'fecha_venta': str(venta.fecha_venta),\n",
        "            'hora_venta': str(venta.hora_venta),\n",
        "            'venta_id': str(venta.venta_id),\n",
        "            'producto_id': venta.producto_id,\n",
        "            'cantidad': venta.cantidad,\n",
        "            'monto': float(venta.monto),\n",
        "            'cliente_id': venta.cliente_id\n",
        "        })\n",
        "    \n",
        "    print(f\"âœ… ExtraÃ­das {len(ventas_data)} ventas de Cassandra\")\n",
        "    \n",
        "    # Crear DataFrame de ventas\n",
        "    ventas_schema = StructType([\n",
        "        StructField(\"fecha_venta\", StringType(), False),\n",
        "        StructField(\"hora_venta\", StringType(), False),\n",
        "        StructField(\"venta_id\", StringType(), False),\n",
        "        StructField(\"producto_id\", StringType(), True),\n",
        "        StructField(\"cantidad\", IntegerType(), True),\n",
        "        StructField(\"monto\", DoubleType(), True),\n",
        "        StructField(\"cliente_id\", StringType(), True)\n",
        "    ])\n",
        "    \n",
        "    ventas_df = spark.createDataFrame(ventas_data, schema=ventas_schema)\n",
        "    \n",
        "    # Convertir fecha a formato Date\n",
        "    ventas_df = ventas_df.withColumn(\"fecha\", to_date(col(\"fecha_venta\"), \"yyyy-MM-dd\"))\n",
        "    \n",
        "    print(\"ğŸ“‹ MUESTRA DE VENTAS CARGADAS EN SPARK:\")\n",
        "    ventas_df.select(\"fecha\", \"producto_id\", \"cantidad\", \"monto\", \"cliente_id\").show(10)\n",
        "    \n",
        "    print(\"\\nâœ… Datos de Cassandra cargados exitosamente en Spark\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error en integraciÃ³n Spark + Cassandra: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    productos_df = None\n",
        "    ventas_df = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# ğŸ”¬ ANÃLISIS AVANZADO CON SPARK + CASSANDRA\n",
        "\n",
        "if 'productos_df' in locals() and productos_df and 'ventas_df' in locals() and ventas_df:\n",
        "    print(\"ğŸ”¬ INICIANDO ANÃLISIS AVANZADO CON DATOS DE CASSANDRA...\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # PASO 2: JOINS ENTRE DATAFRAMES DE CASSANDRA\n",
        "    print(\"\\n2ï¸âƒ£ REALIZANDO JOINS ENTRE TABLAS DE CASSANDRA:\")\n",
        "    \n",
        "    # Join entre productos y ventas\n",
        "    ventas_con_productos = ventas_df.join(\n",
        "        productos_df,\n",
        "        ventas_df.producto_id == productos_df.producto_id,\n",
        "        \"inner\"\n",
        "    ).select(\n",
        "        ventas_df.fecha,\n",
        "        ventas_df.hora_venta,\n",
        "        ventas_df.venta_id,\n",
        "        productos_df.nombre.alias(\"producto_nombre\"),\n",
        "        productos_df.categoria,\n",
        "        productos_df.precio.alias(\"precio_unitario\"),\n",
        "        ventas_df.cantidad,\n",
        "        ventas_df.monto,\n",
        "        ventas_df.cliente_id\n",
        "    ).withColumn(\n",
        "        \"revenue_calculado\", \n",
        "        col(\"precio_unitario\") * col(\"cantidad\")\n",
        "    )\n",
        "    \n",
        "    print(\"âœ… Join completado entre productos y ventas\")\n",
        "    print(\"\\nğŸ“Š MUESTRA DE DATOS UNIDOS:\")\n",
        "    ventas_con_productos.show(10, truncate=False)\n",
        "    \n",
        "    # PASO 3: ANÃLISIS TEMPORAL DE VENTAS\n",
        "    print(\"\\n3ï¸âƒ£ ANÃLISIS TEMPORAL DE VENTAS:\")\n",
        "    \n",
        "    # Registrar como tablas temporales para SQL\n",
        "    ventas_con_productos.createOrReplaceTempView(\"ventas_completas\")\n",
        "    productos_df.createOrReplaceTempView(\"productos_cassandra\")\n",
        "    ventas_df.createOrReplaceTempView(\"ventas_cassandra\")\n",
        "    \n",
        "    # AnÃ¡lisis 1: Ventas por dÃ­a\n",
        "    print(\"\\nğŸ“ˆ VENTAS DIARIAS:\")\n",
        "    ventas_diarias = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            fecha,\n",
        "            COUNT(*) as num_transacciones,\n",
        "            SUM(cantidad) as unidades_vendidas,\n",
        "            SUM(monto) as revenue_total,\n",
        "            AVG(monto) as ticket_promedio,\n",
        "            COUNT(DISTINCT cliente_id) as clientes_unicos\n",
        "        FROM ventas_completas\n",
        "        GROUP BY fecha\n",
        "        ORDER BY fecha\n",
        "    \"\"\")\n",
        "    \n",
        "    ventas_diarias.show()\n",
        "    \n",
        "    # AnÃ¡lisis 2: Productos mÃ¡s vendidos\n",
        "    print(\"\\nğŸ† TOP PRODUCTOS MÃS VENDIDOS:\")\n",
        "    top_productos = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            producto_nombre,\n",
        "            categoria,\n",
        "            COUNT(*) as num_ventas,\n",
        "            SUM(cantidad) as unidades_totales,\n",
        "            SUM(monto) as revenue_total,\n",
        "            AVG(monto) as ticket_promedio\n",
        "        FROM ventas_completas\n",
        "        GROUP BY producto_nombre, categoria\n",
        "        ORDER BY revenue_total DESC\n",
        "    \"\"\")\n",
        "    \n",
        "    top_productos.show()\n",
        "    \n",
        "    # AnÃ¡lisis 3: PatrÃ³n horario de ventas\n",
        "    print(\"\\nâ° PATRÃ“N HORARIO DE VENTAS:\")\n",
        "    patron_horario = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            SUBSTRING(hora_venta, 1, 2) as hora,\n",
        "            COUNT(*) as num_ventas,\n",
        "            SUM(monto) as revenue_total,\n",
        "            AVG(monto) as ticket_promedio\n",
        "        FROM ventas_completas\n",
        "        GROUP BY SUBSTRING(hora_venta, 1, 2)\n",
        "        ORDER BY hora\n",
        "    \"\"\")\n",
        "    \n",
        "    patron_horario.show()\n",
        "    \n",
        "    # PASO 4: ANÃLISIS POR CATEGORÃAS\n",
        "    print(\"\\n4ï¸âƒ£ ANÃLISIS DETALLADO POR CATEGORÃAS:\")\n",
        "    \n",
        "    categoria_analysis = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            categoria,\n",
        "            COUNT(DISTINCT producto_nombre) as productos_diferentes,\n",
        "            COUNT(*) as total_transacciones,\n",
        "            SUM(cantidad) as unidades_vendidas,\n",
        "            SUM(monto) as revenue_total,\n",
        "            AVG(monto) as ticket_promedio,\n",
        "            MIN(monto) as venta_minima,\n",
        "            MAX(monto) as venta_maxima,\n",
        "            COUNT(DISTINCT cliente_id) as clientes_unicos\n",
        "        FROM ventas_completas\n",
        "        GROUP BY categoria\n",
        "        ORDER BY revenue_total DESC\n",
        "    \"\"\")\n",
        "    \n",
        "    categoria_analysis.show()\n",
        "    \n",
        "    # PASO 5: ANÃLISIS DE CLIENTES\n",
        "    print(\"\\n5ï¸âƒ£ ANÃLISIS DE COMPORTAMIENTO DE CLIENTES:\")\n",
        "    \n",
        "    cliente_analysis = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            COUNT(DISTINCT cliente_id) as total_clientes,\n",
        "            AVG(compras_por_cliente) as avg_compras_por_cliente,\n",
        "            AVG(gasto_por_cliente) as avg_gasto_por_cliente,\n",
        "            MAX(compras_por_cliente) as max_compras_cliente,\n",
        "            MAX(gasto_por_cliente) as max_gasto_cliente\n",
        "        FROM (\n",
        "            SELECT \n",
        "                cliente_id,\n",
        "                COUNT(*) as compras_por_cliente,\n",
        "                SUM(monto) as gasto_por_cliente\n",
        "            FROM ventas_completas\n",
        "            GROUP BY cliente_id\n",
        "        ) cliente_stats\n",
        "    \"\"\")\n",
        "    \n",
        "    cliente_analysis.show()\n",
        "    \n",
        "    # Top clientes\n",
        "    print(\"\\nğŸ‘‘ TOP 10 CLIENTES POR REVENUE:\")\n",
        "    top_clientes = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            cliente_id,\n",
        "            COUNT(*) as num_compras,\n",
        "            SUM(cantidad) as unidades_compradas,\n",
        "            SUM(monto) as gasto_total,\n",
        "            AVG(monto) as ticket_promedio,\n",
        "            COUNT(DISTINCT categoria) as categorias_diferentes\n",
        "        FROM ventas_completas\n",
        "        GROUP BY cliente_id\n",
        "        ORDER BY gasto_total DESC\n",
        "        LIMIT 10\n",
        "    \"\"\")\n",
        "    \n",
        "    top_clientes.show()\n",
        "    \n",
        "    # PASO 6: MÃ‰TRICAS DE RENDIMIENTO Y CACHING\n",
        "    print(\"\\n6ï¸âƒ£ OPTIMIZACIÃ“N Y CACHING:\")\n",
        "    \n",
        "    # Cache de DataFrames frecuentemente usados\n",
        "    ventas_con_productos.cache()\n",
        "    print(\"âœ… DataFrame ventas_con_productos cacheado\")\n",
        "    \n",
        "    # Contar registros (fuerza la evaluaciÃ³n del cache)\n",
        "    total_registros = ventas_con_productos.count()\n",
        "    print(f\"ğŸ“Š Total de registros en anÃ¡lisis: {total_registros:,}\")\n",
        "    \n",
        "    # EstadÃ­sticas de particionamiento\n",
        "    print(f\"ğŸ“Š NÃºmero de particiones: {ventas_con_productos.rdd.getNumPartitions()}\")\n",
        "    \n",
        "    # PASO 7: AGREGACIONES COMPLEJAS CON WINDOW FUNCTIONS\n",
        "    print(\"\\n7ï¸âƒ£ ANÃLISIS AVANZADO CON WINDOW FUNCTIONS:\")\n",
        "    \n",
        "    from pyspark.sql.window import Window\n",
        "    \n",
        "    # Ranking de productos por dÃ­a\n",
        "    window_spec = Window.partitionBy(\"fecha\").orderBy(col(\"revenue_total\").desc())\n",
        "    \n",
        "    ranking_diario = ventas_con_productos.groupBy(\"fecha\", \"producto_nombre\", \"categoria\") \\\n",
        "        .agg(\n",
        "            sum(\"monto\").alias(\"revenue_total\"),\n",
        "            sum(\"cantidad\").alias(\"unidades_vendidas\"),\n",
        "            count(\"*\").alias(\"num_transacciones\")\n",
        "        ) \\\n",
        "        .withColumn(\"ranking_dia\", row_number().over(window_spec)) \\\n",
        "        .filter(col(\"ranking_dia\") <= 3) \\\n",
        "        .orderBy(\"fecha\", \"ranking_dia\")\n",
        "    \n",
        "    print(\"\\nğŸ† TOP 3 PRODUCTOS POR DÃA:\")\n",
        "    ranking_diario.show(20, truncate=False)\n",
        "    \n",
        "    print(\"\\nğŸ‰ Â¡AnÃ¡lisis avanzado completado exitosamente!\")\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ No hay datos disponibles para anÃ¡lisis\")\n",
        "    print(\"ğŸ’¡ Ejecuta las celdas anteriores para cargar los datos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”„ **Casos de Uso HÃ­bridos: Spark + HBase + Cassandra**\n",
        "\n",
        "### ğŸ¯ **Escenarios del Mundo Real:**\n",
        "\n",
        "#### **1. ğŸª E-commerce Multi-canal**\n",
        "- **HBase**: CatÃ¡logo de productos, inventario en tiempo real\n",
        "- **Cassandra**: Historial de clicks, eventos de usuario, logs\n",
        "- **Spark**: AnÃ¡lisis de comportamiento, recomendaciones, ETL\n",
        "\n",
        "#### **2. ğŸ¦ Sistema Bancario**\n",
        "- **HBase**: Transacciones en tiempo real, balances de cuentas\n",
        "- **Cassandra**: Logs de auditorÃ­a, historial de transacciones\n",
        "- **Spark**: DetecciÃ³n de fraude, anÃ¡lisis de riesgo, reportes\n",
        "\n",
        "#### **3. ğŸŒ IoT y TelemetrÃ­a**\n",
        "- **HBase**: Estados actuales de sensores, configuraciones\n",
        "- **Cassandra**: Series temporales de sensores, mÃ©tricas\n",
        "- **Spark**: AnÃ¡lisis de patrones, alertas, predicciones\n",
        "\n",
        "#### **4. ğŸ“± AplicaciÃ³n Social**\n",
        "- **HBase**: Perfiles de usuario, relaciones sociales\n",
        "- **Cassandra**: Timeline de posts, mensajes, notificaciones\n",
        "- **Spark**: AnÃ¡lisis de sentimientos, trending topics, ML\n",
        "\n",
        "### ğŸ—ï¸ **Arquitectura HÃ­brida:**\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    HYBRID BIG DATA ARCHITECTURE                 â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                 â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
        "â”‚  â”‚   Data Sources  â”‚    â”‚  Processing     â”‚    â”‚   Storage   â”‚  â”‚\n",
        "â”‚  â”‚                 â”‚    â”‚                 â”‚    â”‚             â”‚  â”‚\n",
        "â”‚  â”‚ â€¢ Web Apps      â”‚â”€â”€â”€â–ºâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”€â”€â”€â–ºâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚\n",
        "â”‚  â”‚ â€¢ Mobile Apps   â”‚    â”‚ â”‚   Apache    â”‚ â”‚    â”‚ â”‚ HBase   â”‚ â”‚  â”‚\n",
        "â”‚  â”‚ â€¢ IoT Sensors   â”‚    â”‚ â”‚   Spark     â”‚ â”‚    â”‚ â”‚ (OLTP)  â”‚ â”‚  â”‚\n",
        "â”‚  â”‚ â€¢ APIs          â”‚    â”‚ â”‚             â”‚ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚\n",
        "â”‚  â”‚ â€¢ Batch Files   â”‚    â”‚ â”‚ â€¢ Driver    â”‚ â”‚    â”‚             â”‚  â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â”‚ â€¢ Workers   â”‚ â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚\n",
        "â”‚                         â”‚ â”‚ â€¢ SQL       â”‚ â”‚    â”‚ â”‚Cassandraâ”‚ â”‚  â”‚\n",
        "â”‚                         â”‚ â”‚ â€¢ MLlib     â”‚ â”‚    â”‚ â”‚(Analytics)â”‚ â”‚  â”‚\n",
        "â”‚                         â”‚ â”‚ â€¢ Streaming â”‚ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚\n",
        "â”‚                         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚             â”‚  â”‚\n",
        "â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚\n",
        "â”‚                                                â”‚ â”‚  HDFS   â”‚ â”‚  â”‚\n",
        "â”‚                                                â”‚ â”‚(Archive)â”‚ â”‚  â”‚\n",
        "â”‚                                                â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚\n",
        "â”‚                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
        "â”‚                                                                 â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
        "â”‚  â”‚                    DATA FLOW                            â”‚   â”‚\n",
        "â”‚  â”‚                                                         â”‚   â”‚\n",
        "â”‚  â”‚  Real-time â”€â”€â”€â”€â–º HBase â”€â”€â”€â”€â–º Spark â”€â”€â”€â”€â–º Cassandra     â”‚   â”‚\n",
        "â”‚  â”‚  (OLTP)          (Fast)     (Process)   (Analytics)    â”‚   â”‚\n",
        "â”‚  â”‚                                                         â”‚   â”‚\n",
        "â”‚  â”‚  Batch â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º HDFS â”€â”€â”€â”€â–º Spark â”€â”€â”€â”€â–º HBase/Cass    â”‚   â”‚\n",
        "â”‚  â”‚  (ETL)           (Storage)   (Process)   (Serve)       â”‚   â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ’¡ **Mejores PrÃ¡cticas de IntegraciÃ³n:**\n",
        "\n",
        "#### **ğŸ”§ ConfiguraciÃ³n:**\n",
        "- **Connection Pooling**: Reutilizar conexiones entre Spark tasks\n",
        "- **Batch Size**: Optimizar tamaÃ±o de lotes para escritura\n",
        "- **Partitioning**: Alinear particiones Spark con particiones NoSQL\n",
        "- **Caching**: Cache DataFrames frecuentemente accedidos\n",
        "\n",
        "#### **âš¡ Rendimiento:**\n",
        "- **Predicate Pushdown**: Filtros aplicados en la fuente\n",
        "- **Columnar Projection**: Leer solo columnas necesarias\n",
        "- **Locality Awareness**: Ejecutar tareas cerca de los datos\n",
        "- **Parallel Processing**: Maximizar paralelizaciÃ³n\n",
        "\n",
        "#### **ğŸ›¡ï¸ Confiabilidad:**\n",
        "- **Retry Logic**: Reintentos automÃ¡ticos en fallos\n",
        "- **Circuit Breakers**: ProtecciÃ³n contra cascading failures\n",
        "- **Monitoring**: MÃ©tricas de rendimiento y salud\n",
        "- **Backup Strategies**: Estrategias de respaldo hÃ­bridas\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# ğŸŒŸ CASO DE USO HÃBRIDO: ANÃLISIS CROSS-PLATFORM\n",
        "\n",
        "print(\"ğŸŒŸ DEMOSTRANDO CASO DE USO HÃBRIDO...\")\n",
        "print(\"=\"*45)\n",
        "print(\"ğŸ“Š Combinando datos de HBase y Cassandra en un anÃ¡lisis unificado\")\n",
        "\n",
        "# Verificar que tenemos datos de ambas fuentes\n",
        "hbase_available = 'transformed_df' in locals() and transformed_df is not None\n",
        "cassandra_available = 'ventas_con_productos' in locals() and ventas_con_productos is not None\n",
        "\n",
        "if hbase_available and cassandra_available:\n",
        "    print(\"\\nâœ… Datos disponibles de ambas fuentes:\")\n",
        "    print(f\"   ğŸ›ï¸ HBase: DataFrame 'transformed_df'\")\n",
        "    print(f\"   ğŸŒŸ Cassandra: DataFrame 'ventas_con_productos'\")\n",
        "    \n",
        "    # PASO 1: ANÃLISIS COMPARATIVO DE INVENTARIOS\n",
        "    print(\"\\n1ï¸âƒ£ ANÃLISIS COMPARATIVO DE INVENTARIOS:\")\n",
        "    \n",
        "    # Datos de HBase (inventario actual)\n",
        "    print(\"\\nğŸ“¦ INVENTARIO ACTUAL (HBase):\")\n",
        "    hbase_inventory = transformed_df.select(\n",
        "        col(\"product_id\"),\n",
        "        col(\"name\").alias(\"producto_nombre\"),\n",
        "        col(\"category\").alias(\"categoria\"),\n",
        "        col(\"price\").alias(\"precio\"),\n",
        "        col(\"stock\"),\n",
        "        col(\"inventory_value\")\n",
        "    )\n",
        "    \n",
        "    hbase_inventory.show()\n",
        "    \n",
        "    # Datos de Cassandra (ventas histÃ³ricas)\n",
        "    print(\"\\nğŸ’° RESUMEN DE VENTAS (Cassandra):\")\n",
        "    cassandra_sales = ventas_con_productos.groupBy(\"producto_nombre\", \"categoria\") \\\n",
        "        .agg(\n",
        "            sum(\"cantidad\").alias(\"total_vendido\"),\n",
        "            sum(\"monto\").alias(\"revenue_total\"),\n",
        "            count(\"*\").alias(\"num_transacciones\"),\n",
        "            avg(\"monto\").alias(\"ticket_promedio\")\n",
        "        )\n",
        "    \n",
        "    cassandra_sales.show()\n",
        "    \n",
        "    # PASO 2: ANÃLISIS HÃBRIDO - ROTACIÃ“N DE INVENTARIO\n",
        "    print(\"\\n2ï¸âƒ£ ANÃLISIS HÃBRIDO - ROTACIÃ“N DE INVENTARIO:\")\n",
        "    \n",
        "    # Unir datos de ambas fuentes por nombre de producto\n",
        "    inventory_turnover = hbase_inventory.join(\n",
        "        cassandra_sales,\n",
        "        hbase_inventory.producto_nombre == cassandra_sales.producto_nombre,\n",
        "        \"left_outer\"\n",
        "    ).select(\n",
        "        hbase_inventory.product_id,\n",
        "        hbase_inventory.producto_nombre,\n",
        "        hbase_inventory.categoria,\n",
        "        hbase_inventory.precio,\n",
        "        hbase_inventory.stock,\n",
        "        hbase_inventory.inventory_value,\n",
        "        coalesce(cassandra_sales.total_vendido, lit(0)).alias(\"total_vendido\"),\n",
        "        coalesce(cassandra_sales.revenue_total, lit(0.0)).alias(\"revenue_total\"),\n",
        "        coalesce(cassandra_sales.num_transacciones, lit(0)).alias(\"num_transacciones\")\n",
        "    ).withColumn(\n",
        "        \"turnover_ratio\",\n",
        "        when(col(\"stock\") > 0, col(\"total_vendido\") / col(\"stock\")).otherwise(0)\n",
        "    ).withColumn(\n",
        "        \"stock_days\",\n",
        "        when(col(\"total_vendido\") > 0, col(\"stock\") * 30 / col(\"total_vendido\")).otherwise(999)\n",
        "    ).withColumn(\n",
        "        \"performance_score\",\n",
        "        (col(\"turnover_ratio\") * 0.4) + \n",
        "        (when(col(\"stock_days\") < 30, 1.0).otherwise(0.5) * 0.3) +\n",
        "        (col(\"revenue_total\") / 10000 * 0.3)\n",
        "    )\n",
        "    \n",
        "    print(\"\\nğŸ“Š ANÃLISIS DE ROTACIÃ“N DE INVENTARIO:\")\n",
        "    inventory_turnover.select(\n",
        "        \"product_id\", \"producto_nombre\", \"categoria\", \"stock\", \n",
        "        \"total_vendido\", \"turnover_ratio\", \"stock_days\", \"performance_score\"\n",
        "    ).orderBy(col(\"performance_score\").desc()).show()\n",
        "    \n",
        "    # PASO 3: RECOMENDACIONES INTELIGENTES\n",
        "    print(\"\\n3ï¸âƒ£ RECOMENDACIONES INTELIGENTES BASADAS EN DATOS HÃBRIDOS:\")\n",
        "    \n",
        "    # Registrar vista temporal\n",
        "    inventory_turnover.createOrReplaceTempView(\"inventory_analysis\")\n",
        "    \n",
        "    # Productos con alto rendimiento\n",
        "    high_performers = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            product_id,\n",
        "            producto_nombre,\n",
        "            categoria,\n",
        "            stock,\n",
        "            total_vendido,\n",
        "            revenue_total,\n",
        "            ROUND(turnover_ratio, 2) as turnover_ratio,\n",
        "            ROUND(stock_days, 1) as stock_days,\n",
        "            ROUND(performance_score, 2) as performance_score,\n",
        "            CASE \n",
        "                WHEN performance_score > 1.5 THEN 'ğŸš€ Estrella'\n",
        "                WHEN performance_score > 1.0 THEN 'â­ Alto rendimiento'\n",
        "                WHEN performance_score > 0.5 THEN 'ğŸ“ˆ Promedio'\n",
        "                ELSE 'âš ï¸ Bajo rendimiento'\n",
        "            END as clasificacion\n",
        "        FROM inventory_analysis\n",
        "        ORDER BY performance_score DESC\n",
        "    \"\"\")\n",
        "    \n",
        "    print(\"\\nğŸ† CLASIFICACIÃ“N DE PRODUCTOS POR RENDIMIENTO:\")\n",
        "    high_performers.show(truncate=False)\n",
        "    \n",
        "    # Alertas de inventario\n",
        "    print(\"\\nğŸš¨ ALERTAS DE INVENTARIO:\")\n",
        "    alerts = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            product_id,\n",
        "            producto_nombre,\n",
        "            categoria,\n",
        "            stock,\n",
        "            stock_days,\n",
        "            CASE \n",
        "                WHEN stock_days < 7 THEN 'ğŸ”´ CRÃTICO: Restock inmediato'\n",
        "                WHEN stock_days < 15 THEN 'ğŸŸ¡ ADVERTENCIA: Planificar restock'\n",
        "                WHEN stock_days > 90 THEN 'ğŸ”µ INFO: Exceso de inventario'\n",
        "                ELSE 'âœ… OK: Niveles normales'\n",
        "            END as alerta\n",
        "        FROM inventory_analysis\n",
        "        WHERE stock_days < 15 OR stock_days > 90\n",
        "        ORDER BY stock_days ASC\n",
        "    \"\"\")\n",
        "    \n",
        "    alerts.show(truncate=False)\n",
        "    \n",
        "    # PASO 4: ESTADÃSTICAS FINALES DEL ANÃLISIS HÃBRIDO\n",
        "    print(\"\\n4ï¸âƒ£ ESTADÃSTICAS FINALES DEL ANÃLISIS HÃBRIDO:\")\n",
        "    \n",
        "    final_stats = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            COUNT(*) as total_productos,\n",
        "            SUM(stock) as total_stock_units,\n",
        "            SUM(inventory_value) as total_inventory_value,\n",
        "            SUM(total_vendido) as total_units_sold,\n",
        "            SUM(revenue_total) as total_revenue,\n",
        "            AVG(turnover_ratio) as avg_turnover_ratio,\n",
        "            AVG(stock_days) as avg_stock_days,\n",
        "            COUNT(CASE WHEN stock_days < 15 THEN 1 END) as productos_criticos,\n",
        "            COUNT(CASE WHEN performance_score > 1.0 THEN 1 END) as productos_alto_rendimiento\n",
        "        FROM inventory_analysis\n",
        "    \"\"\").collect()[0]\n",
        "    \n",
        "    print(\"\\nğŸ“Š RESUMEN EJECUTIVO:\")\n",
        "    print(f\"   ğŸ“¦ Total productos analizados: {final_stats['total_productos']}\")\n",
        "    print(f\"   ğŸ“ˆ Total unidades en stock: {final_stats['total_stock_units']:,}\")\n",
        "    print(f\"   ğŸ’° Valor total inventario: ${final_stats['total_inventory_value']:,.2f}\")\n",
        "    print(f\"   ğŸ“Š Total unidades vendidas: {final_stats['total_units_sold']:,}\")\n",
        "    print(f\"   ğŸ’µ Revenue total: ${final_stats['total_revenue']:,.2f}\")\n",
        "    print(f\"   ğŸ”„ Ratio rotaciÃ³n promedio: {final_stats['avg_turnover_ratio']:.2f}\")\n",
        "    print(f\"   ğŸ“… DÃ­as promedio de stock: {final_stats['avg_stock_days']:.1f}\")\n",
        "    print(f\"   ğŸš¨ Productos crÃ­ticos: {final_stats['productos_criticos']}\")\n",
        "    print(f\"   â­ Productos alto rendimiento: {final_stats['productos_alto_rendimiento']}\")\n",
        "    \n",
        "    # PASO 5: EXPORTAR RESULTADOS\n",
        "    print(\"\\n5ï¸âƒ£ PREPARANDO RESULTADOS PARA EXPORT:\")\n",
        "    \n",
        "    # Cache del anÃ¡lisis final para uso posterior\n",
        "    inventory_turnover.cache()\n",
        "    high_performers.cache()\n",
        "    \n",
        "    print(\"âœ… DataFrames de anÃ¡lisis hÃ­brido cacheados\")\n",
        "    print(\"ğŸ’¾ Listos para exportar a sistemas downstream\")\n",
        "    \n",
        "    print(\"\\nğŸ‰ Â¡ANÃLISIS HÃBRIDO COMPLETADO EXITOSAMENTE!\")\n",
        "    print(\"ğŸ”— Datos de HBase y Cassandra integrados con Ã©xito\")\n",
        "    print(\"ğŸ“ˆ Insights accionables generados para el negocio\")\n",
        "\n",
        "elif hbase_available:\n",
        "    print(\"âš ï¸ Solo datos de HBase disponibles\")\n",
        "    print(\"ğŸ’¡ Ejecuta las celdas de integraciÃ³n con Cassandra para anÃ¡lisis completo\")\n",
        "    \n",
        "elif cassandra_available:\n",
        "    print(\"âš ï¸ Solo datos de Cassandra disponibles\")\n",
        "    print(\"ğŸ’¡ Ejecuta las celdas de integraciÃ³n con HBase para anÃ¡lisis completo\")\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ No hay datos disponibles de ninguna fuente\")\n",
        "    print(\"ğŸ’¡ Ejecuta las celdas anteriores para cargar datos de HBase y Cassandra\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ‰ **Resumen y Conclusiones del Tutorial**\n",
        "\n",
        "### âœ… **Lo que Hemos Logrado:**\n",
        "\n",
        "#### **1. ğŸš€ ConfiguraciÃ³n Spark Optimizada**\n",
        "- âœ… SesiÃ³n Spark configurada para integraciÃ³n NoSQL\n",
        "- âœ… Recursos optimizados para cluster mode\n",
        "- âœ… Configuraciones especÃ­ficas para conectores NoSQL\n",
        "- âœ… Monitoreo y verificaciÃ³n de estado del cluster\n",
        "\n",
        "#### **2. ğŸ›ï¸ IntegraciÃ³n HBase + Spark**\n",
        "- âœ… ConexiÃ³n exitosa entre Spark y HBase\n",
        "- âœ… Lectura de datos desde HBase a DataFrames\n",
        "- âœ… Transformaciones distribuidas con Spark SQL\n",
        "- âœ… AnÃ¡lisis de ROI y agregaciones avanzadas\n",
        "- âœ… Caching para optimizaciÃ³n de rendimiento\n",
        "\n",
        "#### **3. ğŸŒŸ IntegraciÃ³n Cassandra + Spark**\n",
        "- âœ… ConexiÃ³n exitosa entre Spark y Cassandra\n",
        "- âœ… AnÃ¡lisis temporal de series de datos\n",
        "- âœ… Joins complejos entre tablas distribuidas\n",
        "- âœ… Window functions para rankings dinÃ¡micos\n",
        "- âœ… AnÃ¡lisis de comportamiento de clientes\n",
        "\n",
        "#### **4. ğŸ”„ Casos de Uso HÃ­bridos**\n",
        "- âœ… IntegraciÃ³n de datos de mÃºltiples fuentes NoSQL\n",
        "- âœ… AnÃ¡lisis de rotaciÃ³n de inventario cross-platform\n",
        "- âœ… GeneraciÃ³n de alertas inteligentes\n",
        "- âœ… Recomendaciones basadas en datos hÃ­bridos\n",
        "- âœ… MÃ©tricas de rendimiento empresarial\n",
        "\n",
        "### ğŸ“Š **Arquitectura Final Implementada:**\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                  ECOSISTEMA BIG DATA COMPLETO                  â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                 â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
        "â”‚  â”‚   SPARK 3.5.3   â”‚â—„â”€â”€â–ºâ”‚     HBASE       â”‚    â”‚  CASSANDRA  â”‚  â”‚\n",
        "â”‚  â”‚                 â”‚    â”‚                 â”‚    â”‚             â”‚  â”‚\n",
        "â”‚  â”‚ â€¢ Driver: 1.2GB â”‚    â”‚ â€¢ Thrift API    â”‚    â”‚ â€¢ CQL API   â”‚  â”‚\n",
        "â”‚  â”‚ â€¢ Workers: 2x   â”‚    â”‚ â€¢ Column Store  â”‚    â”‚ â€¢ Ring Arch â”‚  â”‚\n",
        "â”‚  â”‚ â€¢ Executors:800Mâ”‚    â”‚ â€¢ HDFS Backend  â”‚    â”‚ â€¢ P2P Nodes â”‚  â”‚\n",
        "â”‚  â”‚ â€¢ Hive Support  â”‚    â”‚ â€¢ Strong Cons.  â”‚    â”‚ â€¢ Eventual  â”‚  â”‚\n",
        "â”‚  â”‚ â€¢ SQL Engine    â”‚    â”‚ â€¢ Real-time     â”‚    â”‚ â€¢ Scalable  â”‚  â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
        "â”‚           â”‚                       â”‚                      â”‚       â”‚\n",
        "â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
        "â”‚                                   â”‚                              â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\n",
        "â”‚  â”‚                  JUPYTER ECOSYSTEM                      â”‚     â”‚\n",
        "â”‚  â”‚                                                         â”‚     â”‚\n",
        "â”‚  â”‚  ğŸ““ 01_spark_cluster_professional.ipynb               â”‚     â”‚\n",
        "â”‚  â”‚  ğŸ““ 02_nosql_foundations.ipynb                         â”‚     â”‚\n",
        "â”‚  â”‚  ğŸ““ 03_spark_nosql_integration.ipynb                  â”‚     â”‚\n",
        "â”‚  â”‚                                                         â”‚     â”‚\n",
        "â”‚  â”‚  â€¢ Tutoriales completos CRUD                           â”‚     â”‚\n",
        "â”‚  â”‚  â€¢ Casos de uso reales                                 â”‚     â”‚\n",
        "â”‚  â”‚  â€¢ Mejores prÃ¡cticas                                   â”‚     â”‚\n",
        "â”‚  â”‚  â€¢ AnÃ¡lisis hÃ­bridos                                   â”‚     â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ¯ **Casos de Uso Implementados:**\n",
        "\n",
        "#### **ğŸª E-commerce Analytics**\n",
        "- AnÃ¡lisis de inventario en tiempo real\n",
        "- Tracking de ventas y comportamiento de clientes\n",
        "- Recomendaciones de productos\n",
        "- Alertas de stock crÃ­tico\n",
        "\n",
        "#### **ğŸ“Š Business Intelligence**\n",
        "- Dashboards en tiempo real\n",
        "- AnÃ¡lisis de tendencias temporales\n",
        "- MÃ©tricas de rendimiento KPI\n",
        "- Reportes ejecutivos automatizados\n",
        "\n",
        "#### **ğŸ” Data Science & ML**\n",
        "- Feature engineering distribuido\n",
        "- AnÃ¡lisis exploratorio de datos\n",
        "- PreparaciÃ³n de datasets para ML\n",
        "- AnÃ¡lisis de patrones complejos\n",
        "\n",
        "### ğŸ’¡ **Mejores PrÃ¡cticas Aplicadas:**\n",
        "\n",
        "#### **âš¡ Rendimiento:**\n",
        "- âœ… Caching estratÃ©gico de DataFrames\n",
        "- âœ… Particionamiento optimizado\n",
        "- âœ… Pushdown de predicados\n",
        "- âœ… ConfiguraciÃ³n de memoria ajustada\n",
        "\n",
        "#### **ğŸ›¡ï¸ Confiabilidad:**\n",
        "- âœ… Manejo de errores robusto\n",
        "- âœ… VerificaciÃ³n de conectividad\n",
        "- âœ… Timeouts configurados\n",
        "- âœ… Logging estructurado\n",
        "\n",
        "#### **ğŸ”§ Mantenibilidad:**\n",
        "- âœ… CÃ³digo bien documentado\n",
        "- âœ… Estructura modular\n",
        "- âœ… ConfiguraciÃ³n externalizada\n",
        "- âœ… Ejemplos reproducibles\n",
        "\n",
        "### ğŸš€ **PrÃ³ximos Pasos Sugeridos:**\n",
        "\n",
        "1. **ğŸ“ˆ Escalabilidad**: Implementar en clusters multi-nodo\n",
        "2. **ğŸ”„ Streaming**: Agregar Spark Streaming para datos en tiempo real  \n",
        "3. **ğŸ¤– Machine Learning**: Integrar MLlib para modelos predictivos\n",
        "4. **ğŸ“Š VisualizaciÃ³n**: Conectar con herramientas como Grafana/Tableau\n",
        "5. **ğŸ” Seguridad**: Implementar autenticaciÃ³n y autorizaciÃ³n\n",
        "6. **ğŸ“¦ ProductizaciÃ³n**: ContainerizaciÃ³n con Kubernetes\n",
        "\n",
        "### ğŸ“ **Conocimientos Adquiridos:**\n",
        "\n",
        "- âœ… ConfiguraciÃ³n avanzada de Spark para entornos distribuidos\n",
        "- âœ… Operaciones CRUD completas en HBase y Cassandra\n",
        "- âœ… IntegraciÃ³n seamless entre tecnologÃ­as Big Data\n",
        "- âœ… AnÃ¡lisis de datos hÃ­bridos y cross-platform\n",
        "- âœ… OptimizaciÃ³n de rendimiento en ecosistemas complejos\n",
        "- âœ… Casos de uso reales de la industria\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ‰ **Â¡Felicitaciones!**\n",
        "\n",
        "Has completado exitosamente un tutorial integral de **Apache Spark + NoSQL** que cubre desde conceptos fundamentales hasta implementaciones avanzadas de casos de uso hÃ­bridos. Este conocimiento te permitirÃ¡ diseÃ±ar e implementar soluciones de Big Data robustas y escalables en entornos de producciÃ³n.\n",
        "\n",
        "**Â¡ContinÃºa explorando y construyendo soluciones increÃ­bles con Big Data! ğŸš€**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# ğŸš€ CREACIÃ“N DE SESIÃ“N SPARK INTEGRADA CON NoSQL\n",
        "\n",
        "print(\"ğŸš€ CREANDO SESIÃ“N SPARK PARA INTEGRACIÃ“N NoSQL...\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "# Detener cualquier sesiÃ³n existente\n",
        "try:\n",
        "    spark.stop()\n",
        "    print(\"ğŸ›‘ SesiÃ³n anterior detenida\")\n",
        "    time.sleep(3)\n",
        "except:\n",
        "    print(\"ğŸ” No habÃ­a sesiÃ³n activa\")\n",
        "\n",
        "# CONFIGURACIÃ“N OPTIMIZADA PARA INTEGRACIÃ“N NoSQL\n",
        "print(\"\\nâš™ï¸ Configurando Spark con conectores NoSQL...\")\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"EducacionIT-Spark-NoSQL-Integration\") \\\n",
        "    .master(\"spark://master:7077\") \\\n",
        "    .config(\"spark.driver.memory\", \"1200m\") \\\n",
        "    .config(\"spark.driver.cores\", \"2\") \\\n",
        "    .config(\"spark.executor.memory\", \"800m\") \\\n",
        "    .config(\"spark.executor.cores\", \"1\") \\\n",
        "    .config(\"spark.executor.instances\", \"2\") \\\n",
        "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.cassandra.connection.host\", \"cassandra\") \\\n",
        "    .config(\"spark.cassandra.connection.port\", \"9042\") \\\n",
        "    .config(\"spark.cassandra.connection.keepAlive\", \"true\") \\\n",
        "    .config(\"spark.cassandra.connection.timeout_ms\", \"10000\") \\\n",
        "    .config(\"spark.cassandra.read.timeout_ms\", \"120000\") \\\n",
        "    .config(\"spark.network.timeout\", \"300s\") \\\n",
        "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "# InformaciÃ³n de la sesiÃ³n integrada\n",
        "print(\"\\nğŸ‰ Â¡SESIÃ“N SPARK + NoSQL CREADA EXITOSAMENTE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"ğŸ·ï¸  AplicaciÃ³n: {spark.sparkContext.appName}\")\n",
        "print(f\"ğŸ”— Master URL: {spark.sparkContext.master}\")\n",
        "print(f\"ğŸ“Š Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
        "print(f\"ğŸš€ VersiÃ³n Spark: {spark.version}\")\n",
        "\n",
        "# Verificar configuraciÃ³n de conectores\n",
        "print(f\"\\nğŸ”§ CONFIGURACIÃ“N DE CONECTORES:\")\n",
        "print(f\"   ğŸŒŸ Cassandra Host: {spark.conf.get('spark.cassandra.connection.host')}\")\n",
        "print(f\"   ğŸ”Œ Cassandra Port: {spark.conf.get('spark.cassandra.connection.port')}\")\n",
        "print(f\"   â° Connection Timeout: {spark.conf.get('spark.cassandra.connection.timeout_ms')}ms\")\n",
        "print(f\"   ğŸ“– Read Timeout: {spark.conf.get('spark.cassandra.read.timeout_ms')}ms\")\n",
        "\n",
        "# Verificar conectividad con executors\n",
        "print(f\"\\nğŸ“Š ESTADO DEL CLUSTER:\")\n",
        "try:\n",
        "    executors = spark.sparkContext.statusTracker().getExecutorInfos()\n",
        "    active_executors = [e for e in executors if e.isActive]\n",
        "    print(f\"   âœ… Executors activos: {len(active_executors)}\")\n",
        "    \n",
        "    total_cores = sum([e.totalCores for e in active_executors])\n",
        "    total_memory = sum([e.maxMemory for e in active_executors])\n",
        "    \n",
        "    print(f\"   ğŸ–¥ï¸ Total cores: {total_cores}\")\n",
        "    print(f\"   ğŸ’¾ Total memoria: {total_memory / (1024**3):.2f} GB\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"   âš ï¸ Error obteniendo informaciÃ³n de executors: {e}\")\n",
        "\n",
        "print(\"\\nâœ… Spark configurado para integraciÃ³n con HBase y Cassandra\")\n",
        "print(\"ğŸ”— Listo para anÃ¡lisis distribuido hÃ­brido...\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
